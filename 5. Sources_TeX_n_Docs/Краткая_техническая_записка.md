# Определение кода Голланда по результатам психометрических тестов личности

## Цель и суть задачи

Цель работы — автоматизация процесса профориентации с помощью методов машинного обучения.  
Задача состояла в том, чтобы предсказать **код Голланда (RIASEC)** по неполным результатам других психометрических тестов.  
Профиль личности рассматривался как **вектор из шести числовых оценок**, каждая из которых отражает выраженность соответствующего типа личности, либо как **упорядоченный набор из шести меток**, отсортированных по степени выраженности.  

Тест Голланда широко используется в профориентации, но чувствителен к социально-культурным факторам и не всегда стабилен. В работе предлагается определять код Голланда по данным других, более надёжных тестов — Леонгарда, Айзенка, Кеттелла, Шварца и «Большой пятёрки».  

В исследовании использовались анонимизированные данные **1278 пользователей** мини-приложения **«Психологические тесты» ВКонтакте**, разработанного лабораторией прикладного искусственного интеллекта СПб ФИЦ РАН.  
Результаты работы применяются в этой лаборатории: найденный лучший пайплайн и обученная модель используются при анализе новых данных пользователей.

---

## Метрики качества

Для оценки точности и сопоставления разных типов алгоритмов применялись специализированные метрики:  

- **C-индекс** — профильная метрика предметной области, мера **сходства профессиональных профилей**, применявшаяся как основная при сравнении всех моделей и подходов.  
- **Регрессия** — среднеквадратичная ошибка (**RMSE**) по шести выходным переменным
- **Классификация** — показатели **Hits≥K@3** ($K = 1..3$), отражающие долю случаев, когда истинный код присутствует в числе первых 1–3 предсказаний
- **Ранжирование** — метрика **NDCG@3 (Normalized Discounted Cumulative Gain)**, оценивающая качество порядка первых трёх элементов предсказанного списка

Все модели и ансамбли сравнивались именно по значению C-индекса, как универсальной метрике качества в данной предметной области, также позволяющей сравнивать между собой и различные подходы.

---

## Подходы к постановке задачи

### Многоцелевая регрессия

Рассматривались три способа решения задачи прогнозирования шести факторов RIASEC:

- **Multioutput** — обучение шести независимых моделей, каждая из которых предсказывает один из факторов
- **Regressor chain** — последовательность моделей, где выходы предыдущих используются как признаки для следующих, что позволяет учитывать зависимости между факторами  
- **Модели со встроенной поддержкой мульти-выхода** — CatBoost, LightGBM, Random Forest и ExtraTrees, которые способны предсказывать несколько выходов одновременно

---

### Классификация

Были реализованы три постановки задачи:

- **Multiclass** — классификация на шесть классов, каждому объекту соответствует один основной тип; итоговая тройка формируется по трём наибольшим вероятностям
- **Multilabel** — шесть независимых бинарных классификаторов, определяющих, входит ли код в «топ-3»; финальная тройка кодов выбирается по наибольшим вероятностям принадлежности
- **Label powerset** — классификация на множество комбинаций трёх кодов Голланда, при этом внутри тройки порядок не учитывается. Модель фактически предсказывает тройку кодов, совпадающую с набором наиболее выраженных типов

---

### Списочное ранжирование

В рамках подхода ранжирования задача формулировалась как предсказание **полного упорядоченного списка шести кодов Голланда**.  
Использовались три архитектуры моделей:

- **Многослойный перцептрон (MLP)**
- **Deep & Cross Network**
- **Трансформер**

Для обучения применялись функции потерь:

- **ApproxNDCG** — приближённая оптимизация метрики NDCG
- **LambdaRank** — градиентная оптимизация парных сравнений
- **ListNet@1** и **ListNet@3** — вероятностные функции потерь, минимизирующие различие между распределениями ранговых позиций 

Метрика качества — **NDCG@3**, отражающая точность порядка трёх наиболее выраженных типов личности.

---

## Восстановление неполных данных

В работе реализованы и сравнены четыре подхода:

- **MICE (Multivariate Imputation by Chained Equations)**  
  • Плюсы: учитывает взаимозависимости признаков, выполняет итеративную множественную импутацию
  • Минусы: вычислительно сложен, возможны проблемы со сходимостью при высокой корреляции признаков

- **Soft Impute**  
  • Плюсы: основан на регуляризованном сингулярном разложении (SVD), устойчив и хорошо масштабируется 
  • Минусы: описывает преимущественно линейные зависимости

- **Маски пропусков**  
  • Плюсы: простота реализации, не создаёт синтетических значений
  • Минусы: модель должна самостоятельно учитывать наличие или отсутствие значения как информативный признак

- **Ансамбли по комбинациям тестов**  
  • Плюсы: адаптируется к конкретным комбинациям заполненных тестов
  • Минусы: требует большого числа моделей и усложняет сопровождение

Наилучший результат показал метод **Soft Impute** в сочетании с ансамблем регрессоров, где веса подбирались методом роя частиц.

---

## Взвешенное ансамблирование (блендинг)

Для объединения результатов базовых моделей использовалось **взвешенное ансамблирование (blending)** — линейная комбинация предсказаний с оптимизацией весов по целевой метрике.  
Рассматривались шесть способов подбора весов:

- **Равные веса** — базовый способ без оптимизации
- **Вектор Шэпли** — стохастическая аппроксимация вклада каждой модели на основе оценок важности
- **Grid search** — перебор весов по равномерной сетке с шагом 0.05–0.1 и выбор комбинации, максимизирующей метрику
- **Квадратичная оптимизация (Quadratic programming)** — минимизация квадратичной ошибки ансамбля при ограничении суммы весов
- **Генетический алгоритм (Genetic algorithm)** — эволюционная оптимизация: случайная инициализация, операции скрещивания и мутации, отбор лучших комбинаций по метрике
- **Метод роя частиц (Particle swarm optimization, PSO)** — оптимизация в пространстве весов с использованием популяции частиц, каждая из которых обновляет положение под действием собственной инерции и притяжения к лучшим решениям. Такой подход обеспечивает эффективный поиск глобального оптимума и устойчивость ансамбля к переобучению

Метод роя частиц показал наилучшие результаты по C-индексу и устойчивости ансамблей.

---

## Объём и глубина исследования

Разработан вычислительный конвейер, включающий все этапы — от восстановления пропусков и снижения размерности до обучения моделей и построения ансамблей.  
Комбинация параметров всех модулей (способ восстановления данных, применение PCA, тип постановки задачи, вариант подхода, набор базовых моделей, их гиперпараметры и метод ансамблирования) образует **до 63 000 возможных конфигураций**.  

Предложен способ автоматизированного перебора конфигураций, позволяющий протестировать большое число вариантов без полного перебора всех комбинаций.  
Подбор гиперпараметров для каждой модели осуществлялся с помощью **Grid search** по заданным диапазонам параметров и выполнялся в составе единого вычислительного эксперимента.

---

## Результаты вычислительного эксперимента

| Категория задачи | Лучшая модель | Метрика (C-индекс) |
|------------------|----------------|--------------------|
| Регрессия (multioutput) | Lasso (L1) | 11.175 |
| Классификация (multilabel) | kNN | 10.838 |
| Ранжирование | MLP с функцией потерь ListNet@3 | 10.788 |
| Ансамбль регрессоров (multioutput, PSO) | Lasso + Stepwise + CatBoost + ExtraTrees | **11.663** |
| Ансамбль классификаторов (multilabel, PSO) | kNN + SVM + Logistic L1 + XGBoost + LightGBM | **11.625** |
| Восстановленные данные (Soft Impute + PSO) | Ансамбль регрессоров | 10.740 |

**Выводы:**
- Ансамблевые методы обеспечивают устойчивое повышение точности по сравнению с одиночными моделями
- Классические ML-модели (Lasso, ExtraTrees, LightGBM) показали более высокие результаты, чем нейросетевые, что связано с ограниченным объёмом данных
- Использование Soft Impute и методов оптимизации весов (PSO) позволило добиться лучших показателей при работе с неполными данными
- Итоговые значения C-индекса 11.6–11.7 превосходят результаты аналогичных исследований (≈ 11.0)

---

## Технологический стек

- **R 4.4.2** — основной язык реализации вычислительного конвейера (предобработка, восстановление данных, обучение, ансамблирование, эксперименты)
- **Python 3.12.3** — реализация нейросетевых и ранжирующих моделей
- **Shiny** — веб-интерфейс прототипа инструмента профориентации
- Использовались специализированные библиотеки для статистического анализа, машинного обучения и визуализации данных

---

## Прототип и внедрение

Создан прототип инструмента профориентации на платформе **R Shiny**.  
Пользователь вводит частичные результаты тестов, система выполняет их валидацию, восстанавливает пропуски методом Soft Impute, применяет обученный ансамбль моделей и отображает предсказанный код Голланда с пояснением.  
Приложение развернуто в открытом доступе и используется в рамках лабораторных исследований СПб ФИЦ РАН.

---

## Итоговые результаты

- Реализованы четыре метода восстановления данных, три подхода к постановке задачи и шесть алгоритмов ансамблирования
- Проведён вычислительный эксперимент с автоматизированным перебором до 63 000 конфигураций моделей
- Лучшая модель — **взвешенный ансамбль регрессоров с оптимизацией весов методом роя частиц (PSO)** — достигла **C-индекса = 11.663**
- Разработан программный прототип и проведена оценка качества на неполных данных (C-индекс = 10.74)
- Результаты внедрены в научно-исследовательские работы СПб ФИЦ РАН и представлены на конференции SCM 2025
