---
title: Learn-to-rank (ранжирование)
jupyter: python3
---

# Импорты

```{python}
import pandas as pd
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, Dataset, DataLoader

from regr_metrics_func import *

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
```


# Данные

```{python}
# data = pd.read_feather("../0. Data/wide_data.feather")

# X = data.loc[:, ~data.columns.isin(["HL_1", "HL_2", "HL_3", "HL_4", "HL_5", "HL_6", "id"])]
# y = data.loc[:, "HL_1":"HL_6"]

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=242)

# scaler = StandardScaler().fit(X_train)
# X_train = scaler.transform(X_train)
# X_test = scaler.transform(X_test)

X_train = pd.read_feather("../0. Data/3. Saved_params/X_train.feather").to_numpy()
y_train = pd.read_feather("../0. Data/3. Saved_params/Y_train.feather")
X_test  = pd.read_feather("../0. Data/3. Saved_params/X_test.feather").to_numpy()
y_test  = pd.read_feather("../0. Data/3. Saved_params/Y_test.feather")
```


# Learn-to-rank
Ранжирование, listwise-подход

Общие функции и классы:
```{python}
class CustomDataset(Dataset):
    def __init__(self, X, y):
        self.X = X.astype(np.float32)
        self.y = y.astype(np.float32).values
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        features = self.X[idx]
        targets = self.y[idx]
        
        # Нормализация целей через softmax для получения распределения
        targets_normalized = torch.softmax(torch.tensor(targets), dim=-1).numpy()
        
        return {
            'features': features,
            'targets': targets_normalized
        }


# Обучение модели
def train_model(model, loss_func, X_train, y_train, num_epochs=100, batch_size=62):
    dataset = CustomDataset(X_train, y_train)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    if hasattr(model, 'fit') and hasattr(model, 'predict'):
        model.fit(X, y, group)
        return model

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for batch in dataloader:
            features = batch['features']
            targets = batch['targets']
            
            optimizer.zero_grad()
            preds = model(features)
            loss = loss_func(preds, targets)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        # if (epoch + 1) % 25 == 0:
        #     print(f"Epoch {epoch+1}/{num_epochs} — avg loss: {avg_loss:.4f}")
    print(f"Loss: {total_loss / len(dataloader):.4f}")    
    return model


def predict_scores(model, X_new):
    X_tensor = torch.tensor(X_new.astype(np.float32))
    with torch.no_grad():
        scores = model(X_tensor)
    # top3 = torch.topk(scores, k=3, dim=1).indices.tolist()
    return scores.numpy()

  
def ndcg_at_k(y_pred, y_true, k = 3):
    y_pred = y_pred
    y_true = np.asarray(y_true)
  
    # y_true, y_pred — массивы shape (n_queries, n_items)
    ndcgs = []
    for true, pred in zip(y_true, y_pred):
        order = np.argsort(pred)[::-1][:k]
        gains = 2**true[order] - 1
        discounts = np.log2(np.arange(2, k+2))
        dcg = (gains / discounts).sum()
        # ideal
        ideal = np.sort(true)[::-1][:k]
        idcg = ((2**ideal - 1) / discounts).sum()
        ndcgs.append(dcg / (idcg + 1e-10))
    return round(np.mean(ndcgs), 4)
```


## Архитектуры нейронных сетей

### MLP

```{python}
class MLPRanker(nn.Module):
    def __init__(self, input_dim=55, output_dim=6):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 128), 
            nn.BatchNorm1d(128), 
            nn.Dropout(0.1), 

            nn.Linear(128, 64), 
            nn.BatchNorm1d(64), 
            nn.Dropout(0.1), 

            nn.Linear(64, 32), 
            nn.BatchNorm1d(32), 
            nn.Dropout(0.1), 

            nn.Linear(32, output_dim)
        )
        
    def forward(self, x):
        return self.fc(x)
```


### MLP with ReLU

```{python}
class NonlinearMLPRanker(nn.Module):
    def __init__(self, input_dim=55, output_dim=6):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.1),

            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.1),

            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.1),

            nn.Linear(32, output_dim)
        )

    def forward(self, x):
        return self.fc(x)
```


### Listwise Transformer (Cross‑item Attention)

```{python}
# Listwise Transformer с item‑embedding
class ListwiseTransformer(nn.Module):
    def __init__(self, user_dim=55, item_embed_dim=8, d_model=64, nhead=4, num_layers=2, n_items=6):
        super().__init__()
        self.n_items = n_items

        # learnable‑вектор для каждого из 6 items
        self.item_emb = nn.Parameter(torch.randn(n_items, item_embed_dim))

        # проекция (user_feats || item_emb) → d_model
        self.input_proj = nn.Linear(user_dim + item_embed_dim, d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.out = nn.Linear(d_model, 1)

    def forward(self, user_feats):
        b = user_feats.size(0)
        u = user_feats.unsqueeze(1).expand(-1, self.n_items, -1)
        e = self.item_emb.unsqueeze(0).expand(b, -1, -1)
        x = torch.cat([u, e], dim=2)  # (b, 6, user_dim+item_embed_dim)
        # Transformer
        h = self.input_proj(x)        # (b, 6, d_model)
        h = h.permute(1,0,2)          # (6, b, d_model)
        h = self.encoder(h)           # (6, b, d_model)
        h = h.permute(1,0,2)          # (b, 6, d_model)
        scores = self.out(h).squeeze(-1)  # (b, 6)
        return scores
```


### Deep & Cross Network (DCN)

```{python}
# DCN‑архитектура с item‑embedding
class DCNLayer(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.w = nn.Parameter(torch.randn(dim))
        self.b = nn.Parameter(torch.zeros(dim))

    def forward(self, x):
        # x: (batch*6, dim)
        xw = (x * self.w).sum(dim=1, keepdim=True)  # (batch*6, 1)
        cross = x * xw + self.b
        return cross + x  # residual

class DCNUserItemRanker(nn.Module):
    def __init__(self, user_dim=55, item_embed_dim=8, cross_layers=3, deep_dims=[128,64,32], n_items=6):
        super().__init__()
        self.n_items = n_items
        self.item_emb = nn.Parameter(torch.randn(n_items, item_embed_dim))
        total_dim = user_dim + item_embed_dim

        # cross‑network
        self.cross_net = nn.ModuleList([DCNLayer(total_dim) for _ in range(cross_layers)])
        # deep‑network
        deep = []
        prev = total_dim
        for h in deep_dims:
            deep += [nn.Linear(prev, h), nn.ReLU(), nn.Dropout(0.1)]
            prev = h
        self.deep_net = nn.Sequential(*deep)
        # финальный выход
        self.out = nn.Linear(prev + total_dim, 1)

    def forward(self, user_feats):
        b = user_feats.size(0)
        u = user_feats.unsqueeze(1).expand(-1, self.n_items, -1)          # (b,6,user_dim)
        e = self.item_emb.unsqueeze(0).expand(b, -1, -1)                  # (b,6,item_emb)
        x = torch.cat([u, e], dim=2)                                      # (b,6,total_dim)
        x_flat = x.view(b*self.n_items, -1)  # (b*6, total_dim)
        # cross
        x_cross = x_flat
        for layer in self.cross_net:
            x_cross = layer(x_cross)
        # deep
        x_deep = self.deep_net(x_flat)
        x_cat = torch.cat([x_cross, x_deep], dim=1)   # (b*6, total_dim+deep_last)
        scores = self.out(x_cat).view(b, self.n_items)  # (b,6)
        return scores
```


## Функции потерь

```{python}
# ListNet
def listnet_loss(y_pred, y_true):
    P_pred = torch.softmax(y_pred, dim=1)
    P_true = torch.softmax(y_true, dim=1)
    # кросс-энтропия (вместо дивергенции KL как некое приближение)
    return -torch.mean(torch.sum(P_true * torch.log(P_pred + 1e-10), dim=1))


# ListNet Top-3
def listnet_loss_topk(y_pred, y_true, k=3, eps=1e-10):
    # 1) строим полные распределения
    P_pred = torch.softmax(y_pred, dim=1)  # (batch, n_items)
    P_true = torch.softmax(y_true, dim=1)  # (batch, n_items)
    # 2) для каждого примера берём индексы топ‑k по true
    _, idx_true = torch.sort(y_true, dim=1, descending=True)  # (batch, n_items)
    topk_idx = idx_true[:, :k]                                # (batch, k)
    # 3) собираем векторы P_pred и P_true только по этим индексам
    P_pred_k = P_pred.gather(1, topk_idx)  # (batch, k)
    P_true_k = P_true.gather(1, topk_idx)  # (batch, k)
    # 4) кросс‑энтропия на top‑k
    loss = -torch.mean(torch.sum(P_true_k * torch.log(P_pred_k + eps), dim=1))
    return loss


# дифференциируемая ApproxNDCG
def approx_ndcg_loss(scores, relevance, eps=1e-10):

    batch_size, num_targets = scores.shape
    # 1. Аппроксимируем ранги через попарные сравнения
    diff = scores.unsqueeze(2) - scores.unsqueeze(1)  # (batch, 6, 6) 
    # здесь сигмоида ("релаксация рангов"), чтобы NDCG была дифференциируемой
    approx_ranks = torch.sigmoid(diff).sum(dim=2)  # (batch, 6)
    # 2. Рассчитываем DCG
    dcg = (torch.pow(2.0, relevance) - 1) / torch.log2(approx_ranks + 1 + eps)
    dcg = dcg.sum(dim=1)  # (batch, )
    # 3. Рассчитываем IDCG (идеальное ранжирование)
    ideal_sorted, _ = torch.sort(relevance, dim=1, descending=True)  # (batch, 6)
    ideal_ranks = torch.arange(1, num_targets + 1, device=scores.device).float().unsqueeze(0)  # (1, 6)
    idcg = (torch.pow(2.0, ideal_sorted) - 1) / torch.log2(ideal_ranks + 1 + eps)
    idcg = idcg.sum(dim=1)  # (batch, )
    # 4. NDCG = DCG / IDCG, loss = 1 - NDCG
    ndcg = dcg / (idcg + eps)
    return 1 - ndcg.mean()


# LambdaRank
def lambda_rank_loss(scores, relevance, sigma=1.0):
    """
    scores: tensor of shape (batch_size, n_items)
    relevance: tensor of shape (batch_size, n_items)
    Implements LambdaRank pairwise loss with NDCG-based weights.
    """
    device = scores.device
    batch_size, n_items = scores.size()

    # Compute ideal DCG for normalization
    ideal_relevance, _ = torch.sort(relevance, descending=True, dim=1)
    positions = torch.arange(1, n_items+1, device=device).float().unsqueeze(0)
    ideal_gain = (2**ideal_relevance - 1)
    ideal_discount = torch.log2(positions + 1)
    ideal_dcg = torch.sum(ideal_gain / ideal_discount, dim=1)

    # Pairwise differences
    S_diff = scores.unsqueeze(2) - scores.unsqueeze(1)  # (B, N, N)
    P_ij = 1.0 / (1.0 + torch.exp(-sigma * S_diff))          # Sigmoid

    # Compute ΔNDCG for each pair (i,j)
    ranks = positions
    discount_i = 1.0 / torch.log2(ranks + 1)               # shape (1, N)

    # Compute gain differences for swapping
    gain = (2**relevance - 1)
    delta_gain = torch.abs(
        (gain.unsqueeze(2) - gain.unsqueeze(1)) * (discount_i.unsqueeze(2) - discount_i.unsqueeze(1))
    )  # (B, N, N)

    # ΔNDCG = |gain diff| / ideal_dcg
    delta_ndcg = delta_gain / (ideal_dcg.unsqueeze(1).unsqueeze(2) + 1e-10)
    lambda_ij = sigma * torch.abs(delta_ndcg) * (0.5 * (1 - torch.sign(S_diff)))  # only positive when s_i < s_j
    lambda_i = torch.sum(lambda_ij, dim=2) - torch.sum(lambda_ij, dim=1)
    loss = torch.sum(scores * lambda_i) / batch_size
    return loss
```


## Модели

```{python}
def evaluate_losses(model_classes, losses, num_epochs=100,
                    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test):
    records = []
    for model_cls in model_classes:
        model_name = model_cls.__name__
        for name, loss_fn in losses.items():
            # print(f"{name}")
            model = model_cls()
            model = train_model(model, loss_fn, X_train, y_train, num_epochs=num_epochs)
            preds = predict_scores(model, X_test)
            cidx  = df_metric(preds, y_test, calc_C_index)
            ndcg3 = ndcg_at_k(preds, y_test, k=3)
            # results[name] = {'C-index': cidx, f'NDCG@3': ndcg3}

            records.append({
                'model_cls': model_name,
                'loss_key': name,
                'C-index': cidx,
                'NDCG@3': ndcg3
            })
        df_results = pd.DataFrame.from_records(
            records,
            columns=['model_cls', 'loss_key', 'C-index', 'NDCG@3']
        )
    return df_results
```

```{python}
losses = {
    'ListNet@1': listnet_loss,
    'ListNet@3': listnet_loss_topk,
    'ApproxNDCG': approx_ndcg_loss,
    'LambdaRank': lambda_rank_loss
}

model_classes = [
    MLPRanker, 
    # NonlinearMLPRanker, # всегда хуже MLP_Ranker
    DCNUserItemRanker,
    ListwiseTransformer
]
```

```{python}
results = evaluate_losses(model_classes, losses)
print(results)
```

```{python}
cindex_table = results.pivot_table(
    index='loss_key',
    columns='model_cls',
    values='C-index',
    aggfunc='first'
).reset_index()

ndcg_table = results.pivot_table(
    index='loss_key',
    columns='model_cls',
    values='NDCG@3',
    aggfunc='first'
).reset_index()

result = pd.concat([cindex_table, ndcg_table], axis=1).reset_index()
print(result)
```

```{python}
from pathlib import Path
from openpyxl import load_workbook

excel_path = Path(__file__).parent.parent / "4. Output" / "01. Python_models.xlsx"
mode = 'a' if excel_path.exists() else 'w'
new_sheet_name = 'learn_to_rank'

if mode == 'a':
    wb = load_workbook(excel_path)
    if new_sheet_name in wb.sheetnames:
        raise ValueError(f"Лист '{new_sheet_name}' уже есть в файле {excel_path}")

with pd.ExcelWriter(excel_path, engine='openpyxl', mode=mode) as writer:
    result.to_excel(writer, sheet_name=new_sheet_name, index=False)
```
