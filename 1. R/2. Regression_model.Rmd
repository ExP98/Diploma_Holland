---
title: "Регрессионные модели"
author: "Egor Glushkov"
---

Идеи: сделать общий интерфейс (ООП?) для всех моделей + для chain- и indep-регрессоров


# 1. Библиотеки и функции
```{r include=FALSE}
library(R6)
library(dotty)
library(xgboost)
library(lightgbm)
library(randomForest)
library(catboost)
library(caret)

set.seed(42)
source(paste0(here::here(), "/1. R/4. data_preparation.R"))
```

Функции:
```{r}
my_rmse <- \(y1, y2) sqrt(sum((y1 - y2)^2) / length(y1))
cosine_sim <- \(y1, y2) (sum(y1 * y2)) / (sqrt(sum(y1 ^ 2)) * sqrt(sum(y2 ^ 2)))
cosine_dist <- \(y1, y2) sqrt(2 * (1 - cosine_sim(y1, y2)))


smart_integer_round <- function(six_vals) {
  modif <- six_vals * 42 / sum(six_vals) + 10*.Machine$double.eps
  resid <- 42 - sum(round(modif))
  
  ind_to_change <- order(abs(modif - round(modif)), decreasing = TRUE)[1:abs(resid)]
  modif[ind_to_change] <- modif[ind_to_change] + sign(resid) * 1
  
  int_rnd_values <- round(modif)
  if (sum(int_rnd_values) != 42) warning(str_glue("Sum of {toString(int_rnd_values)} != 42. Input: {toString(six_vals)}. \n"))
  return(int_rnd_values)
}


prediction_correction <- function(preds) {
  correct_preds <- preds %>%
    as.data.table() %>% 
    apply(., 1, smart_integer_round) %>% 
    t()
  return(correct_preds)
}


# sapply(1:nrow(pred_test), \(i) smart_integer_round(pred_test[i, ])) %>% colSums()

show_custom_metrics <- function(my_pred, case_name) {
  aRMSE_ <- sapply(1:nrow(Y_test), \(i) my_rmse(my_pred[i, ], Y_test[i, ])) %>% mean()
  aCosDist_ <- sapply(1:nrow(Y_test), \(i) cosine_dist(my_pred[i, ], Y_test[i, ])) %>% mean()
  msg <- str_glue("{case_name}. aRMSE: {round(aRMSE_, 3)}; aCosDist: {round(aCosDist_, 3)}")
  return(msg)
}
```


```{r}
perform_indep_multioutput_regression <- function(model_class, print_msg = "", 
                                                 X_train_, Y_train_, X_test_, Y_test_,
                                                 print_all = TRUE) {
  models <- vector("list", 6)
  for (col_i in 1:6) {
    models[[col_i]] <- model_class$new(X_train_, Y_train_[, col_i], X_test_, Y_test_[, col_i])
    if (print_all) print(models[[col_i]]$rmse_test)
  }
  
  ind_pred <- sapply(models, \(x) x$pred_test) %>% prediction_correction()
  print(show_custom_metrics(ind_pred, paste0(print_msg, " Indep")))
  return(invisible(ind_pred))
}


perform_chain_multioutput_regression <- function(model_class, print_msg = "", 
                                                 X_train_, Y_train_, X_test_, Y_test_,
                                                 print_all = TRUE) {
  models <- vector("list", 6)
  
  cbind_X_train <- X_train_
  cbind_X_test <- X_test_
  
  for (col_i in 1:6) {
    if (col_i != 1) {
      cbind_X_train <- cbind(cbind_X_train, col_i = models[[col_i-1]]$pred_train)
      cbind_X_test <- cbind(cbind_X_test, col_i = models[[col_i-1]]$pred_test)
      colnames(cbind_X_train)[ncol(cbind_X_train)] <- paste0("output_", col_i)
      colnames(cbind_X_test)[ncol(cbind_X_test)] <- paste0("output_", col_i)
    }
    models[[col_i]] <- model_class$new(cbind_X_train, Y_train_[, col_i], cbind_X_test, Y_test_[, col_i])
    if (print_all) print(models[[col_i]]$rmse_test)
  }
  
  chain_pred <- sapply(models, \(x) x$pred_test) %>% prediction_correction()
  print(show_custom_metrics(chain_pred, paste0(print_msg, " Chained")))
  return(invisible(chain_pred))
}
```



# 2. Данные

Разделение данных на трейн и тест:
```{r}
features <- wide_data %>% 
  copy() %>% 
  .[, .SD, .SDcols = !(names(wide_data) %like% "HL|HL2|id")] %>% 
  as.matrix()

targets <- wide_data %>% 
  copy() %>% 
  .[, .SD, .SDcols = names(wide_data) %like% "HL_"] %>% 
  as.matrix()

split_idx <- sample(c(TRUE, FALSE), nrow(features), replace = TRUE, prob = c(0.8, 0.2))

.[X_train_unscaled, X_test_unscaled] <- list(features[split_idx, ], features[!split_idx, ])
.[Y_train, Y_test] <- list(targets[split_idx, ], targets[!split_idx, ])

mean_train <- apply(X_train_unscaled, 2, mean, na.rm = TRUE)
sd_train <- apply(X_train_unscaled, 2, sd, na.rm = TRUE)

X_train <- scale(X_train_unscaled, center = mean_train, scale = sd_train)
X_test  <- scale(X_test_unscaled, center = mean_train, scale = sd_train)
```


# 3. Модель

```{r}
my_template_model <- R6Class(
  classname = "my_template_model",
  public = list(
    model = NULL,
    importance = NULL,
    pred_train = NULL,
    pred_test = NULL,
    rmse_test = NA,
    
    calc_importance = function() {
      self$importance <- NULL
      return(self$importance)
    },
    
    initialize = function(X_train_, y_train_, X_test_ = NULL, y_test_ = NULL, ...) {
      private$fit(X_train_, y_train_, ...)
      self$pred_train <- private$predict(X_train_)
      
      if (!is.null(X_test_)) self$pred_test <- private$predict(X_test_, is_test = TRUE)
      if (!is.null(y_test_) && !is.null(self$pred_test)) private$calc_rmse(y_test_, self$pred_test)
      
      return(invisible(self))
    }
  ),
  
  private = list(
    calc_rmse = function(y_test_, y_preds = self$pred_test) {
      self$rmse_test <- my_rmse(y_test_, self$pred_test)
      return(invisible(self))
    },
    
    fit = function(X_train_, y_train_, ...) {
      self$model <- NULL
      return(invisible(self))
    },
    
    predict = function(X_, is_test = FALSE) {
      preds <- predict(self$model, X_)
      if (is_test) self$pred_test <- preds
      return(preds)
    }
  )
)
```


## 3.1 Константный предсказатель
```{r}
# const_pred <- matrix(rep(rep(7, times = 6), each = nrow(Y_test)), nrow = nrow(Y_test))
const_pred <- matrix(rep(colMeans(Y_train), each = nrow(Y_train)), nrow = nrow(Y_train))
show_custom_metrics(const_pred, "Константное")
```


## 3.2 XGBoost

### Число раундов
Как выглядит модель, сколько раундов требуется:
```{r}
ii <- 1

res <- vector("list", 20)
for (i in 1:20) {
  xgb_model <- xgboost(data = X_train, label = Y_train[, ii],
                       nrounds = i, objective = "reg:squarederror", eval_metric = "rmse", verbose = 0)
  
  pred_y <- predict(xgb_model, X_test)
  res[[i]] <- data.frame(
    nrounds = i,
    test_rmse = my_rmse(Y_test[, ii], pred_y),
    test_round_rmse = my_rmse(Y_test[, ii], pred_y %>% round())
  )
}

res %>% rbindlist() %>% .[["test_rmse"]] %>% plot(main = "test_rmse")
res %>% rbindlist() %>% .[["test_round_rmse"]] %>% plot(main = "test_round_rmse")
```
15 раундов хватает.


```{r}
my_XGBoost_model <- R6Class(
  classname = "my_XGBoost_model",
  inherit = my_template_model,
  
  public = list(
    calc_importance = function() {
      self$importance <- xgb.importance(model = self$model)
      return(self$importance)
    }
  ),
  
  private = list(
    fit = function(X_train_, y_train_, ...) {
      self$model <- xgboost(data = X_train_, label = y_train_, nrounds = 15, verbose = 0,
                            objective = "reg:squarederror", eval_metric = "rmse", ...)
      return(invisible(self))
    }
  )
)
```


```{r}
perform_indep_multioutput_regression(my_XGBoost_model, "XGBoost", X_train, Y_train, X_test, Y_test)
perform_chain_multioutput_regression(my_XGBoost_model, "XGBoost", X_train, Y_train, X_test, Y_test)
```

```{r}
lapply(indep_xgb, \(x) x$calc_importance())
```
Для XGBoost (и позже для Random Forest) самая важная фича -- CT_1 (открытость-замкнутость)

Порядок предсказания может иметь значение!


## 3.3 LightGBM

```{r}
my_LightGBM_model <- R6Class(
  classname = "my_LightGBM_model",
  inherit = my_template_model,
  
  public = list(
    initialize = function(X_train_, y_train_, X_test_, y_test_) {
      X_train_mat_ <- data.matrix(X_train_)
      X_test_mat_ <- data.matrix(X_test_)
      
      dtrain <- lgb.Dataset(data = X_train_mat_, label = y_train_, free_raw_data = FALSE)
      dtest <- lgb.Dataset.create.valid(dtrain, data = X_test_mat_, label = y_test_)
        
      private$fit(dtrain, dtest)
      self$pred_train <- private$predict(X_train_mat_)
      
      if (!is.null(X_test_)) self$pred_test <- private$predict(X_test_mat_, is_test = TRUE)
      if (!is.null(y_test_) && !is.null(self$pred_test)) private$calc_rmse(y_test_, self$pred_test)
      
      return(invisible(self))
    }
  ),
  
  private = list(
    fit = function(dtrain, dtest) {
      self$model <- lgb.train(
        params = list(objective = "regression", metric = "rmse", num_iterations = 250),
        data = dtrain,
        valids = list(train = dtrain, eval = dtest),
        verbose = -1
      )
      return(invisible(self))
    }
  )
)
```

```{r}
perform_indep_multioutput_regression(my_LightGBM_model, "LightGBM", X_train, Y_train, X_test, Y_test)
perform_chain_multioutput_regression(my_LightGBM_model, "LightGBM", X_train, Y_train, X_test, Y_test)
```


### Cross Validation

```{r}
my_LightGBM_CV_model <- R6Class(
  classname = "my_LightGBM_CV_model",
  inherit = my_template_model,
  
  public = list(
    initialize = function(X_train_, y_train_, X_test_, y_test_) {
      X_train_mat_ <- data.matrix(X_train_)
      X_test_mat_ <- data.matrix(X_test_)
      
      dtrain <- lgb.Dataset(data = X_train_mat_, label = y_train_, free_raw_data = FALSE)
      private$fit(dtrain)
      self$pred_train <- private$predict(X_train_mat_)
      
      if (!is.null(X_test_)) self$pred_test <- private$predict(X_test_mat_, is_test = TRUE)
      if (!is.null(y_test_) && !is.null(self$pred_test)) private$calc_rmse(y_test_, self$pred_test)
      
      return(invisible(self))
    }
  ),
  
  private = list(
    fit = function(dtrain) {
      self$model <- lgb.cv(
        params = list(objective = "regression", metric = "rmse"), # , seed = 43, deterministic = TRUE
        data = dtrain,
        nfold = 5L,
        verbose = -1
      )
      return(invisible(self))
    },
    
    predict = function(X_, is_test = FALSE) {
      calc_mean_matrix <- function(mat_list) Reduce('+', mat_list) / length(mat_list)

      preds <- lapply(self$model$boosters, \(cv_mdl) predict(cv_mdl, X_)$booster) %>% calc_mean_matrix()
      if (is_test) self$pred_test <- preds
      return(preds)
    }
  )
)
```

```{r}
perform_indep_multioutput_regression(my_LightGBM_CV_model, "LightGBM CV", X_train, Y_train, X_test, Y_test)
perform_chain_multioutput_regression(my_LightGBM_CV_model, "LightGBM CV", X_train, Y_train, X_test, Y_test)
```


## 3.4 Random Forest

```{r}
my_RandomForest_model <- R6Class(
  classname = "my_RandomForest_model",
  inherit = my_template_model,
  
  public = list(
    calc_importance = function() {
      self$importance <- self$model$importance
      return(self$importance)
    }
  ),
  
  private = list(
    fit = function(X_train_, y_train_) {
      # na.action = na.omit,
      self$model <- randomForest(x = X_train_, y = y_train_, ntree = 1000, importance = TRUE) 
      return(invisible(self))
    }
  )
)
```

```{r}
perform_indep_multioutput_regression(my_RandomForest_model, "Random Forest", X_train, Y_train, X_test, Y_test)
perform_chain_multioutput_regression(my_RandomForest_model, "Random Forest", X_train, Y_train, X_test, Y_test)
```


## 3.5 lm

```{r}
my_lm_model <- R6Class(
  classname = "my_lm_model",
  inherit = my_template_model,
  
  public = list(
    initialize = function(X_train_, y_train_, X_test_ = NULL, y_test_ = NULL, ...) {
      X_train_ <- as.data.frame(X_train_)
      X_test_  <- as.data.frame(X_test_)
      
      private$fit(X_train_, y_train_, ...)
      self$pred_train <- private$predict(X_train_)
      
      if (!is.null(X_test_)) self$pred_test <- private$predict(X_test_, is_test = TRUE)
      if (!is.null(y_test_) && !is.null(self$pred_test)) private$calc_rmse(y_test_, self$pred_test)
      
      return(invisible(self))
    }
  ),
  
  private = list(
    fit = function(X_train_, y_train_) {
      self$model  <- lm(target ~ ., data = data.frame(X_train_, target = y_train_))
      return(invisible(self))
    }
  )
)
```

```{r}
lm_model <- lm(cbind(HL_1, HL_2, HL_3, HL_4, HL_5, HL_6) ~ ., 
               data = data.frame(X_train, Y_train)
               # , na.action = na.omit
               )

lm_ind_pred <- predict(lm_model, newdata = X_test %>% as.data.frame()) %>% prediction_correction()
show_custom_metrics(lm_ind_pred, "lm")
```

```{r}
perform_indep_multioutput_regression(my_lm_model, "lm", X_train, Y_train, X_test, Y_test)
perform_chain_multioutput_regression(my_lm_model, "lm", X_train, Y_train, X_test, Y_test)
```


## 3.6 CatBoost

```{r}
my_Catboost_model <- R6Class(
  classname = "my_Catboost_model",
  inherit = my_template_model,
  
  public = list(
    initialize = function(X_train_, y_train_, X_test_, y_test_) {
      train_pool <- catboost.load_pool(data = X_train_, label = y_train_)
      test_pool  <- catboost.load_pool(data = X_test_,  label = y_test_)
        
      private$fit(train_pool, test_pool)
      self$pred_train <- private$predict(train_pool)
      
      if (!is.null(X_test_)) self$pred_test <- private$predict(test_pool, is_test = TRUE)
      if (!is.null(y_test_) && !is.null(self$pred_test)) private$calc_rmse(y_test_, self$pred_test)
      
      return(invisible(self))
    }
  ),
  
  private = list(
    fit = function(train_pool, test_pool) {
      self$model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = list(loss_function = 'RMSE', logging_level = "Silent")
      )
      return(invisible(self))
    },
    
    predict = function(X_, is_test = FALSE) {
      preds <- catboost.predict(self$model, X_)
      if (is_test) self$pred_test <- preds
      return(preds)
    }
  )
)
```

```{r}
perform_indep_multioutput_regression(my_Catboost_model, "CatBoost", X_train, Y_train, X_test, Y_test)
perform_chain_multioutput_regression(my_Catboost_model, "CatBoost", X_train, Y_train, X_test, Y_test)
```


# 4. PCA

```{r}
pca_model <- prcomp(X_train, center = TRUE, scale. = TRUE)

# cumsum(pca_model$sdev^2 / sum(pca_model$sdev^2))
data.table(
  k = 1:length(pca_model$sdev),
  ssq = pca_model$sdev^2
) %>% 
  .[, cumsum := cumsum(ssq / sum(ssq))] %>% 
  .[]

plot(pca_model$sdev^2 / sum(pca_model$sdev^2), 
     type = "b", 
     xlab = "Главные компоненты", 
     ylab = "Доля объясненной дисперсии")
```

```{r}
pca_scaler <- function(X_df, pca_model_, k = 32) {
  res <- scale(X_df, center = pca_model_$center, scale = pca_model_$scale) %*% 
    pca_model_$rotation %>% 
    .[, 1:k]
  return(res)
}
```

## lm

Зависимость k (~PCA) и результатов lm:
```{r}
check_k_pca_lm <- function(k) {
  X_train_pca_reduced <- pca_scaler(X_train, pca_model, k)
  X_test_pca_reduced  <- pca_scaler(X_test,  pca_model, k)
  
  lm_model <- lm(cbind(HL_1, HL_2, HL_3, HL_4, HL_5, HL_6) ~ ., 
                 data = data.frame(X_train_pca_reduced, Y_train))
  
  lm_pred <- predict(lm_model, newdata = data.frame(X_test_pca_reduced, Y_test)) %>% 
    apply(., 1, smart_integer_round) %>% 
    t()
  
  aRMSE_ <- sapply(1:nrow(Y_test), 
                   \(i) my_rmse(lm_pred[i, ], Y_test[i, ])) %>% mean()
  return(c(k = k, aRMSE = aRMSE_))
}

res_k_lm <- sapply(1:ncol(X_train), check_k_pca_lm) %>% 
  t() %>% 
  as.data.table()

plotly::plot_ly(res_k_lm, x = ~k, y = ~aRMSE, type = 'scatter', mode = 'lines+markers')
```

```{r}
best_k <- res_k_lm[aRMSE == min(aRMSE), k]
X_train_pca_reduced <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca_reduced  <- pca_scaler(X_test,  pca_model, k = best_k)

lm_model <- lm(cbind(HL_1, HL_2, HL_3, HL_4, HL_5, HL_6) ~ ., 
               data = data.frame(X_train_pca_reduced, Y_train))

lm_pca_pred <- predict(lm_model, newdata = data.frame(X_test_pca_reduced, Y_test)) %>%
  prediction_correction()
show_custom_metrics(lm_pca_pred, "PCA + lm")
```


## Random Forest

Подбор лучшего k для PCA + Random Forest
```{r}
check_k_pca_RF <- function(k) {
  X_train_pca_reduced <- pca_scaler(X_train, pca_model, k)
  X_test_pca_reduced  <- pca_scaler(X_test,  pca_model, k)
  
  models <- list()
  
  for (col_i in 1:6) {
    models[[col_i]] <- randomForest(
      x = X_train_pca_reduced,
      y = Y_train[, col_i],
      ntree = 1000
    )
  }
  
  rf_pred <- sapply(1:6, \(i) predict(models[[i]], data.frame(X_test_pca_reduced, Y_test))) %>% 
    prediction_correction()
  
  aRMSE_ <- sapply(1:nrow(Y_test), 
                   \(i) my_rmse(rf_pred[i, ], Y_test[i, ])) %>% mean()
  return(c(k = k, aRMSE = aRMSE_))
}

res_k_rf <- sapply(seq(2, ncol(X_train), by = 3), check_k_pca_RF) %>% 
  t() %>% 
  as.data.table()

plotly::plot_ly(res_k_rf, x = ~k, y = ~aRMSE, type = 'scatter', mode = 'lines+markers')
```

```{r}
best_k <- res_k_rf[aRMSE == min(aRMSE), k]
X_train_pca_reduced <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca_reduced  <- pca_scaler(X_test,  pca_model, k = best_k)
  
rf_pred <- vector("list", 6)
for (col_i in 1:6) {
  models[[col_i]] <- randomForest(
    x = X_train_pca_reduced,
    y = Y_train[, col_i],
    ntree = 1000
  )
  
  rf_pred[[col_i]] <- predict(models[[col_i]], data.frame(X_test_pca_reduced, Y_test))
}

rf_ind_pca_pred <- rf_pred %>% prediction_correction()
show_custom_metrics(rf_ind_pca_pred, "PCA + Random Forest")
```


## XGBoost
```{r}
indep_pred <- vector("list", 6)
for (col_i in 1:6) {
  xgb_model <- xgboost(data = X_train_pca_reduced, label = Y_train[, col_i],
                       nrounds = 15, objective = "reg:squarederror", eval_metric = "rmse",
                       verbose = 0)
  
  indep_pred[[col_i]] <- predict(xgb_model, X_test_pca_reduced)
}

xgb_pca_ind_pred <- indep_pred %>% prediction_correction()
show_custom_metrics(xgb_pca_ind_pred, "PCA + XGBoost независимое")
```


## Catboost

```{r}
indep_catb_pred <- vector("list", 6)
for (col_i in 1:6) {
  train_pool <- catboost.load_pool(data = X_train_pca_reduced, label = Y_train[, col_i])
  test_pool  <- catboost.load_pool(data = X_test_pca_reduced,  label = Y_test[, col_i])
      
  catb_model <- catboost.train(
    learn_pool = train_pool,
    test_pool = test_pool,
    params = list(loss_function = 'RMSE', logging_level = "Silent")
  )
  
  indep_catb_pred[[col_i]] <- catboost.predict(catb_model, test_pool)
}

catb_pca_ind_pred <- indep_catb_pred %>% prediction_correction()
show_custom_metrics(catb_pca_ind_pred, "PCA + Catboost независимое")
```


# 5. Multilabel classification

```{r}
bool_mask_row <- function(matrix_row) {
  ind <- matrix_row %>% order(decreasing = TRUE) %>% .[1:3]
  b_vec <- rep(FALSE, 6)
  b_vec[ind] <- TRUE
  return(b_vec)
}

targets_bool <- apply(targets, 1, bool_mask_row) %>% t()
.[Y_b_train, Y_b_test] <- list(targets_bool[split_idx, ], targets_bool[!split_idx, ])
```

```{r}
library(arrow)

targets_bool %>% 
  as.data.table() %>% 
  rename_all(~colnames(targets)) %>% 
  arrow::write_feather("../0. Data/Y_bool.feather")
```


# 6. Python

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor, RegressorChain
# from sklearn.metrics import root_mean_squared_error

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor 
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
```

```{python}
def rmse(y_true, y_pred):
    mean_squared_error = np.mean((y_true - y_pred) ** 2)  # Среднее значение квадратов ошибок
    return np.sqrt(mean_squared_error)


def check_MO_regression(model_obj, model_name=""):
    MO_regr = {'Indep': MultiOutputRegressor, 'Chain': RegressorChain}
    for name, model in MO_regr.items():
        y_pred = model(model_obj).fit(X_train, y_train).predict(X_test)
        print(f"{model_name} {name}. RMSE: {round(rmse(y_test, y_pred), 3)}")
    return(None)


def plot_nn_losses(hist):
    plt.plot(hist.history['loss'], label='Train Loss')
    plt.plot(hist.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
    return(None)
```

```{python}
data = pd.read_feather("../0. Data/wide_data.feather")

X = data.loc[:, ~data.columns.isin(["HL_1", "HL_2", "HL_3", "HL_4", "HL_5", "HL_6", "id"])]
y = data.loc[:, "HL_1":"HL_6"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

y_bool = pd.read_feather("../0. Data/Y_bool.feather")

y_b_train, y_b_test = train_test_split(y_bool, test_size=0.2, random_state=42)
```

```{python}
linear = LinearRegression().fit(X_train, y_train)
rf = RandomForestRegressor().fit(X_train, y_train)
etr = ExtraTreesRegressor().fit(X_train, y_train)
knn = KNeighborsRegressor().fit(X_train, y_train)

print(f"Linear. RMSE: {rmse(y_test, linear.predict(X_test))}")
print(f"ExtraTree. RMSE: {rmse(y_test, etr.predict(X_test))}")
print(f"Random Forest. RMSE: {rmse(y_test, rf.predict(X_test))}")
print(f"kNN. RMSE: {rmse(y_test, knn.predict(X_test))}")
```

