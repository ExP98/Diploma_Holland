---
title: "Регрессионные модели"
author: "Egor Glushkov"
---

# 1. Библиотеки и функции
```{r include=FALSE}
library(R6)
library(dotty)
library(xgboost)
library(lightgbm)
library(randomForest)
library(catboost)
library(caret)
library(plotly)
library(arrow)

set.seed(42)
source(paste0(here::here(), "/1. R/4. data_preparation.R"))
```

Функции:
```{r}
my_rmse <- \(y1, y2) sqrt(sum((y1 - y2)^2) / length(y1))
cosine_sim <- \(y1, y2) (sum(y1 * y2)) / (sqrt(sum(y1 ^ 2)) * sqrt(sum(y2 ^ 2)))
cosine_dist <- \(y1, y2) sqrt(2 * (1 - cosine_sim(y1, y2)))


smart_integer_round <- function(six_vals) {
  modif <- six_vals * 42 / sum(six_vals) + 10*.Machine$double.eps
  resid <- 42 - sum(round(modif))
  
  ind_to_change <- order(abs(modif - round(modif)), decreasing = TRUE)[1:abs(resid)]
  modif[ind_to_change] <- modif[ind_to_change] + sign(resid) * 1
  
  int_rnd_values <- round(modif)
  if (sum(int_rnd_values) != 42) warning(str_glue("Sum of {toString(int_rnd_values)} != 42. Input: {toString(six_vals)}. \n"))
  return(int_rnd_values)
}


prediction_correction <- function(preds) {
  correct_preds <- preds %>%
    as.data.table() %>% 
    apply(., 1, smart_integer_round) %>% 
    t()
  return(correct_preds)
}


# sapply(1:nrow(pred_test), \(i) smart_integer_round(pred_test[i, ])) %>% colSums()

show_custom_metrics <- function(my_pred, case_name, Y_test_ = Y_test) {
  aRMSE_ <- sapply(1:nrow(Y_test_), \(i) my_rmse(my_pred[i, ], Y_test_[i, ])) %>% mean()
  aCosDist_ <- sapply(1:nrow(Y_test_), \(i) cosine_dist(my_pred[i, ], Y_test_[i, ])) %>% mean()
  msg <- str_glue("{case_name}. aRMSE: {round(aRMSE_, 3)}; aCosDist: {round(aCosDist_, 3)}")
  return(msg)
}


pca_scaler <- function(X_df, pca_model_, k = 32) {
  res <- scale(X_df, center = pca_model_$center, scale = pca_model_$scale) %*% 
    pca_model_$rotation %>% 
    .[, 1:k]
  return(res)
}


scalar_matrix_production <- function(w_vec, mat_list) {
  return((map2(w_vec, mat_list, `*`) %>% reduce(`+`)) / sum(w_vec))
}


prepare_psytest_dataset <- function(wide_data_, wide_data2_, 
                                    psytest_label = c("BF", "EY", "CT", "LN", "SC"),
                                    use_PCA_b = FALSE) {
  psytest_label <- match.arg(psytest_label)
  
  wide_data_ <- wide_data_ %>% copy() %>% 
    .[, .SD, .SDcols = patterns(paste0(psytest_label, "|HL"))] %>% 
    drop_na()
  
  wide_data2_ <- wide_data2_ %>% copy() %>% 
    .[, .SD, .SDcols = patterns(paste0(psytest_label, "|HL"))] %>% 
    drop_na()
  
  X_train_unsc <- wide_data2_ %>% .[, .SD, .SDcols = patterns(psytest_label)]
  X_test_unsc  <- wide_data_  %>% .[, .SD, .SDcols = patterns(psytest_label)]
  
  Y_train <- wide_data2_ %>% .[, .SD, .SDcols = patterns("HL_")] %>% as.matrix()
  Y_test  <- wide_data_  %>% .[, .SD, .SDcols = patterns("HL_")] %>% as.matrix()
  
  mean_train <- apply(X_train_unsc, 2, mean, na.rm = TRUE)
  sd_train <- apply(X_train_unsc, 2, sd, na.rm = TRUE)
  
  X_train <- scale(X_train_unsc, center = mean_train, scale = sd_train)
  X_test  <- scale(X_test_unsc, center = mean_train, scale = sd_train)
  
  if (use_PCA_b) {
    pca_model <- prcomp(X_train, center = TRUE, scale. = TRUE)
    opt_k <- data.table(k = 1:length(pca_model$sdev),
                        ssq = pca_model$sdev^2) %>% 
      .[, cumsum := cumsum(ssq / sum(ssq))] %>% 
      .[, limit_b := cumsum >= 0.85] %>% 
      .[limit_b == TRUE, min(k)]
    
    X_train <- pca_scaler(X_train, pca_model, opt_k)
    X_test <- pca_scaler(X_test, pca_model, opt_k)
  }
  
  n_train <- nrow(X_train)
  n_test <- nrow(X_test)
  
  print(str_glue("Train: {n_train} rows; test: {n_test} rows ({round(n_test / (n_train + n_test) * 100, 2)}%)."))
  if (nrow(X_train) == 0 | nrow(X_test) == 0) error("X_train or X_test are empty!")
  
  return(list(X_train = X_train, Y_train = Y_train, X_test = X_test, Y_test = Y_test))
}
```


```{r}
perform_indep_multioutput_regression <- function(model_class, print_msg = "", 
                                                 X_train_, Y_train_, X_test_, Y_test_,
                                                 print_metric = TRUE) {
  models <- vector("list", 6)
  for (col_i in 1:6) {
    models[[col_i]] <- model_class$new(X_train_, Y_train_[, col_i], X_test_, Y_test_[, col_i])
    # print(models[[col_i]]$rmse_test)
  }
  
  ind_pred <- sapply(models, \(x) x$pred_test) %>% prediction_correction()
  if (print_metric) print(show_custom_metrics(ind_pred, paste0(print_msg, " Indep"), Y_test_ = Y_test_))
  return(invisible(ind_pred))
}


perform_chain_multioutput_regression <- function(model_class, print_msg = "", 
                                                 X_train_, Y_train_, X_test_, Y_test_,
                                                 print_metric = TRUE) {
  models <- vector("list", 6)
  
  cbind_X_train <- X_train_
  cbind_X_test <- X_test_
  
  for (col_i in 1:6) {
    if (col_i != 1) {
      cbind_X_train <- cbind(cbind_X_train, col_i = models[[col_i-1]]$pred_train)
      cbind_X_test <- cbind(cbind_X_test, col_i = models[[col_i-1]]$pred_test)
      colnames(cbind_X_train)[ncol(cbind_X_train)] <- paste0("output_", col_i)
      colnames(cbind_X_test)[ncol(cbind_X_test)] <- paste0("output_", col_i)
    }
    models[[col_i]] <- model_class$new(cbind_X_train, Y_train_[, col_i], cbind_X_test, Y_test_[, col_i])
    # print(models[[col_i]]$rmse_test)
  }
  
  chain_pred <- sapply(models, \(x) x$pred_test) %>% prediction_correction()
  if (print_metric) print(show_custom_metrics(chain_pred, paste0(print_msg, " Chained"), Y_test_ = Y_test_))
  return(invisible(chain_pred))
}
```


# 2. Данные

Разделение данных на трейн и тест:
```{r}
features <- wide_data %>% 
  copy() %>% 
  .[, .SD, .SDcols = !(names(wide_data) %like% "HL|HL2|id")] %>% 
  as.matrix()

targets <- wide_data %>% 
  copy() %>% 
  .[, .SD, .SDcols = names(wide_data) %like% "HL_"] %>% 
  as.matrix()

split_idx <- sample(c(TRUE, FALSE), nrow(features), replace = TRUE, prob = c(0.8, 0.2))

.[X_train_unscaled, X_test_unscaled] <- list(features[split_idx, ], features[!split_idx, ])
.[Y_train, Y_test] <- list(targets[split_idx, ], targets[!split_idx, ])

mean_train <- apply(X_train_unscaled, 2, mean, na.rm = TRUE)
sd_train <- apply(X_train_unscaled, 2, sd, na.rm = TRUE)

# X_train <- X_train_unscaled
# X_test <- X_test_unscaled

X_train <- scale(X_train_unscaled, center = mean_train, scale = sd_train)
X_test  <- scale(X_test_unscaled, center = mean_train, scale = sd_train)
```


# 3. Модель

```{r}
my_template_model <- R6Class(
  classname = "my_template_model",
  public = list(
    model = NULL,
    importance = NULL,
    pred_train = NULL,
    pred_test = NULL,
    rmse_test = NA,
    
    calc_importance = function() {
      self$importance <- NULL
      return(self$importance)
    },
    
    initialize = function(X_train_, y_train_, X_test_ = NULL, y_test_ = NULL, ...) {
      private$fit(X_train_, y_train_, ...)
      self$pred_train <- private$predict(X_train_)
      
      if (!is.null(X_test_)) self$pred_test <- private$predict(X_test_, is_test = TRUE)
      if (!is.null(y_test_) && !is.null(self$pred_test)) private$calc_rmse(y_test_, self$pred_test)
      
      return(invisible(self))
    }
  ),
  
  private = list(
    calc_rmse = function(y_test_, y_preds = self$pred_test) {
      self$rmse_test <- my_rmse(y_test_, self$pred_test)
      return(invisible(self))
    },
    
    fit = function(X_train_, y_train_, ...) {
      self$model <- NULL
      return(invisible(self))
    },
    
    predict = function(X_, is_test = FALSE) {
      preds <- predict(self$model, X_)
      if (is_test) self$pred_test <- preds
      return(preds)
    }
  )
)
```


## 3.1 Константный предсказатель
```{r}
# const_pred <- matrix(rep(rep(7, times = 6), each = nrow(Y_test)), nrow = nrow(Y_test))
const_pred <- matrix(rep(colMeans(Y_train), each = nrow(Y_test)), nrow = nrow(Y_test))
show_custom_metrics(const_pred, "Константное")
```


## 3.2 XGBoost

### Число раундов
Как выглядит модель, сколько раундов требуется:
```{r}
ii <- 1

res <- vector("list", 20)
for (i in 1:20) {
  xgb_model <- xgboost(data = X_train, label = Y_train[, ii],
                       nrounds = i, objective = "reg:squarederror", eval_metric = "rmse", verbose = 0)
  
  pred_y <- predict(xgb_model, X_test)
  res[[i]] <- data.frame(
    nrounds = i,
    test_rmse = my_rmse(Y_test[, ii], pred_y),
    test_round_rmse = my_rmse(Y_test[, ii], pred_y %>% round())
  )
}

res %>% rbindlist() %>% .[["test_rmse"]] %>% plot(main = "test_rmse")
res %>% rbindlist() %>% .[["test_round_rmse"]] %>% plot(main = "test_round_rmse")
```
15 раундов хватает.


```{r}
my_XGBoost_model <- R6Class(
  classname = "my_XGBoost_model",
  inherit = my_template_model,
  
  public = list(
    calc_importance = function() {
      self$importance <- xgb.importance(model = self$model)
      return(self$importance)
    }
  ),
  
  private = list(
    fit = function(X_train_, y_train_, ...) {
      self$model <- xgboost(data = X_train_, label = y_train_, nrounds = 15, verbose = 0,
                            objective = "reg:squarederror", eval_metric = "rmse", ...)
      return(invisible(self))
    }
  )
)
```


```{r}
.[xgb_ind_pred, xgb_chain_pred] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_XGBoost_model, "XGBoost", X_train, Y_train, X_test, Y_test
)

# perform_indep_multioutput_regression(my_XGBoost_model, "XGBoost", X_train, Y_train, X_test, Y_test)
# perform_chain_multioutput_regression(my_XGBoost_model, "XGBoost", X_train, Y_train, X_test, Y_test)
```

```{r}
lapply(indep_xgb, \(x) x$calc_importance())
```
Для XGBoost (и позже для Random Forest) самая важная фича -- CT_1 (открытость-замкнутость)

Порядок предсказания может иметь значение!


## 3.3 LightGBM

```{r}
my_LightGBM_model <- R6Class(
  classname = "my_LightGBM_model",
  inherit = my_template_model,
  
  public = list(
    initialize = function(X_train_, y_train_, X_test_, y_test_) {
      X_train_mat_ <- data.matrix(X_train_)
      X_test_mat_ <- data.matrix(X_test_)
      
      dtrain <- lgb.Dataset(data = X_train_mat_, label = y_train_, free_raw_data = FALSE)
      dtest <- lgb.Dataset.create.valid(dtrain, data = X_test_mat_, label = y_test_)
        
      private$fit(dtrain, dtest)
      self$pred_train <- private$predict(X_train_mat_)
      
      if (!is.null(X_test_)) self$pred_test <- private$predict(X_test_mat_, is_test = TRUE)
      if (!is.null(y_test_) && !is.null(self$pred_test)) private$calc_rmse(y_test_, self$pred_test)
      
      return(invisible(self))
    }
  ),
  
  private = list(
    fit = function(dtrain, dtest) {
      self$model <- lgb.train(
        params = list(objective = "regression", metric = "rmse", num_iterations = 250),
        data = dtrain,
        valids = list(train = dtrain, eval = dtest),
        verbose = -1
      )
      return(invisible(self))
    }
  )
)
```

```{r}
.[lgbm_ind_pred, lgbm_chain_pred] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_LightGBM_model, "LightGBM", X_train, Y_train, X_test, Y_test
)
```


### Cross Validation

```{r}
my_LightGBM_CV_model <- R6Class(
  classname = "my_LightGBM_CV_model",
  inherit = my_template_model,
  
  public = list(
    initialize = function(X_train_, y_train_, X_test_, y_test_) {
      X_train_mat_ <- data.matrix(X_train_)
      X_test_mat_ <- data.matrix(X_test_)
      
      dtrain <- lgb.Dataset(data = X_train_mat_, label = y_train_, free_raw_data = FALSE)
      private$fit(dtrain)
      self$pred_train <- private$predict(X_train_mat_)
      
      if (!is.null(X_test_)) self$pred_test <- private$predict(X_test_mat_, is_test = TRUE)
      if (!is.null(y_test_) && !is.null(self$pred_test)) private$calc_rmse(y_test_, self$pred_test)
      
      return(invisible(self))
    }
  ),
  
  private = list(
    fit = function(dtrain) {
      self$model <- lgb.cv(
        params = list(objective = "regression", metric = "rmse"), # , seed = 43, deterministic = TRUE
        data = dtrain,
        nfold = 5L,
        verbose = -1
      )
      return(invisible(self))
    },
    
    predict = function(X_, is_test = FALSE) {
      calc_mean_matrix <- function(mat_list) Reduce('+', mat_list) / length(mat_list)

      preds <- lapply(self$model$boosters, \(cv_mdl) predict(cv_mdl, X_)$booster) %>% calc_mean_matrix()
      if (is_test) self$pred_test <- preds
      return(preds)
    }
  )
)
```

```{r}
.[lgbm_cv_ind_pred, lgbm_cv_chain_pred] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_LightGBM_CV_model, "LightGBM CV", X_train, Y_train, X_test, Y_test
)
```


## 3.4 Random Forest

```{r}
my_RandomForest_model <- R6Class(
  classname = "my_RandomForest_model",
  inherit = my_template_model,
  
  public = list(
    calc_importance = function() {
      self$importance <- self$model$importance
      return(self$importance)
    }
  ),
  
  private = list(
    fit = function(X_train_, y_train_) {
      # na.action = na.omit,
      self$model <- randomForest(x = X_train_, y = y_train_, ntree = 1000, importance = TRUE) 
      return(invisible(self))
    }
  )
)
```

```{r}
.[rf_ind_pred, rf_chain_pred] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_RandomForest_model, "Random Forest", X_train, Y_train, X_test, Y_test
)
```


## 3.5 lm

```{r}
my_lm_model <- R6Class(
  classname = "my_lm_model",
  inherit = my_template_model,
  
  public = list(
    initialize = function(X_train_, y_train_, X_test_ = NULL, y_test_ = NULL, ...) {
      X_train_ <- as.data.frame(X_train_)
      X_test_  <- as.data.frame(X_test_)
      
      private$fit(X_train_, y_train_, ...)
      self$pred_train <- private$predict(X_train_)
      
      if (!is.null(X_test_)) self$pred_test <- private$predict(X_test_, is_test = TRUE)
      if (!is.null(y_test_) && !is.null(self$pred_test)) private$calc_rmse(y_test_, self$pred_test)
      
      return(invisible(self))
    }
  ),
  
  private = list(
    fit = function(X_train_, y_train_) {
      self$model  <- lm(target ~ ., data = data.frame(X_train_, target = y_train_))
      return(invisible(self))
    }
  )
)
```

```{r}
lm_model <- lm(cbind(HL_1, HL_2, HL_3, HL_4, HL_5, HL_6) ~ ., 
               data = data.frame(X_train, Y_train)
               # , na.action = na.omit
               )

lm_ind_pred <- predict(lm_model, newdata = X_test %>% as.data.frame()) %>% prediction_correction()
show_custom_metrics(lm_ind_pred, "lm")
```

```{r}
.[lm_ind_pred, lm_chain_pred] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_lm_model, "lm", X_train, Y_train, X_test, Y_test
)
```


## 3.6 CatBoost

```{r}
my_Catboost_model <- R6Class(
  classname = "my_Catboost_model",
  inherit = my_template_model,
  
  public = list(
    initialize = function(X_train_, y_train_, X_test_, y_test_) {
      train_pool <- catboost.load_pool(data = X_train_, label = y_train_)
      test_pool  <- catboost.load_pool(data = X_test_,  label = y_test_)
        
      private$fit(train_pool, test_pool)
      self$pred_train <- private$predict(train_pool)
      
      if (!is.null(X_test_)) self$pred_test <- private$predict(test_pool, is_test = TRUE)
      if (!is.null(y_test_) && !is.null(self$pred_test)) private$calc_rmse(y_test_, self$pred_test)
      
      return(invisible(self))
    }
  ),
  
  private = list(
    fit = function(train_pool, test_pool) {
      self$model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = list(loss_function = 'RMSE', logging_level = "Silent")
      )
      return(invisible(self))
    },
    
    predict = function(X_, is_test = FALSE) {
      preds <- catboost.predict(self$model, X_)
      if (is_test) self$pred_test <- preds
      return(preds)
    }
  )
)
```

```{r}
.[catb_ind_pred, catb_chain_pred] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_Catboost_model, "CatBoost", X_train %>% apply(2, as.double), Y_train, X_test %>% apply(2, as.double), Y_test
)
```


# 4. PCA

```{r}
pca_model <- prcomp(X_train, center = TRUE, scale. = TRUE)

# cumsum(pca_model$sdev^2 / sum(pca_model$sdev^2))
data.table(
  k = 1:length(pca_model$sdev),
  ssq = pca_model$sdev^2
) %>% 
  .[, cumsum := cumsum(ssq / sum(ssq))] %>% 
  .[]

plot(pca_model$sdev^2 / sum(pca_model$sdev^2), 
     type = "b", 
     xlab = "Главные компоненты", 
     ylab = "Доля объясненной дисперсии")
```

```{r}
check_k_pca_for_model <- function(k, model, X_train_, X_test_) {
  X_train_pca <- pca_scaler(X_train_, pca_model, k)
  X_test_pca  <- pca_scaler(X_test_,  pca_model, k)
  
  tmp_preds <- perform_indep_multioutput_regression(model, print_msg = "RF", X_train_pca, Y_train,
                                                    X_test_pca, Y_test, print_metric = FALSE)
  
  aRMSE_ <- sapply(1:nrow(Y_test), \(i) my_rmse(tmp_preds[i, ], Y_test[i, ])) %>% mean()
  return(c(k = k, aRMSE = aRMSE_))
}


get_k_PCA_model_table <- function(model, by = 5, X_train_ = X_train, X_test_ = X_test, plot_b = TRUE) {
  res_k_model <- sapply(seq(2, ncol(X_train_), by = by), 
                        check_k_pca_for_model, 
                        model = model, X_train_ = X_train_, X_test_ = X_test_) %>% 
    t() %>% 
    as.data.table()
  
  if (plot_b) plot_ly(res_k_model, x = ~k, y = ~aRMSE, type = 'scatter', mode = 'lines+markers') %>% show()
  return(res_k_model)
}
```


## 4.1 lm

Зависимость k (~PCA) и результатов lm:
```{r}
res_k_lm <- get_k_PCA_model_table(my_lm_model, 1)
```


```{r}
best_k <- res_k_lm[aRMSE == min(aRMSE), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[lm_pca_ind_pred, lm_pca_chain_pred] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_lm_model, "lm + PCA", X_train_pca, Y_train, X_test_pca, Y_test
)
```


## 4.2 Random Forest

Подбор лучшего k для PCA + Random Forest

```{r}
res_k_rf <- get_k_PCA_model_table(my_RandomForest_model, 5)
```

k = 27
```{r}
best_k <- res_k_rf[aRMSE == min(aRMSE), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[rf_pca_ind_pred, rf_pca_chain_pred] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_RandomForest_model, "Random Forest + PCA", X_train_pca, Y_train, X_test_pca, Y_test
)
```


## 4.3 XGBoost

```{r}
res_k_xgb <- get_k_PCA_model_table(my_XGBoost_model, 2)
```

k = 26
```{r}
best_k <- res_k_xgb[aRMSE == min(aRMSE), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[xgb_pca_ind_pred, xgb_pca_chain_pred] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_XGBoost_model, "XGBoost + PCA", X_train_pca, Y_train, X_test_pca, Y_test
)
```


## 4.4 Catboost

```{r}
res_k_catb <- get_k_PCA_model_table(my_Catboost_model, 4)
```

k = 30
```{r}
best_k <- res_k_catb[aRMSE == min(aRMSE), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[catb_pca_ind_pred, catb_pca_chain_pred] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_Catboost_model, "Catboost + PCA", X_train_pca, Y_train, X_test_pca, Y_test
)
```


## 4.5 LightGBM

```{r}
res_k_lgb <- get_k_PCA_model_table(my_LightGBM_model, 3)
```

k = 11
```{r}
best_k <- res_k_lgb[aRMSE == min(aRMSE), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[lgb_pca_ind_pred, lgb_pca_chain_pred] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_LightGBM_model, "LightGBM + PCA", X_train_pca, Y_train, X_test_pca, Y_test
)
```


## 4.6 LightGBM CV

```{r}
res_k_lgb_cv <- get_k_PCA_model_table(my_LightGBM_CV_model, 3)
```

k = 11
```{r}
best_k <- res_k_lgb_cv[aRMSE == min(aRMSE), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[lgb_cv_pca_ind_pred, lgb_cv_pca_chain_pred] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_LightGBM_CV_model, "LightGBM CV + PCA", X_train_pca, Y_train, X_test_pca, Y_test
)
```


# 5. Ensemble

## 5.1 Стэкинг моделей на данных без пропусков

### Без PCA
const_pred, xgb_ind_pred, xgb_chain_pred, lgbm_ind_pred, lgbm_chain_pred, lgbm_cv_ind_pred, lgbm_cv_chain_pred, rf_ind_pred, rf_chain_pred, lm_ind_pred, catb_ind_pred, catb_chain_pred

```{r}
scalar_matrix_production(w_vec = c(0.5, 0.5), list(const_pred, xgb_ind_pred)) %>% 
  show_custom_metrics(., "Const & Xgb", Y_test_ = Y_test)
```

```{r}
models_pred <- list(xgb_ind_pred, xgb_chain_pred, lgbm_ind_pred, lgbm_chain_pred, lgbm_cv_ind_pred, lgbm_cv_chain_pred, rf_ind_pred, rf_chain_pred, lm_ind_pred, catb_ind_pred, catb_chain_pred)

w <- rep(1/length(models_pred), length(models_pred))
scalar_matrix_production(w_vec = w, models_pred) %>% show_custom_metrics("All", Y_test_ = Y_test)
```


```{r}
selected_models_pred <- list(xgb_ind_pred, lgbm_chain_pred, lgbm_cv_ind_pred, rf_ind_pred, lm_ind_pred, catb_ind_pred, catb_chain_pred)

w <- c(1, 5, 5, 5, 1, 2, 6)
scalar_matrix_production(w_vec = w, selected_models_pred) %>% show_custom_metrics("Selected", Y_test_ = Y_test)
```

```{r}
selected2_models_pred <- list(lgbm_chain_pred, lgbm_cv_ind_pred, rf_ind_pred, catb_ind_pred, catb_chain_pred)

w <- rep(1/length(selected2_models_pred), length(selected2_models_pred))
scalar_matrix_production(w_vec = w, selected2_models_pred) %>% show_custom_metrics("Selected 2", Y_test_ = Y_test)
```

```{r}
selected3_models_pred <- list(lgbm_ind_pred, rf_ind_pred, lm_ind_pred, catb_ind_pred)

w <- c(1, 1, 1, 3)
# w <- rep(1/length(selected3_models_pred), length(selected3_models_pred))
scalar_matrix_production(w_vec = w, selected3_models_pred) %>% show_custom_metrics("Selected 3", Y_test_ = Y_test)
```


### С PCA
lm_pca_ind_pred, lm_pca_chain_pred, rf_pca_ind_pred, rf_pca_chain_pred, xgb_pca_ind_pred, xgb_pca_chain_pred, catb_pca_ind_pred, catb_pca_chain_pred, lgb_pca_ind_pred, lgb_pca_chain_pred, lgb_cv_pca_ind_pred, lgb_cv_pca_chain_pred

```{r}
models_pred <- list(lm_pca_ind_pred, lm_pca_chain_pred, rf_pca_ind_pred, rf_pca_chain_pred, xgb_pca_ind_pred, xgb_pca_chain_pred, catb_pca_ind_pred, catb_pca_chain_pred, lgb_pca_ind_pred, lgb_pca_chain_pred, lgb_cv_pca_ind_pred, lgb_cv_pca_chain_pred)

w <- rep(1/length(models_pred), length(models_pred))
scalar_matrix_production(w_vec = w, models_pred) %>% show_custom_metrics("All", Y_test_ = Y_test)
```


```{r}
selected_models_pred <- list(xgb_pca_ind_pred, lgb_pca_chain_pred, lgb_cv_pca_ind_pred, rf_pca_ind_pred, lm_pca_ind_pred, catb_pca_ind_pred, catb_pca_chain_pred)

w <- c(1, 5, 5, 5, 1, 2, 6)
scalar_matrix_production(w_vec = w, selected_models_pred) %>% show_custom_metrics("Selected", Y_test_ = Y_test)
```

```{r}
selected2_models_pred <- list(lgb_pca_chain_pred, lgb_cv_pca_ind_pred, rf_pca_ind_pred, catb_pca_ind_pred, catb_pca_chain_pred)

w <- rep(1/length(selected2_models_pred), length(selected2_models_pred))
sel2_preds <- scalar_matrix_production(w_vec = w, selected2_models_pred)
sel2_preds %>% show_custom_metrics("Selected 2", Y_test_ = Y_test)
```

```{r}
selected3_models_pred <- list(lgb_pca_ind_pred, rf_pca_ind_pred, lm_pca_ind_pred, catb_pca_ind_pred)

w <- c(1, 1, 1, 3)
# w <- rep(1/length(selected3_models_pred), length(selected3_models_pred))
scalar_matrix_production(w_vec = w, selected3_models_pred) %>% show_custom_metrics("Selected 3", Y_test_ = Y_test)
```


## 5.2 Стэкинг моделей отдельно на психологических тестах

Найти наилучшую модель для каждого из тестов. Мб миксовать с PCA.

```{r}
prepare_psytest_dataset_by_split <- function(input_dt, psytest_label = c("BF", "EY", "CT", "LN", "SC")) {
  psytest_label <- match.arg(psytest_label)
  
  input_dt <- input_dt %>% 
    copy() %>% 
    .[, .SD, .SDcols = names(input_dt) %like% paste0(psytest_label, "|HL")] %>% 
    drop_na()
  
  split_idx <- sample(c(TRUE, FALSE), nrow(input_dt), replace = TRUE, prob = c(0.8, 0.2))
  
  .[X_train_unscaled, X_test_unscaled] <- list(
    input_dt[split_idx, .SD, .SDcols = names(input_dt) %like% psytest_label], 
    input_dt[!split_idx, .SD, .SDcols = names(input_dt) %like% psytest_label])
  
  .[Y_train, Y_test] <- list(input_dt[split_idx, .SD, .SDcols = names(input_dt) %like% "HL_"], 
                             input_dt[!split_idx, .SD, .SDcols = names(input_dt) %like% "HL_"]) %>% 
    lapply(as.matrix)
  
  mean_train <- apply(X_train_unscaled, 2, mean, na.rm = TRUE)
  sd_train <- apply(X_train_unscaled, 2, sd, na.rm = TRUE)
  
  X_train <- scale(X_train_unscaled, center = mean_train, scale = sd_train)
  X_test  <- scale(X_test_unscaled, center = mean_train, scale = sd_train)
  
  if (nrow(X_train) == 0 | nrow(X_test) == 0) error("X_train or X_test are empty!")
  
  return(list(X_train = X_train, Y_train = Y_train, X_test = X_test, Y_test = Y_test))
}
```

Важно: обучаем на новых потестовых данных (wide_data2), оцениваем на wide_data (старые данные, которые есть по всем тестам, потому в конце можно оценивать ансамбль на них же).


### Catboost
Для примера берем Catboost и обучаем его на данных каждого и тестов по отдельности.

1. BF
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "BF")

.[BF_catb_ind, BF_catb_chain] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_Catboost_model, "BF - CatBoost", df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]]
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

2. EY
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "EY")

.[EY_catb_ind, EY_catb_chain] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_Catboost_model, "EY - CatBoost", df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]]
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```


3. CT
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "CT")

.[CT_catb_ind, CT_catb_chain] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_Catboost_model, "CT - CatBoost", df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]]
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

4. LN
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "LN")

.[LN_catb_ind, LN_catb_chain] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_Catboost_model, "LN - CatBoost", df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]]
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

5. SC
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "SC")

.[SC_catb_ind, SC_catb_chain] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_Catboost_model, "SC - CatBoost", df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]]
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

Ансамбль
```{r}
models_pred <- list(BF_catb_chain, EY_catb_ind, CT_catb_ind, LN_catb_ind, SC_catb_ind)

w1 <- rep(1/length(models_pred), length(models_pred))
w2 <- c(1, 1, 10, 5, 2) # BF 2.203 (1), EY 2.203 (1), CT 2.089 (10), LN 2.149 (5), SC 2.192 (2)

scalar_matrix_production(w_vec = w1, models_pred) %>% 
  show_custom_metrics("All equal", Y_test_ = df[["Y_test"]])

scalar_matrix_production(w_vec = w2, models_pred) %>% 
  show_custom_metrics("All preferences", Y_test_ = df[["Y_test"]])
```
Catboost:
All equal. aRMSE: 2.117; aCosDist: 0.288
All preferences. aRMSE: 2.083; aCosDist: 0.284


### Random Forest

1. BF
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "BF")

.[BF_rf_ind, BF_rf_chain] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_RandomForest_model, "BF - Random Forest", df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]]
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

2. EY
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "EY")

.[EY_rf_ind, EY_rf_chain] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_RandomForest_model, "EY - Random Forest", df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]]
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```


3. CT
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "CT")

.[CT_rf_ind, CT_rf_chain] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_RandomForest_model, "CT - Random Forest", df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]]
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

4. LN
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "LN")

.[LN_rf_ind, LN_rf_chain] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_RandomForest_model, "LN - Random Forest", df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]]
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

5. SC
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "SC")

.[SC_rf_ind, SC_rf_chain] <- map(
  list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
  my_RandomForest_model, "SC - Random Forest", df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]]
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

Ансамбль
```{r}
# list(BF_rf_chain 2.243, EY_rf_ind 2.241, CT_rf_ind 2.089, LN_rf_ind 2.193, SC_rf_ind 2.205)
models_pred <- list(BF_rf_chain, EY_rf_ind, CT_rf_ind, LN_rf_ind, SC_rf_ind)

w1 <- rep(1/length(models_pred), length(models_pred))
w2 <- c(1, 1, 6, 2, 2)
w3 <- c(1, 1, 10, 2, 2)

scalar_matrix_production(w_vec = w1, models_pred) %>% 
  show_custom_metrics("All equal", Y_test_ = df[["Y_test"]])

scalar_matrix_production(w_vec = w2, models_pred) %>% 
  show_custom_metrics("All preferences", Y_test_ = df[["Y_test"]])

scalar_matrix_production(w_vec = w3, models_pred) %>% 
  show_custom_metrics("All preferences 2", Y_test_ = df[["Y_test"]])
```

### Пример поиска лучшей модели для одного теста

Перечислим все модели:
```{r}
one_test_model_map <- function(model, model_label, df_ = df) {
  x <- map(
    list(perform_indep_multioutput_regression, perform_chain_multioutput_regression), exec,
    model, model_label, df_[["X_train"]], df_[["Y_train"]], df_[["X_test"]], df_[["Y_test"]]
  )
}


show_constant_pred_metric <- function(df_) {
  res <- rep(colMeans(df_[["Y_train"]]), each = nrow(df_[["Y_test"]])) %>% 
    matrix(nrow = nrow(df_[["Y_test"]])) %>% 
    show_custom_metrics("Константное", Y_test_ = df_[["Y_test"]])
  return(res)
}


class_models <- data.table(
  models = list(my_XGBoost_model, my_LightGBM_model, my_LightGBM_CV_model, my_RandomForest_model, my_lm_model, my_Catboost_model),
  labels = list("XGBoost", "LightGBM", "LightGBM CV", "Random Forest", "lm", "CatBoost")
)
```

c("BF", "EY", "CT", "LN", "SC")
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "LN", use_PCA_b = T)
x <- map2(class_models$models, class_models$labels, one_test_model_map, df_ = df)
show_constant_pred_metric(df)
```

---

*CT**: 
1) *no PCA*
XGBoost Indep. aRMSE: 2.178; aCosDist: 0.296
XGBoost Chained. aRMSE: 2.264; aCosDist: 0.308
LightGBM Indep. aRMSE: 2.127; aCosDist: 0.29
LightGBM Chained. aRMSE: 2.118; aCosDist: 0.288
LightGBM CV Indep. aRMSE: 2.116; aCosDist: 0.288
LightGBM CV Chained. aRMSE: 2.123; aCosDist: 0.289
Random Forest Indep. aRMSE: 2.101; aCosDist: 0.286
Random Forest Chained. aRMSE: 2.104; aCosDist: 0.287
lm Indep. aRMSE: 2.076; aCosDist: 0.282
lm Chained. aRMSE: 2.076; aCosDist: 0.282
CatBoost Indep. aRMSE: 2.089; aCosDist: 0.284
CatBoost Chained. aRMSE: 2.109; aCosDist: 0.287
Константное. aRMSE: 2.23; aCosDist: 0.304

2) *PCA*
Train: 654 rows; test: 339 rows (34.14%).
XGBoost Indep. aRMSE: 2.257; aCosDist: 0.307
XGBoost Chained. aRMSE: 2.295; aCosDist: 0.312
LightGBM Indep. aRMSE: 2.125; aCosDist: 0.289
LightGBM Chained. aRMSE: 2.149; aCosDist: 0.293
LightGBM CV Indep. aRMSE: 2.14; aCosDist: 0.292
LightGBM CV Chained. aRMSE: 2.117; aCosDist: 0.288
Random Forest Indep. aRMSE: 2.135; aCosDist: 0.291
Random Forest Chained. aRMSE: 2.156; aCosDist: 0.294
lm Indep. aRMSE: 2.077; aCosDist: 0.283
lm Chained. aRMSE: 2.077; aCosDist: 0.283
CatBoost Indep. aRMSE: 2.124; aCosDist: 0.289
CatBoost Chained. aRMSE: 2.129; aCosDist: 0.29
Константное. aRMSE: 2.23; aCosDist: 0.304

---

```{r}
# models_pred <- list(x[[1]][[1]], x[[2]][[2]], x[[3]][[1]], x[[4]][[2]], x[[5]][[1]], x[[6]][[1]])
models_pred <- list(x[[1]][[1]], x[[2]][[2]], x[[3]][[1]], x[[4]][[1]], x[[5]][[1]], x[[6]][[1]])

w1 <- rep(1/length(models_pred), length(models_pred))
w2 <- c(1, 2, 2, 2, 9, 2)

scalar_matrix_production(w_vec = w1, models_pred) %>% 
  show_custom_metrics("All equal", Y_test_ = df[["Y_test"]])

scalar_matrix_production(w_vec = w2, models_pred) %>% 
  show_custom_metrics("All preferences", Y_test_ = df[["Y_test"]])
```

LN (no PCA):
```{r}
models_pred2 <- list(x[[1]][[1]], x[[2]][[1]], x[[3]][[2]], x[[4]][[1]], x[[5]][[1]], x[[6]][[1]])
w3 <- c(1, 3, 3, 3, 3, 13)

scalar_matrix_production(w_vec = w3, models_pred2) %>% 
  show_custom_metrics("All preferences", Y_test_ = df[["Y_test"]])
```



# 6. Multilabel classification

```{r}
bool_mask_row <- function(matrix_row) {
  ind <- matrix_row %>% order(decreasing = TRUE) %>% .[1:3]
  b_vec <- rep(FALSE, 6)
  b_vec[ind] <- TRUE
  return(b_vec)
}

targets_bool <- apply(targets, 1, bool_mask_row) %>% t()
.[Y_b_train, Y_b_test] <- list(targets_bool[split_idx, ], targets_bool[!split_idx, ])
```

```{r}
ind_pred <- vector("list", 6)

for (col_i in 1:6) {
  rf <- randomForest(x = X_train, y = as.factor(Y_b_train[, col_i]), ntree = 1000, importance = TRUE) 
  ind_pred[[col_i]] <- predict(rf, X_test, type = "prob")[, "TRUE"]
}

Y_pred <- ind_pred %>% as.data.table() %>% apply(., 1, bool_mask_row) %>% t()
# Y_b_test

# Случайные предсказания
Y_rand <- replicate(n = nrow(Y_b_test), sample(c(rep(TRUE, 3), rep(FALSE, 3)), size = 6)) %>% t()
```


## Метрики

1. Hamming Loss - Доля неправильно предсказанных меток
```{r}
# доля неправильных ответов. От 0 до 1
hamming_loss <- function(y_true, y_pred) {
  mean(y_true != y_pred)
}

hamming_loss(Y_b_test, Y_pred) # 0.377451
hamming_loss(Y_b_test, Y_rand) # 0.5392157
# hamming_loss(Y_b_test, Y_b_test)
# hamming_loss(Y_b_test, !Y_b_test)
```

2. Top-k Accuracy
```{r}
# Расчет точности
mean(rowSums(Y_pred * Y_b_test) >= 1)  # Хотя бы одна верная - 0.9852941
mean(rowSums(Y_pred * Y_b_test) >= 2)  # Хотя бы 2 верные    - 0.7352941
mean(rowSums(Y_pred * Y_b_test) >= 3)  # Все 3 верные        - 0.1470588

# последнее аналогично Subset Accuracy (Exact Match) - Доля объектов с полностью правильными метками
subset_accuracy <- function(y_true, y_pred) {
  mean(apply(y_true == y_pred, 1, all))
}

subset_accuracy(Y_b_test, Y_pred) # 0.1470588
```

```{r}
# Расчет точности
mean(rowSums(Y_rand * Y_b_test) >= 1)  # Хотя бы одна верная - 0.8970588
mean(rowSums(Y_rand * Y_b_test) >= 2)  # Хотя бы 2 верные    - 0.4264706
mean(rowSums(Y_rand * Y_b_test) >= 3)  # Все 3 верные        - 0.05882353

subset_accuracy(Y_b_test, Y_rand)
```


3. Precision, Recall, F1-Score
```{r}
library(Metrics)

calc_class_metrics <- function(y_true, y_pred) {
  answ <- data.table(y_true = y_true, y_pred = y_pred)
  TP <- answ[y_true == TRUE & y_pred == TRUE, .N]
  FP <- answ[y_true == FALSE & y_pred == TRUE, .N]
  FN <- answ[y_true == TRUE & y_pred == FALSE, .N]
  TN <- answ[y_true == FALSE & y_pred == FALSE, .N]
  
  return(list(
    recall = TP / (TP + FN),
    precision = TP / (TP + FP),
    acc = (TP + TN) / (TP + FP + FN + TN),
    F1_score = (2 * TP) / (2 * TP + FP + FN)
  ))
}

# calc_class_metrics(Y_b_test[, 1], Y_pred[, 1])ф
```

```{r}
get_confus_elems <- function(y_true, y_pred) {
  answ <- data.table(y_true = y_true, y_pred = y_pred)
  TP <- answ[y_true == TRUE & y_pred == TRUE, .N]
  FP <- answ[y_true == FALSE & y_pred == TRUE, .N]
  FN <- answ[y_true == TRUE & y_pred == FALSE, .N]
  TN <- answ[y_true == FALSE & y_pred == FALSE, .N]
  return(list(TP = TP, FP = FP, FN = FN, TN = TN))   
}

get_total_confus_dt <- function(Y_true, Y_pred) {
  conf_dt <- sapply(1:6, \(i) get_confus_elems(Y_true[, i], Y_pred[, i])) %>% 
    t() %>% 
    as.data.table() %>% 
    unnest(cols = c(TP, FP, FN, TN))
  return(conf_dt)
}

recall_    <- function(conf_row) conf_row[["TP"]] / (conf_row[["TP"]] + conf_row[["FN"]])
precision_ <- function(conf_row) conf_row[["TP"]] / (conf_row[["TP"]] + conf_row[["FP"]])
F1_score_  <- function(conf_row) {
  return((2 * conf_row[["TP"]]) / (2 * conf_row[["TP"]] + conf_row[["FP"]] + conf_row[["FN"]]))
}

micro_metric <- \(total_conf_dt, metric_f) colSums(total_conf_dt) %>% metric_f()
macro_metric <- \(total_conf_dt, metric_f) apply(total_conf_dt, 1, metric_f) %>% mean()

calc_multiclass_metrics <- function(Y_true, Y_pred) {
  total_confus_dt <- get_total_confus_dt(Y_true, Y_pred)
  return(list(
    micro_precision = micro_metric(total_confus_dt, precision_),
    micro_recall    = micro_metric(total_confus_dt, recall_),
    micro_F1_score  = micro_metric(total_confus_dt, F1_score_),
    
    macro_precision = macro_metric(total_confus_dt, precision_),
    macro_recall    = macro_metric(total_confus_dt, recall_),
    macro_F1_score  = macro_metric(total_confus_dt, F1_score_)
  ))
}
```

```{r}
bind_rows(
  pred = calc_multiclass_metrics(Y_b_test, Y_pred),
  rand = calc_multiclass_metrics(Y_b_test, Y_rand)
) %>% mutate(Y = c("pred", "rand"), .before = 1)
```

4. Jaccard Similarity Score
Сходство между метками:
```{r}
jaccard_score <- function(y_true, y_pred) {
  intersection <- rowSums(y_true * y_pred)
  union <- rowSums((y_true + y_pred) > 0)
  mean(intersection / union)
}

jaccard_score(Y_b_test, Y_pred) # 0.4911765
jaccard_score(Y_b_test, Y_rand) # 0.3367647
```


```{r}
library(mlr3verse)
library(mlr3spatial)

# Создание dataframe
n_target <- 6
full_data <- cbind(features, targets_bool) %>% as.data.table()

target_labels <- paste0("Label_", 1:n_target)
colnames(full_data)[(ncol(full_data)-6+1):ncol(full_data)] <- target_labels

# getTaskData(yeast.task)

# task <- mlr3::as_task_classif(x = full_data, target = target_labels)

# Создание multilabel задачи
task <- as_task_classif_st(
  x = full_data,
  target = paste0("Label_", 1:n_target),
  id = "psychology_test",
  positive = "1",
  label = "Psychological Scales Classification"
)

# Установка типа задачи как multilabel
task$properties <- union(task$properties, "multilabel")

# Разделение данных на обучающую и тестовую выборки
split <- partition(task, ratio = 0.8)

# Создание модели (Random Forest через ranger)
learner <- lrn("classif.ranger",
  num.trees = 100,
  predict_type = "prob",
  importance = "impurity"
)

# Создание мультиметочного ученика с бинарными преобразованиями
multilabel_learner <- mlr_learners$get("classif.ranger") %>>%
  po("multilabel", method = "onevsrest") %>>%
  as_learner()

# Обучение модели
multilabel_learner$train(task, split$train)

# Предсказание на тестовых данных
prediction <- multilabel_learner$predict(task, split$test)

# Оценка результатов
measures <- list(
  msr("multilabel.hamloss"),
  msr("multilabel.f1"),
  msr("multilabel.acc")
)

prediction$score(measures)
```



# 7. Python

```{r}
wide_data %>% 
  as.data.table() %>% 
  arrow::write_feather("../0. Data/wide_data.feather")

targets_bool %>% 
  as.data.table() %>% 
  rename_all(~colnames(targets)) %>% 
  arrow::write_feather("../0. Data/Y_bool.feather")
```


```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor, RegressorChain
# from sklearn.metrics import root_mean_squared_error

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor 
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
```

```{python}
def rmse(y_true, y_pred):
    mean_squared_error = np.mean((y_true - y_pred) ** 2)  # Среднее значение квадратов ошибок
    return np.sqrt(mean_squared_error)


def check_MO_regression(model_obj, model_name=""):
    MO_regr = {'Indep': MultiOutputRegressor, 'Chain': RegressorChain}
    for name, model in MO_regr.items():
        y_pred = model(model_obj).fit(X_train, y_train).predict(X_test)
        print(f"{model_name} {name}. RMSE: {round(rmse(y_test, y_pred), 3)}")
    return(None)


def plot_nn_losses(hist):
    plt.plot(hist.history['loss'], label='Train Loss')
    plt.plot(hist.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
    return(None)
```

```{python}
data = pd.read_feather("../0. Data/wide_data.feather")

X = data.loc[:, ~data.columns.isin(["HL_1", "HL_2", "HL_3", "HL_4", "HL_5", "HL_6", "id"])]
y = data.loc[:, "HL_1":"HL_6"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

y_bool = pd.read_feather("../0. Data/Y_bool.feather")

y_b_train, y_b_test = train_test_split(y_bool, test_size=0.2, random_state=42)
```

```{python}
linear = LinearRegression().fit(X_train, y_train)
rf = RandomForestRegressor().fit(X_train, y_train)
etr = ExtraTreesRegressor().fit(X_train, y_train)
knn = KNeighborsRegressor().fit(X_train, y_train)

print(f"Linear. RMSE: {rmse(y_test, linear.predict(X_test))}")
print(f"ExtraTree. RMSE: {rmse(y_test, etr.predict(X_test))}")
print(f"Random Forest. RMSE: {rmse(y_test, rf.predict(X_test))}")
print(f"kNN. RMSE: {rmse(y_test, knn.predict(X_test))}")
```
