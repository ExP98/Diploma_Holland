---
title: "Регрессионные модели"
author: "Egor Glushkov"
---

Идеи: сделать общий интерфейс (ООП?) для всех моделей + для chain- и indep-регрессоров


# 1. Библиотеки и функции
```{r include=FALSE}
library(mlr3verse)
library(dotty)
library(xgboost)
library(lightgbm)
library(randomForest)
library(catboost)
library(caret)

set.seed(42)
source(paste0(here::here(), "/1. R/4. data_preparation.R"))
```

Функции:
```{r}
my_rmse <- \(y1, y2) sqrt(sum((y1 - y2)^2) / length(y1))
cosine_sim <- \(y1, y2) (sum(y1 * y2)) / (sqrt(sum(y1 ^ 2)) * sqrt(sum(y2 ^ 2)))
cosine_dist <- \(y1, y2) sqrt(2 * (1 - cosine_sim(y1, y2)))


smart_integer_round <- function(six_vals) {
  modif <- six_vals * 42 / sum(six_vals) + 10*.Machine$double.eps
  resid <- 42 - sum(round(modif))
  
  ind_to_change <- order(abs(modif - round(modif)), decreasing = TRUE)[1:abs(resid)]
  modif[ind_to_change] <- modif[ind_to_change] + sign(resid) * 1
  
  int_rnd_values <- round(modif)
  if (sum(int_rnd_values) != 42) warning(str_glue("Sum of {toString(int_rnd_values)} != 42. Input: {toString(six_vals)}. \n"))
  return(int_rnd_values)
}

# sapply(1:nrow(pred_test), \(i) smart_integer_round(pred_test[i, ])) %>% colSums()

show_custom_metrics <- function(my_pred, case_name) {
  aRMSE_ <- sapply(1:nrow(Y_test), \(i) mlr3measures::rmse(my_pred[i, ], Y_test[i, ])) %>% mean()
  aCosDist_ <- sapply(1:nrow(Y_test), \(i) cosine_dist(my_pred[i, ], Y_test[i, ])) %>% mean()
  print(str_glue("{case_name}. aRMSE: {round(aRMSE_, 3)}; aCosDist: {round(aCosDist_, 3)}"))
  return(invisible(NULL))
}
```


# 2. Данные

Разделение данных на трейн и тест:
```{r}
features <- untd_dt %>% 
  copy() %>% 
  .[, .SD, .SDcols = !(names(untd_dt) %like% "HL|HL2|id")] %>% 
  as.matrix()

targets <- untd_dt %>% 
  copy() %>% 
  .[, .SD, .SDcols = names(untd_dt) %like% "HL_"] %>% 
  as.matrix()

split_idx <- sample(c(TRUE, FALSE), nrow(features), replace = TRUE, prob = c(0.8, 0.2))

.[X_train_unscaled, X_test_unscaled] <- list(features[split_idx, ], features[!split_idx, ])
.[Y_train, Y_test] <- list(targets[split_idx, ], targets[!split_idx, ])

mean_train <- apply(X_train_unscaled, 2, mean, na.rm = TRUE)
sd_train <- apply(X_train_unscaled, 2, sd, na.rm = TRUE)

X_train <- scale(X_train_unscaled, center = mean_train, scale = sd_train)
X_test  <- scale(X_test_unscaled, center = mean_train, scale = sd_train)
```


# 3. Модель

## 3.1 Константный предсказатель
```{r}
# const_pred <- matrix(rep(rep(7, times = 6), each = nrow(Y_test)), nrow = nrow(Y_test))
const_pred <- matrix(rep(colMeans(Y_train), each = nrow(Y_train)), nrow = nrow(Y_train))
show_custom_metrics(const_pred, "Константное")
```

## 3.2 XGBoost

### Число раундов
Как выглядит модель, сколько раундов требуется:
```{r}
ii <- 1

res <- vector("list", 20)
for (i in 1:20) {
  xgb_model <- xgboost(data = X_train, label = Y_train[, ii],
                       nrounds = i, objective = "reg:squarederror", eval_metric = "rmse", verbose = 0)
  
  pred_y <- predict(xgb_model, X_test)
  res[[i]] <- data.frame(
    nrounds = i,
    test_rmse = mlr3measures::rmse(Y_test[, ii], pred_y),
    test_round_rmse = mlr3measures::rmse(Y_test[, ii], pred_y %>% round())
  )
}

res %>% rbindlist() %>% .[["test_rmse"]] %>% plot(main = "test_rmse")
res %>% rbindlist() %>% .[["test_round_rmse"]] %>% plot(main = "test_round_rmse")
```
15 раундов хватает.

### Независимое обучение
Обучение и предсказание модели по шести признакам независимо:
```{r}
indep_pred <- vector("list", 6)
importance_list <- vector("list", 6)

for (feature_num in 1:6) {
  xgb_model <- xgboost(data = X_train, label = Y_train[, feature_num],
                       nrounds = 15, objective = "reg:squarederror", eval_metric = "rmse", verbose = 0)
  
  indep_pred[[feature_num]] <- predict(xgb_model, X_test)
  importance_list[[feature_num]] <- xgb.importance(model = xgb_model)
}

indep_pred <- indep_pred %>% as.data.table() %>% as.matrix()
correct_indep_pred <- apply(indep_pred, 1, smart_integer_round) %>% t()

# show_custom_metrics(indep_pred, "XGBoost. Неприведенное независимое")
show_custom_metrics(correct_indep_pred, "XGBoost. Приведенное независимое")
# rowSums(res)[rowSums(res) != 42]
```

```{r}
importance_list
```
Для XGBoost (и позже для Random Forest) самая важная фича -- CT_1 (открытость-замкнутость)


### Chained regression

```{r}
col_i <- 1

local_train_set <- X_train
local_test_set <- X_test

bst <- xgboost(data = local_train_set, label = Y_train[, col_i], nrounds = 15, 
               objective = "reg:squarederror", eval_metric = "rmse", verbose = 0)

# bst$evaluation_log
pred_train_i <- predict(bst, local_train_set) 
pred_test_i  <- predict(bst, local_test_set) 
mlr3measures::rmse(Y_test[, col_i], pred_test_i)

all_test_preds <- NULL
all_test_preds[[col_i]] <- pred_test_i
```

```{r}
for (col_i in 2:6) {
  local_train_set <- cbind(local_train_set, col_i = pred_train_i)
  colnames(local_train_set)[ncol(local_train_set)] <- paste0("output_", col_i)
  
  local_test_set <- cbind(local_test_set, col_i = pred_test_i)
  colnames(local_test_set)[ncol(local_test_set)] <- paste0("output_", col_i)
  
  bst <- xgboost(data = local_train_set, label = Y_train[, col_i], nrounds = 15, 
                 objective = "reg:squarederror", eval_metric = "rmse", verbose = 0)
  
  # bst$evaluation_log
  pred_train_i <- predict(bst, local_train_set) 
  pred_test_i  <- predict(bst, local_test_set) 
  
  all_test_preds[[col_i]] <- pred_test_i
  print(mlr3measures::rmse(Y_test[, col_i], pred_test_i))
}


chained_pred <- all_test_preds %>% as.data.table() %>% as.matrix.data.frame()
correct_chain_test <- apply(chained_pred, 1, smart_integer_round) %>% t()

# show_custom_metrics(chained_pred, "Chained неприведенное")
show_custom_metrics(correct_chain_test, "Chained приведенное")
```

Порядок предсказания может иметь значение!


## 3.3 LightGBM

### Независимое
```{r}
x_train_mat <- data.matrix(X_train)
x_test_mat <- data.matrix(X_test)

indep_lgbm_pred <- vector("list", 6)
for (feature_num in 1:6) {
  dtrain <- lightgbm::lgb.Dataset(
    data = x_train_mat,
    label = Y_train[, feature_num],
    free_raw_data = FALSE
  )
  
  dtest <- lgb.Dataset.create.valid(dtrain, data = x_test_mat, label = Y_test[, feature_num])
    
  lgb_model <- lgb.train(
    params = list(objective = "regression", metric = "rmse", num_iterations = 250),
    data = dtrain,
    valids = list(train = dtrain, eval = dtest),
    verbose = -1
  )
  # print(lgb_model$best_score)
  indep_lgbm_pred[[feature_num]] <- predict(lgb_model, x_test_mat)
}

indep_lgbm_pred <- indep_lgbm_pred %>% as.data.table() %>% as.matrix()
correct_indep_lgbm_pred <- apply(indep_lgbm_pred, 1, smart_integer_round) %>% t()

# show_custom_metrics(indep_lgbm_pred, "Неприведенное независимое LGBM")
show_custom_metrics(correct_indep_lgbm_pred, "Приведенное независимое LGBM")
```

### Chained

```{r}
col_i <- 1

local_train_set <- x_train_mat
local_test_set <- x_test_mat

dtrain <- lightgbm::lgb.Dataset(
  data = local_train_set,
  label = Y_train[, col_i],
  free_raw_data = FALSE
)

dtest <- lgb.Dataset.create.valid(dtrain, 
                                  data = local_test_set, 
                                  label = Y_test[, col_i])
lgb_model <- lgb.train(
  params = list(objective = "regression", metric = "rmse", num_iterations = 250),
  data = dtrain,
  valids = list(train = dtrain, eval = dtest),
  verbose = -1
)

pred_train_i <- predict(lgb_model, local_train_set)
pred_test_i  <- predict(lgb_model, local_test_set)
mlr3measures::rmse(Y_test[, col_i], pred_test_i)

all_test_preds <- NULL
all_test_preds[[col_i]] <- pred_test_i
```

```{r}
for (col_i in 2:6) {
  local_train_set <- cbind(local_train_set, col_i = pred_train_i)
  colnames(local_train_set)[ncol(local_train_set)] <- paste0("output_", col_i)
  
  local_test_set <- cbind(local_test_set, col_i = pred_test_i)
  colnames(local_test_set)[ncol(local_test_set)] <- paste0("output_", col_i)
  
  dtrain <- lightgbm::lgb.Dataset(
    data = local_train_set,
    label = Y_train[, col_i],
    free_raw_data = FALSE
  )
  
  dtest <- lgb.Dataset.create.valid(dtrain, 
                                    data = local_test_set, 
                                    label = Y_test[, col_i])
  lgb_model <- lgb.train(
    params = list(objective = "regression", metric = "rmse", num_iterations = 250),
    data = dtrain,
    valids = list(train = dtrain, eval = dtest),
    verbose = -1
  )
  
  pred_train_i <- predict(lgb_model, local_train_set)
  pred_test_i  <- predict(lgb_model, local_test_set)
  
  all_test_preds[[col_i]] <- pred_test_i
  print(mlr3measures::rmse(Y_test[, col_i], pred_test_i))
}

chained_pred <- all_test_preds %>% as.data.table() %>% as.matrix.data.frame()
correct_chain_test <- apply(chained_pred, 1, smart_integer_round) %>% t()

# show_custom_metrics(chained_pred, "LGBM. Chained неприведенное")
show_custom_metrics(correct_chain_test, "LGBM. Chained приведенное")
```


### Cross Validation
Cross Validation model (пример для 1 таргета):
```{r}
lgbm_cv_model <- lgb.cv(
  params = list(objective = "regression", metric = "rmse"),
  data = dtrain,
  nfold = 5L,
  verbose = -1
)

lgbm_cv_model$best_score
```


## 3.4 Random Forest

### Независимое
```{r}
models <- list()

for (feature_num in 1:6) {
  models[[feature_num]] <- randomForest(
    x = X_train,
    y = Y_train[, feature_num],
    ntree = 1000,
    na.action = na.omit
    # importance = TRUE
  )
}

# Predict all targets
rf_pred <- sapply(1:6, function(t) {
  predict(models[[t]], data.frame(X_test, Y_test))
})

correct_rf_pred <- apply(rf_pred, 1, smart_integer_round) %>% t()

show_custom_metrics(rf_pred, "Random Forest неприведенное")
show_custom_metrics(correct_rf_pred, "Random Forest приведенное")
```


### Chained

Подумать, оставлять ли ... %>% round()
```{r}
col_i <- 1

local_train_set <- X_train
local_test_set <- X_test

rf <- randomForest(x = local_train_set, y = Y_train[, col_i], ntree = 1000)

pred_train_i <- predict(rf, local_train_set) 
pred_test_i  <- predict(rf, local_test_set) 
mlr3measures::rmse(Y_test[, col_i], pred_test_i)

all_test_preds <- NULL
all_test_preds[[col_i]] <- pred_test_i
```

```{r}
for (col_i in 2:6) {
  local_train_set <- cbind(local_train_set, col_i = pred_train_i)
  colnames(local_train_set)[ncol(local_train_set)] <- paste0("output_", col_i)
  
  local_test_set <- cbind(local_test_set, col_i = pred_test_i)
  colnames(local_test_set)[ncol(local_test_set)] <- paste0("output_", col_i)
  
  rf <- randomForest(x = local_train_set, y = Y_train[, col_i], ntree = 1000)
  pred_train_i <- predict(rf, local_train_set) 
  pred_test_i  <- predict(rf, local_test_set) 
  
  all_test_preds[[col_i]] <- pred_test_i
  print(mlr3measures::rmse(Y_test[, col_i], pred_test_i))
}

chained_pred <- all_test_preds %>% as.data.table() %>% as.matrix.data.frame()
correct_chain_test <- apply(chained_pred, 1, smart_integer_round) %>% t()

show_custom_metrics(chained_pred, "Chained неприведенное")
show_custom_metrics(correct_chain_test, "Chained приведенное")
```


## 3.5 lm

```{r}
lm_model <- lm(cbind(HL_1, HL_2, HL_3, HL_4, HL_5, HL_6) ~ ., 
               data = data.frame(X_train, Y_train)
               # , na.action = na.omit
               )

lm_pred <- predict(lm_model, newdata = data.frame(X_test, Y_test))
correct_lm_test <- apply(lm_pred, 1, smart_integer_round) %>% t()

# show_custom_metrics(lm_pred, "lm неприведенное")
show_custom_metrics(correct_lm_test, "lm приведенное")
```


## 3.6 Catboost

### Независимое

```{r}
new_X_train <- X_train %>% 
  copy() %>% 
  as.data.table() %>%
  .[, names(.SD) := lapply(.SD, \(x) as.double(x))]

new_X_test <- X_test %>% 
  copy() %>% 
  as.data.table() %>%
  .[, names(.SD) := lapply(.SD, \(x) as.double(x))]

indep_catb_pred <- vector("list", 6)
for (feature_num in 1:6) {
  train_pool <- catboost.load_pool(data = new_X_train, label = Y_train[, feature_num])
  test_pool  <- catboost.load_pool(data = new_X_test,  label = Y_test[, feature_num])
      
  catb_model <- catboost.train(
    learn_pool = train_pool,
    test_pool = test_pool,
    params = list(loss_function = 'RMSE', logging_level = "Silent")
  )
  
  indep_catb_pred[[feature_num]] <- catboost.predict(catb_model, test_pool)
}

indep_catb_pred <- indep_catb_pred %>% as.data.table() %>% as.matrix()
correct_indep_catb_pred <- apply(indep_catb_pred, 1, smart_integer_round) %>% t()

# show_custom_metrics(indep_catb_pred, "Неприведенное независимое Catboost")
show_custom_metrics(correct_indep_catb_pred, "Приведенное независимое Catboost")
```

### Chained

```{r}
col_i <- 1

local_train_set <- new_X_train
local_test_set <- new_X_test

train_pool <- catboost.load_pool(data = local_train_set, label = Y_train[, col_i])
test_pool  <- catboost.load_pool(data = local_test_set,  label = Y_test[, col_i])
  
catb_model <- catboost.train(
  learn_pool = train_pool,
  test_pool = test_pool,
  params = list(loss_function = 'RMSE', logging_level = "Silent")
)

pred_train_i <- catboost.predict(catb_model, train_pool)
pred_test_i  <- catboost.predict(catb_model, test_pool)
mlr3measures::rmse(Y_test[, col_i], pred_test_i)

all_test_preds <- NULL
all_test_preds[[col_i]] <- pred_test_i
```

```{r}
for (col_i in 2:6) {
  local_train_set <- cbind(local_train_set, col_i = pred_train_i)
  colnames(local_train_set)[ncol(local_train_set)] <- paste0("output_", col_i)
  
  local_test_set <- cbind(local_test_set, col_i = pred_test_i)
  colnames(local_test_set)[ncol(local_test_set)] <- paste0("output_", col_i)
  
  train_pool <- catboost.load_pool(data = local_train_set, label = Y_train[, col_i])
  test_pool  <- catboost.load_pool(data = local_test_set,  label = Y_test[, col_i])
    
  catb_model <- catboost.train(
    learn_pool = train_pool,
    test_pool = test_pool,
    params = list(loss_function = 'RMSE', logging_level = "Silent")
  )
  
  pred_train_i <- catboost.predict(catb_model, train_pool)
  pred_test_i  <- catboost.predict(catb_model, test_pool)
  
  all_test_preds[[col_i]] <- pred_test_i
  print(mlr3measures::rmse(Y_test[, col_i], pred_test_i))
}

chained_pred <- all_test_preds %>% as.data.table() %>% as.matrix.data.frame()
correct_chain_test <- apply(chained_pred, 1, smart_integer_round) %>% t()

# show_custom_metrics(chained_pred, "Catboost. Chained неприведенное")
show_custom_metrics(correct_chain_test, "Catboost. Chained приведенное")
```


# 4. PCA

```{r}
pca_model <- prcomp(X_train, center = TRUE, scale. = TRUE)

# cumsum(pca_model$sdev^2 / sum(pca_model$sdev^2))
data.table(
  k = 1:length(pca_model$sdev),
  ssq = pca_model$sdev^2
) %>% 
  .[, cumsum := cumsum(ssq / sum(ssq))] %>% 
  .[]

plot(pca_model$sdev^2 / sum(pca_model$sdev^2), 
     type = "b", 
     xlab = "Главные компоненты", 
     ylab = "Доля объясненной дисперсии")
```

```{r}
pca_scaler <- function(X_df, pca_model_, k = 32) {
  res <- scale(X_df, center = pca_model_$center, scale = pca_model_$scale) %*% 
    pca_model_$rotation %>% 
    .[, 1:k]
  return(res)
}
```

## lm

Зависимость k (~PCA) и результатов lm:
```{r}
check_k_pca_lm <- function(k) {
  X_train_scaled <- pca_scaler(X_train, pca_model, k)
  X_test_scaled  <- pca_scaler(X_test,  pca_model, k)
  
  lm_model <- lm(cbind(HL_1, HL_2, HL_3, HL_4, HL_5, HL_6) ~ ., 
                 data = data.frame(X_train_scaled, Y_train))
  
  lm_pred <- predict(lm_model, newdata = data.frame(X_test_scaled, Y_test)) %>% 
    apply(., 1, smart_integer_round) %>% 
    t()
  
  aRMSE_ <- sapply(1:nrow(Y_test), 
                   \(i) mlr3measures::rmse(lm_pred[i, ], Y_test[i, ])) %>% mean()
  return(c(k = k, aRMSE = aRMSE_))
}

res_k_lm <- sapply(1:ncol(X_train), check_k_pca_lm) %>% 
  t() %>% 
  as.data.table()

plotly::plot_ly(res_k_lm, x = ~k, y = ~aRMSE, type = 'scatter', mode = 'lines+markers')
```

```{r}
best_k <- res_k_lm[aRMSE == min(aRMSE), k]
X_train_scaled <- pca_scaler(X_train, pca_model, k = best_k)
X_test_scaled  <- pca_scaler(X_test,  pca_model, k = best_k)

lm_model <- lm(cbind(HL_1, HL_2, HL_3, HL_4, HL_5, HL_6) ~ ., 
               data = data.frame(X_train_scaled, Y_train))

lm_pred <- predict(lm_model, newdata = data.frame(X_test_scaled, Y_test))
correct_lm_test <- apply(lm_pred, 1, smart_integer_round) %>% t()

show_custom_metrics(lm_pred, "lm неприведенное")
show_custom_metrics(correct_lm_test, "lm приведенное")
```


## Random Forest

```{r}
check_k_pca_RF <- function(k) {
  X_train_scaled <- pca_scaler(X_train, pca_model, k)
  X_test_scaled  <- pca_scaler(X_test,  pca_model, k)
  
  models <- list()
  
  for (feature_num in 1:6) {
    models[[feature_num]] <- randomForest(
      x = X_train_scaled,
      y = Y_train[, feature_num],
      ntree = 1000
    )
  }
  
  rf_pred <- sapply(1:6, \(i) predict(models[[i]], data.frame(X_test_scaled, Y_test))) %>% 
    apply(., 1, smart_integer_round) %>% 
    t()
  
  aRMSE_ <- sapply(1:nrow(Y_test), 
                   \(i) mlr3measures::rmse(rf_pred[i, ], Y_test[i, ])) %>% mean()
  return(c(k = k, aRMSE = aRMSE_))
}

res_k_rf <- sapply(seq(2, ncol(X_train), by = 3), check_k_pca_RF) %>% 
  t() %>% 
  as.data.table()

plotly::plot_ly(res_k_rf, x = ~k, y = ~aRMSE, type = 'scatter', mode = 'lines+markers')
```

```{r}
best_k <- res_k_rf[aRMSE == min(aRMSE), k]
X_train_scaled <- pca_scaler(X_train, pca_model, k = best_k)
X_test_scaled  <- pca_scaler(X_test,  pca_model, k = best_k)
  
models <- list()

for (feature_num in 1:6) {
  models[[feature_num]] <- randomForest(
    x = X_train_scaled,
    y = Y_train[, feature_num],
    ntree = 1000
  )
}

rf_pred <- sapply(1:6, function(t) {
  predict(models[[t]], data.frame(X_test_scaled, Y_test))
})

correct_rf_pred <- apply(rf_pred, 1, smart_integer_round) %>% t()
show_custom_metrics(rf_pred, "Random Forest неприведенное")
show_custom_metrics(correct_rf_pred, "Random Forest приведенное")
```


## XGBoost
```{r}
indep_pred <- vector("list", 6)
for (feature_num in 1:6) {
  xgb_model <- xgboost(data = X_train_scaled, label = Y_train[, feature_num],
                       nrounds = 15, objective = "reg:squarederror", eval_metric = "rmse",
                       verbose = 0)
  
  indep_pred[[feature_num]] <- predict(xgb_model, X_test_scaled)
}

indep_pred <- indep_pred %>% as.data.table() %>% as.matrix()
correct_indep_pred <- apply(indep_pred, 1, smart_integer_round) %>% t()

show_custom_metrics(indep_pred, "PCA + XGBoost. Неприведенное независимое")
show_custom_metrics(correct_indep_pred, "PCA + XGBoost. Приведенное независимое")
```


## Catboost

```{r}
indep_catb_pred <- vector("list", 6)
for (feature_num in 1:6) {
  train_pool <- catboost.load_pool(data = X_train_scaled, label = Y_train[, feature_num])
  test_pool  <- catboost.load_pool(data = X_test_scaled,  label = Y_test[, feature_num])
      
  catb_model <- catboost.train(
    learn_pool = train_pool,
    test_pool = test_pool,
    params = list(loss_function = 'RMSE', logging_level = "Silent")
  )
  
  indep_catb_pred[[feature_num]] <- catboost.predict(catb_model, test_pool)
}

indep_catb_pred <- indep_catb_pred %>% as.data.table() %>% as.matrix()
correct_indep_catb_pred <- apply(indep_catb_pred, 1, smart_integer_round) %>% t()

show_custom_metrics(indep_catb_pred, "PCA + Catboost. Неприведенное независимое")
show_custom_metrics(correct_indep_catb_pred, "PCA + Catboost. Приведенное независимое")
```


# 5. Multilabel classification

```{r}
bool_mask_row <- function(matrix_row) {
  ind <- matrix_row %>% order(decreasing = TRUE) %>% .[1:3]
  b_vec <- rep(FALSE, 6)
  b_vec[ind] <- TRUE
  return(b_vec)
}

targets_bool <- apply(targets, 1, bool_mask_row) %>% t()

.[Y_b_train, Y_b_test] <- list(targets_bool[split_idx, ], targets_bool[!split_idx, ])
```

```{r}
library(arrow)

targets_bool %>% 
  as.data.table() %>% 
  rename_all(~colnames(targets)) %>% 
  arrow::write_feather("../0. Data/Y_bool.feather")
```

