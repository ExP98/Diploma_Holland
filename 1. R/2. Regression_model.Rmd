---
title: "Регрессионные модели"
author: "Egor Glushkov"
---

# 1. Библиотеки и функции
```{r include=FALSE}
library(dotty)
library(caret)
library(plotly)
library(arrow)

set.seed(42)
source(paste0(here::here(), "/1. R/4. Data_preparation.R"))
source(paste0(here::here(), "/1. R/5. Regression_classes_functions.R"))
```


# 2. Данные

Разделение данных на трейн и тест:
```{r}
features <- wide_data %>% 
  copy() %>% 
  .[, .SD, .SDcols = !(names(wide_data) %like% "HL|HL2|id")] %>% 
  as.matrix()

targets <- wide_data %>% 
  copy() %>% 
  .[, .SD, .SDcols = names(wide_data) %like% "HL_"] %>% 
  as.matrix()

split_idx <- sample(c(TRUE, FALSE), nrow(features), replace = TRUE, prob = c(0.8, 0.2))

.[X_train_unscaled, X_test_unscaled] <- list(features[split_idx, ], features[!split_idx, ])
.[Y_train, Y_test] <- list(targets[split_idx, ], targets[!split_idx, ])

mean_train <- apply(X_train_unscaled, 2, mean, na.rm = TRUE)
sd_train <- apply(X_train_unscaled, 2, sd, na.rm = TRUE)

# X_train <- X_train_unscaled
# X_test <- X_test_unscaled

X_train <- scale(X_train_unscaled, center = mean_train, scale = sd_train)
X_test  <- scale(X_test_unscaled, center = mean_train, scale = sd_train)
```

valid
```{r}
split_idx <- sample(c(TRUE, FALSE), nrow(features), replace = TRUE, prob = c(0.6, 0.4))

.[X_train_unscaled, X_test_unscaled] <- list(features[split_idx, ], features[!split_idx, ])
.[Y_train, Y_test] <- list(targets[split_idx, ], targets[!split_idx, ])

mean_train <- apply(X_train_unscaled, 2, mean, na.rm = TRUE)
sd_train <- apply(X_train_unscaled, 2, sd, na.rm = TRUE)

X_train <- scale(X_train_unscaled, center = mean_train, scale = sd_train)
X_test  <- scale(X_test_unscaled, center = mean_train, scale = sd_train)

nn <- round(nrow(X_test)/2)
.[X_test, X_valid] <- list(X_test[1:nn, ], X_test[(nn+1):nrow(X_test), ])
.[Y_test, Y_valid] <- list(Y_test[1:nn, ], Y_test[(nn+1):nrow(X_test), ])
```


# 3. Модель


## Выбор предикторов
Stepwise (back, forward), L2-regression

```{r}
### Метод 1: Многомерная пошаговая регрессия (отдельные модели для каждого таргета)
.[lm_ind_pred, lm_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_stepwise_lm_model, X_train, Y_train, X_test, Y_test, T, F
)
```

Признаки после выполнения пошаговой регрессии
```{r}
features_stepw <- sapply(1:6, \(i) stepwise_results[[i]]$model %>% colnames() %>% .[. != "y"])
# reduce(features_stepw, union)
```

```{r}
### Метод 4: Многомерный случайный лес
rf_predictions <- matrix(0, nrow = nrow(X_test), ncol = ncol(Y_test))
colnames(rf_predictions) <- colnames(Y_test)

for(i in 1:ncol(Y_train)) {
  df <- data.frame(X_train, y = Y_train[,i])
  rf_model <- randomForest(y ~ ., data = df)
  rf_predictions[,i] <- predict(rf_model, X_test)
}

show_custom_metrics(rf_predictions, "RF", Y_test_ = Y_test)
```


Проба
```{r}
models_df <- data.table(
  model = c(my_LightGBM_model, my_XGBoost_model, my_RandomForest_model),
  name = c("LightGBM", "XGBoost", "Random Forest")
)

MO_res <- models_df %>% 
  copy() %>% 
  .[, stack_pred := map(model, \(mdl) perform_MO_regression(mdl, "stack"))] %>% 
  .[, chain_pred := map(model, \(mdl) perform_MO_regression(mdl, "chain"))] %>% 
  .[, (c("stack_rmse", "chain_rmse")) := map(.SD, \(col) map_dbl(col, \(pred) df_metric(pred, Y_test, my_rmse))),
    .SDcols = c("stack_pred", "chain_pred")] %>%
  .[, (c("stack_cos", "chain_cos")) := map(.SD, \(col) map_dbl(col, \(pred) df_metric(pred, Y_test, cosine_dist))),
    .SDcols = c("stack_pred", "chain_pred")] %>% 
  .[, (c("stack_Cind", "chain_Cind")) := map(.SD, \(col) map_dbl(col, \(pred) df_metric(pred, Y_test, calc_C_index))),
    .SDcols = c("stack_pred", "chain_pred")]
```


Попытка сделать настоящий стэкинг:
```{r}
x <- MO_res %>% 
  copy() %>% 
  .[, .(name, stack_pred, id = 1)] %>% 
  dcast(id ~ name, value.var = "stack_pred") %>% 
  .[, id := NULL]


code_res_1 <- x %>% 
  copy() %>% 
  .[, names(.SD) := lapply(.SD, \(col) col[[1]][, 1])] %>% 
  unnest(cols = c(LightGBM, `Random Forest`, XGBoost)) %>% 
  as.data.table() %>% 
  .[, paste0(names(.SD), "_resid") := lapply(.SD, \(col) col - Y_test[, 1])]

RF_resid_pred <- my_RandomForest_model$new(code_res_1, Y_test[, 1])
RF_resid_pred$pred_train
my_rmse(RF_resid_pred$pred_train, Y_test[, 1])
```
Возможно, стоит просто собрать все фичи + ответы всех алгоримтов по первому (2му, 3му, 4му...) коду, обучить на этом очередной алгоритм.


## 3.1 Константный предсказатель
```{r}
# const_pred <- matrix(rep(rep(7, times = 6), each = nrow(Y_test)), nrow = nrow(Y_test))
const_pred <- matrix(rep(colMeans(Y_train), each = nrow(Y_test)), nrow = nrow(Y_test))

show_custom_metrics(const_pred, "Константное")
show_custom_metrics(const_pred %>% prediction_correction(), "Константное")
```


## 3.2 XGBoost

### Число раундов
Как выглядит модель, сколько раундов требуется:
```{r}
ii <- 1

res <- vector("list", 20)
for (i in 1:20) {
  xgb_model <- xgboost(data = X_train, label = Y_train[, ii],
                       nrounds = i, objective = "reg:squarederror", eval_metric = "rmse", verbose = 0)
  
  pred_y <- predict(xgb_model, X_test)
  res[[i]] <- data.frame(
    nrounds = i,
    test_rmse = my_rmse(Y_test[, ii], pred_y),
    test_round_rmse = my_rmse(Y_test[, ii], pred_y %>% round())
  )
}

res %>% rbindlist() %>% .[["test_rmse"]] %>% plot(main = "test_rmse")
res %>% rbindlist() %>% .[["test_round_rmse"]] %>% plot(main = "test_round_rmse")
```
15 раундов хватает.

```{r}
.[xgb_ind_pred, xgb_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_XGBoost_model, X_train, Y_train, X_test, Y_test, T, F
)

# perform_stack_MO_regression(my_XGBoost_model, X_train, Y_train, X_test, Y_test, T, F)
# perform_chain_MO_regression(my_XGBoost_model, X_train, Y_train, X_test, Y_test, T, F)
```

```{r}
lapply(xgb_ind_pred, \(x) x$calc_importance())
```
Для XGBoost (и позже для Random Forest) самая важная фича -- CT_1 (открытость-замкнутость)

Порядок предсказания может иметь значение!


## 3.3 LightGBM

```{r}
.[lgbm_ind_pred, lgbm_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_LightGBM_model, X_train, Y_train, X_test, Y_test, T, F
)
```


### Feature Importance
```{r}
LightGBM_models <- vector("list", 6)
for (col_i in 1:6) {
  LightGBM_models[[col_i]] <- my_LightGBM_model$new(X_train, Y_train[, col_i], X_test, Y_test_[, col_i])
}

res <- lapply(1:6, \(i) lgb.importance(LightGBM_models[[i]]$model, percentage = TRUE)[, .(col = i, Feature, Gain)]) %>% 
  rbindlist() %>% 
  dcast(col ~ Feature, value.var = "Gain", fill = 0) %>% 
  .[, -c("col")]

importance_df <- data.table(
  feature = colnames(res),
  avg_imp = res %>% colMeans()
) %>% 
  .[order(-avg_imp)] %>% 
  .[, cumsum_imp := cumsum(avg_imp)] %>% 
  .[cumsum_imp <= 0.55] %>% 
  .[, names(.SD) := round(.SD, 3), .SDcols = c("avg_imp", "cumsum_imp")]

xlsx::write.xlsx(importance_df, "temporal.xlsx")
```


### Cross Validation

```{r}
.[lgbm_cv_ind_pred, lgbm_cv_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_LightGBM_CV_model, X_train, Y_train, X_test, Y_test, T, F
)
```


## 3.4 Random Forest

```{r}
.[rf_ind_pred, rf_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_RandomForest_model, X_train, Y_train, X_test, Y_test, T, F
)
```


## 3.5 lm

```{r}
lm_model <- lm(cbind(HL_1, HL_2, HL_3, HL_4, HL_5, HL_6) ~ ., 
               data = data.frame(X_train, Y_train)
               # , na.action = na.omit
               )

lm_ind_pred <- predict(lm_model, newdata = X_test %>% as.data.frame()) %>% prediction_correction()
show_custom_metrics(lm_ind_pred, "lm")
```

```{r}
.[lm_ind_pred, lm_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_lm_model, X_train, Y_train, X_test, Y_test, T, F
)
```


```{r}
.[lm_stepw_ind_pred, lm_stepw_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_stepwise_lm_model, X_train, Y_train, X_test, Y_test, T, F
)
```

```{r}
lm_L2_pred <- get_L1_L2_glmnet_preds(alpha = 0) # Ridge
lm_L1_pred <- get_L1_L2_glmnet_preds(alpha = 1) # Lasso
```


При этом по отдельности результат хуже
```{r}
perform_stack_MO_regression(my_L1_L2_glmnet_model, X_train, Y_train, X_test, Y_test, T, alpha = 0)
perform_stack_MO_regression(my_L1_L2_glmnet_model, X_train, Y_train, X_test, Y_test, T, alpha = 1)
```


## 3.6 CatBoost

```{r}
.[catb_ind_pred, catb_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_Catboost_model, X_train %>% apply(2, as.double), Y_train, X_test %>% apply(2, as.double), Y_test, T, F
)
```


## 3.7 kNN

```{r}
for (k in c(2, 5, 7, 10, 12)) {
  message(paste0("k = ", k))
  perform_stack_MO_regression(my_knn_model, X_train, Y_train, X_test, Y_test, T, F, k = k)
}
```


## 3.8 SVR
```{r}
.[svr_ind_pred, svr_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_SVR_model, X_train, Y_train, X_test, Y_test, T, F
)
```


## 3.9 Stacking 

```{r}
library(stacking)

Method <- list(glmnet = data.frame(alpha = c(0.5, 0.8), lambda = c(0.1, 1)),
               pls = data.frame(ncomp = 5))

stacking_train_result <- NULL

for (i in 1:6) {
  stacking_train_result[[i]] <- stacking_train(
    X = X_train,
    Y = Y_train[, i],
    Method = Method,
    Metamodel = "glmnet",
    core = 2,
    cross_validation = FALSE,
    use_X = TRUE,
    TrainEachFold = TRUE,
    num_sample = 3,
    proportion = 0.8
  ) 
}


st_pred <- lapply(1:6, \(i) stacking_predict(newX = X_test, stacking_train_result[[i]]))
st_pred %>% 
  as.data.table() %>% 
  as.matrix() %>% 
  show_custom_metrics(case_name = "Stacking", need_to_correct = TRUE)
```


```{r}
library(MASS)
library(caret)
library(rpart)

set.seed(123)
final_pred <- NULL

for (hol_i in 1:6) {
  y_train <- Y_train[, hol_i]
  y_test <- Y_test[, hol_i]
  folds <- createFolds(y_train, k = 5, list = TRUE)
  
  base_preds <- matrix(0, nrow = nrow(X_train), ncol = 5)
  colnames(base_preds) <- c("lasso", "ridge", "lightgbm", "catboost", "rf")
  
  # Обучение базовых моделей с out-of-fold предсказаниями
  for (i in 1:length(folds)) {
    fold_idx <- folds[[i]]
    
    # Разделение данных на тренировочную и валидационную части
    X_fold_train <- X_train[-fold_idx, ]
    y_fold_train <- y_train[-fold_idx]
    X_fold_val <- X_train[fold_idx, ]
    
    # L1-регрессия (Lasso)
    lasso <- cv.glmnet(X_fold_train, y_fold_train, alpha = 1)
    base_preds[fold_idx, "lasso"] <- predict(lasso, X_fold_val, s = "lambda.min")
    
    # L2-регрессия (Ridge)
    ridge <- cv.glmnet(X_fold_train, y_fold_train, alpha = 0)
    base_preds[fold_idx, "ridge"] <- predict(ridge, X_fold_val, s = "lambda.min")
    
    # LightGBM
    dtrain <- lgb.Dataset(data = X_fold_train, label = y_fold_train)
    params <- list(objective = "regression", metric = "rmse", num_iterations = 250)
    lgb_model <- lgb.train(params, dtrain, 100, verbose = -1)
    base_preds[fold_idx, "lightgbm"] <- predict(lgb_model, X_fold_val)
    
    # CatBoost
    pool_train <- catboost.load_pool(data = X_fold_train, label = y_fold_train)
    pool_val <- catboost.load_pool(data = X_fold_val)
    cat_model <- catboost.train(pool_train, NULL, params = list(loss_function = 'RMSE', logging_level = "Silent"))
    base_preds[fold_idx, "catboost"] <- catboost.predict(cat_model, pool_val)
    
    # Random Forest
    rf_model <- randomForest(X_fold_train, y_fold_train, ntree = 1000)
    base_preds[fold_idx, "rf"] <- predict(rf_model, X_fold_val)
  }
  
  # Обучение мета-модели на предсказаниях базовых моделей
  meta_train <- data.frame(base_preds, target = y_train)
  meta_model <- lm(target ~  ., data = meta_train)
  
  # Переобучение базовых моделей на полных данных
  final_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
  final_ridge <- cv.glmnet(X_train, y_train, alpha = 0)
  final_rf <- randomForest(X_train, y_train, ntree = 1000)
  
  # LightGBM final
  dtrain_full <- lgb.Dataset(data = X_train, label = y_train)
  lgb_final <- lgb.train(params, dtrain_full, 100, verbose = -1)
  
  # CatBoost final
  pool_full <- catboost.load_pool(X_train, label = y_train)
  cat_final <- catboost.train(pool_full, NULL, 
                              params = list(loss_function = 'RMSE', logging_level = "Silent"))
  
  # Предсказания на тестовых данных
  test_preds <- data.table(
    lasso = predict(final_lasso, X_test, s = "lambda.min"),
    ridge = predict(final_ridge, X_test, s = "lambda.min"),
    lightgbm = predict(lgb_final, X_test),
    catboost = catboost.predict(cat_final, catboost.load_pool(X_test)),
    rf = predict(final_rf, X_test)
  ) %>% 
    setnames(c("lasso", "ridge", "lightgbm", "catboost", "rf"))
  
  # Финальное предсказание мета-модели
  final_pred[[hol_i]] <- predict(meta_model, newdata = test_preds)
}
```

```{r}
final_pred %>% 
  as.data.table() %>% 
  as.matrix() %>% 
  show_custom_metrics(case_name = "Stacking", need_to_correct = TRUE)
```

Стэкинг из lm, rpart, randomForest: Stacking. aRMSE: 1.98; aCindex: 9.926


## 3.9 Результирущий датасет с предсказаниями
          
```{r}
models_df <- data.table(
  model = c(NA, my_lm_model, NA, NA, my_XGBoost_model, my_LightGBM_model, my_Catboost_model,
            my_RandomForest_model, my_knn_model, my_SVR_model),
  name = c("const", "lm", "Lasso L1", "Ridge L2", "XGBoost", "LightGBM", "CatBoost", 
           "Random Forest", "kNN", "SVR"),
  need_MO = c(F, T, F, F, rep(T, 6))
)

MO_res <- models_df %>% 
  copy() %>% 
  .[need_MO == TRUE, pred := map(model, \(mdl) perform_MO_regression(mdl, "stack"))] %>% 
  .[name == "const", pred := map(model, \(mdl) matrix(rep(colMeans(Y_train), each = nrow(Y_test)), nrow = nrow(Y_test)))] %>% 
  .[name == "Lasso L1", pred := map(model, \(mdl) get_L1_L2_glmnet_preds(alpha = 1))] %>% 
  .[name == "Ridge L2", pred := map(model, \(mdl) get_L1_L2_glmnet_preds(alpha = 0))] %>% 
  .[, rmse := map_dbl(pred, \(x) df_metric(x, Y_test, my_rmse) %>% round(3))] %>% 
  .[, cos := map_dbl(pred, \(x) df_metric(x, Y_test, cosine_dist) %>% round(3))] %>% 
  .[map_lgl(pred, \(item) !is.null(item)), C_index := map_dbl(pred, \(x) df_metric(x, Y_test, calc_C_index) %>% round(3))]

MO_res[, -c("model", "need_MO", "pred", "cos")]
MO_res[order(-C_index), -c("model", "need_MO", "pred", "cos")]

xlsx::write.xlsx(MO_res[, -c("model", "need_MO", "pred", "cos")], "temporal.xlsx")
```


# 4. PCA

```{r}
pca_model <- prcomp(X_train, center = TRUE, scale. = TRUE)

# cumsum(pca_model$sdev^2 / sum(pca_model$sdev^2))
data.table(
  k = 1:length(pca_model$sdev),
  ssq = pca_model$sdev^2
) %>% 
  .[, cumsum := cumsum(ssq / sum(ssq))] %>% 
  .[]

plot(pca_model$sdev^2 / sum(pca_model$sdev^2), 
     type = "b", 
     xlab = "Главные компоненты", 
     ylab = "Доля объясненной дисперсии")
```


## 4.1 lm

Зависимость k (~PCA) и результатов lm:
```{r}
res_k_lm1 <- get_k_PCA_model_table(my_lm_model, by = 1, metric_f = c_index_dist)
res_k_lm2 <- get_k_PCA_model_table(my_lm_model, by = 1, metric_f = c_index_dist)
```

k = 29
```{r}
best_k <- res_k_lm1[avg_metric == min(avg_metric), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[lm_pca_ind_pred, lm_pca_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_lm_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)
```


## 4.2 Random Forest

Подбор лучшего k для PCA + Random Forest

```{r}
res_k_rf1 <- get_k_PCA_model_table(my_RandomForest_model, by = 3, metric_f = c_index_dist)
res_k_rf2 <- get_k_PCA_model_table(my_RandomForest_model, by = 3, metric_f = my_rmse)
```

k = 11 (rmse), 32 (C-ind)
```{r}
best_k <- res_k_rf1[avg_metric == min(avg_metric), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[rf_pca_ind_pred, rf_pca_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_RandomForest_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)

# best_k <- res_k_rf2[avg_metric == min(avg_metric), k]
# print(str_glue("Best k: {best_k}"))
# X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
# X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)
# 
# .[rf_pca_ind_pred, rf_pca_chain_pred] <- map(
#   list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
#   my_RandomForest_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
# )
```


## 4.3 XGBoost

```{r}
res_k_xgb1 <- get_k_PCA_model_table(my_XGBoost_model, by = 2, metric_f = my_rmse)
res_k_xgb2 <- get_k_PCA_model_table(my_XGBoost_model, by = 2, metric_f = c_index_dist)
```

k = 26 (rmse); 32 (C-index)
```{r}
# best_k <- res_k_xgb1[avg_metric == min(avg_metric), k]
# print(str_glue("Best k: {best_k}"))
# X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
# X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)
# 
# .[xgb_pca_ind_pred, xgb_pca_chain_pred] <- map(
#   list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
#   my_XGBoost_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
# )

best_k <- res_k_xgb2[avg_metric == min(avg_metric), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[xgb_pca_ind_pred, xgb_pca_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_XGBoost_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)
```


## 4.4 Catboost

```{r}
res_k_catb1 <- get_k_PCA_model_table(my_Catboost_model, by = 4, metric_f = my_rmse)
# res_k_catb2 <- get_k_PCA_model_table(my_Catboost_model, by = 4, metric_f = c_index_dist)
```

k = 30 (rmse); 22 (C-index)
```{r}
# best_k <- res_k_catb1[avg_metric == min(avg_metric), k]
best_k <- 30
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[catb_pca_ind_pred, catb_pca_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_Catboost_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)
```


```{r}
best_k <- 22
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[catb_pca_ind_pred, catb_pca_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_Catboost_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)
```


## 4.5 LightGBM

```{r}
res_k_lgb1 <- get_k_PCA_model_table(my_LightGBM_model, by = 3, metric_f = my_rmse)
res_k_lgb2 <- get_k_PCA_model_table(my_LightGBM_model, by = 3, metric_f = c_index_dist)
```

k = 11 (rmse); 8 (C-index)
```{r}
# best_k <- res_k_lgb[avg_metric == min(avg_metric), k]
best_k <- 11
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[lgb_pca_ind_pred, lgb_pca_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_LightGBM_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)
```


## 4.6 LightGBM CV

```{r}
res_k_lgb_cv <- get_k_PCA_model_table(my_LightGBM_CV_model, by = 3, metric_f = c_index_dist)
```

k = 11 (rmse); 17 (C-index)
```{r}
best_k <- res_k_lgb_cv[avg_metric == min(avg_metric), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[lgb_cv_pca_ind_pred, lgb_cv_pca_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_LightGBM_CV_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)
```


## 4.7 kNN

```{r}
res_k_knn1 <- get_k_PCA_model_table(my_knn_model, by = 1, metric_f = my_rmse)
res_k_knn2 <- get_k_PCA_model_table(my_knn_model, by = 1, metric_f = c_index_dist)
```

k = 25 (rmse); 19 (C-index)
```{r}
best_k <- res_k_knn1[avg_metric == min(avg_metric), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[knn_pca_ind_pred] <- map(
  list(perform_stack_MO_regression), exec,
  my_knn_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)

best_k <- res_k_knn2[avg_metric == min(avg_metric), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[knn_pca_ind_pred] <- map(
  list(perform_stack_MO_regression), exec,
  my_knn_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)
```


## 4.8 SVR

```{r}
res_k_svr1 <- get_k_PCA_model_table(my_SVR_model, by = 1, metric_f = my_rmse)
res_k_svr2 <- get_k_PCA_model_table(my_SVR_model, by = 1, metric_f = c_index_dist)
```

k = 40 (rmse); 26 (C-index)
```{r}
best_k <- res_k_svr1[avg_metric == min(avg_metric), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[svr_pca_ind_pred, svr_pca_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_SVR_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)

best_k <- res_k_svr2[avg_metric == min(avg_metric), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[svr_pca_ind_pred, svr_pca_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_SVR_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)
```

## 4.9 Lasso and Ridge

only alpha 1 (lasso)
```{r}
res_k_glmnet <- get_k_PCA_model_table(my_L1_L2_glmnet_model, by = 1, metric_f = my_rmse)
res_k_glmnet <- get_k_PCA_model_table(my_L1_L2_glmnet_model, by = 1, metric_f = c_index_dist)
```

k = 22 (rmse); 23 (C-index)
```{r}
best_k <- res_k_glmnet[avg_metric == min(avg_metric), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[glmnet_pca_ind_pred, glmnet_pca_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_L1_L2_glmnet_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)
```

```{r}
L1_pca_pred <- get_L1_L2_glmnet_preds(X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca, alpha = 1)
L2_pca_pred <- get_L1_L2_glmnet_preds(X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca, alpha = 0)
```


# 5. Ensemble

## 5.1 Стэкинг моделей на данных без пропусков

### Без PCA
const_pred, xgb_ind_pred, xgb_chain_pred, lgbm_ind_pred, lgbm_chain_pred, lgbm_cv_ind_pred, lgbm_cv_chain_pred, rf_ind_pred, rf_chain_pred, lm_ind_pred, catb_ind_pred, catb_chain_pred

```{r}
MO_res[order(rmse), -c("model", "need_MO", "pred")]
MO_res[order(-C_index), -c("model", "need_MO", "pred")]
```

1. Проба на случайных
```{r}
scalar_matrix_production(w_vec = c(0.5, 0.5), list(const_pred, xgb_ind_pred)) %>% 
  show_custom_metrics(., "Const & Xgb", Y_test_ = Y_test)

scalar_matrix_production(w_vec = c(0.5, 0.5), MO_res[name %in% c("const", "XGBoost"), pred]) %>%
  show_custom_metrics(., "Const & Xgb", Y_test_ = Y_test)
```

2. Все или почти все вместе
```{r}
models_pred <- list(xgb_ind_pred, xgb_chain_pred, lgbm_ind_pred, lgbm_chain_pred, lgbm_cv_ind_pred, lgbm_cv_chain_pred, rf_ind_pred, rf_chain_pred, lm_ind_pred, catb_ind_pred, catb_chain_pred)

w <- rep(1/length(models_pred), length(models_pred))
scalar_matrix_production(w_vec = w, models_pred) %>% show_custom_metrics("All", Y_test_ = Y_test)

models_pred2 <- MO_res[name != "const_round", pred]
w2 <- rep(1/length(models_pred2), length(models_pred2))
scalar_matrix_production(w_vec = w2, models_pred2) %>% 
  show_custom_metrics("All", Y_test_ = Y_test)
```

3. Выборочные старые ансамбли: 
3.1
```{r}
selected_models_pred <- list(xgb_ind_pred, lgbm_chain_pred, lgbm_cv_ind_pred, rf_ind_pred, lm_ind_pred, catb_ind_pred, catb_chain_pred)
w <- c(1, 5, 5, 5, 1, 2, 6)
scalar_matrix_production(w_vec = w, selected_models_pred) %>% show_custom_metrics("Selected", Y_test_ = Y_test)


selected_models_pred <- MO_res[name %in% c("XGBoost", "LightGBM", "Random Forest", "lm", "CatBoost"), pred]
w <- c(1, 10, 5, 1, 8)
scalar_matrix_production(w_vec = w, selected_models_pred) %>% show_custom_metrics("Selected", Y_test_ = Y_test)
```

3.2
```{r}
selected2_models_pred <- list(lgbm_chain_pred, lgbm_cv_ind_pred, rf_ind_pred, catb_ind_pred, catb_chain_pred)
w <- rep(1/length(selected2_models_pred), length(selected2_models_pred))
scalar_matrix_production(w_vec = w, selected2_models_pred) %>% show_custom_metrics("Selected 2", Y_test_ = Y_test)

selected2_models_pred <- MO_res[name %in% c("LightGBM", "Random Forest", "CatBoost"), pred]
w <- rep(1/length(selected2_models_pred), length(selected2_models_pred))
scalar_matrix_production(w_vec = w, selected2_models_pred) %>% show_custom_metrics("Selected 2", Y_test_ = Y_test)
```

3.3
```{r}
selected3_models_pred <- list(lgbm_ind_pred, rf_ind_pred, lm_ind_pred, catb_ind_pred)

w <- c(1, 1, 1, 3)
scalar_matrix_production(w_vec = w, selected3_models_pred) %>% show_custom_metrics("Selected 3.1", Y_test_ = Y_test)

w <- rep(1/length(selected3_models_pred), length(selected3_models_pred))
scalar_matrix_production(w_vec = w, selected3_models_pred) %>% show_custom_metrics("Selected 3.2", Y_test_ = Y_test)


selected3_models_pred <- MO_res[name %in% c("LightGBM", "Random Forest", "lm", "CatBoost"), pred]

w <- c(1, 1, 1, 3)
scalar_matrix_production(w_vec = w, selected3_models_pred) %>% show_custom_metrics("Selected 3.3", Y_test_ = Y_test)

w <- rep(1/length(selected3_models_pred), length(selected3_models_pred))
scalar_matrix_production(w_vec = w, selected3_models_pred) %>% show_custom_metrics("Selected 3.4", Y_test_ = Y_test)
```

4. Выборочные новые ансамбли

c("const", "const_round", "lm", "Lasso L1", "Ridge L2", "XGBoost", "LightGBM", "CatBoost", "Random Forest", "kNN", "SVR")
```{r}
models_pred1 <- MO_res[name %in% c("CatBoost", "Lasso L1", "LightGBM", "Random Forest", "Ridge L2"), pred]
w1.1 <- rep(1/length(models_pred1), length(models_pred1))
w1.2 <- c(10, 4, 1, 1, 1)
w1.3 <- c(11, 6, 1, 2, 2)
w1.4 <- c(0.6, 0.25, 0.05, 0.05, 0.05)

scalar_matrix_production(w_vec = w1.1, models_pred1) %>% show_custom_metrics("1.1", Y_test_ = Y_test)
scalar_matrix_production(w_vec = w1.2, models_pred1) %>% show_custom_metrics("1.2", Y_test_ = Y_test)
scalar_matrix_production(w_vec = w1.3, models_pred1) %>% show_custom_metrics("1.3", Y_test_ = Y_test)
scalar_matrix_production(w_vec = w1.4, models_pred1) %>% show_custom_metrics("1.4", Y_test_ = Y_test)
```

```{r}
models_pred2 <- MO_res[name %in% c("Random Forest", "LightGBM", "kNN", "SVR", "Lasso L1", "Ridge L2"), pred]
w2.1 <- rep(1/length(models_pred2), length(models_pred2))
w2.2 <- c(7, 5, 3, 1, 1, 1)
w2.3 <- c(4, 3, 1, 0, 0, 0)
w2.4 <- c(7, 3, 1, 0, 0, 0)

scalar_matrix_production(w_vec = w2.1, models_pred2) %>% show_custom_metrics("2.1", Y_test_ = Y_test)
scalar_matrix_production(w_vec = w2.2, models_pred2) %>% show_custom_metrics("2.2", Y_test_ = Y_test)
scalar_matrix_production(w_vec = w2.3, models_pred2) %>% show_custom_metrics("2.3", Y_test_ = Y_test)
scalar_matrix_production(w_vec = w2.4, models_pred2) %>% show_custom_metrics("2.4", Y_test_ = Y_test)
```

```{r}
models_pred3 <- MO_res[name %in% c("Random Forest", "LightGBM", "Lasso L1"), pred]
w3.1 <- rep(1/length(models_pred3), length(models_pred3))
w3.2 <- c(5, 5, 1)
w3.3 <- c(5, 5, 0)

scalar_matrix_production(w_vec = w3.1, models_pred3) %>% show_custom_metrics("3.1", Y_test_ = Y_test)
scalar_matrix_production(w_vec = w3.2, models_pred3) %>% show_custom_metrics("3.2", Y_test_ = Y_test)
scalar_matrix_production(w_vec = w3.3, models_pred3) %>% show_custom_metrics("3.3", Y_test_ = Y_test)
```

```{r}
models_pred4 <- MO_res[name %in% c("SVR", "LightGBM", "Lasso L1"), pred]
w4.1 <- rep(1/length(models_pred4), length(models_pred4))
w4.2 <- c(1, 5, 5)
w4.3 <- c(0, 1, 1)

scalar_matrix_production(w_vec = w4.1, models_pred4) %>% show_custom_metrics("4.1", Y_test_ = Y_test)
scalar_matrix_production(w_vec = w4.2, models_pred4) %>% show_custom_metrics("4.2", Y_test_ = Y_test)
scalar_matrix_production(w_vec = w4.3, models_pred4) %>% show_custom_metrics("4.3", Y_test_ = Y_test)
```


5. Выборочные ансамбли без округления
```{r}
models_pred5 <- MO_res[name %in% c("SVR", "lm", "LightGBM", "Lasso L1"), pred]
w5.1 <- rep(1/length(models_pred5), length(models_pred5))
w5.2 <- c(1, 1, 10, 10)
w5.3 <- c(0, 0, 10, 10)

scalar_matrix_production(w_vec = w5.1, models_pred5) %>% show_custom_metrics("5.1", Y_test_ = Y_test)
scalar_matrix_production(w_vec = w5.2, models_pred5) %>% show_custom_metrics("5.2", Y_test_ = Y_test)
scalar_matrix_production(w_vec = w5.3, models_pred5) %>% show_custom_metrics("5.3", Y_test_ = Y_test)
```

Выделим следующие ансамбли:
1) all - 1.935 - 9.632
2) RF* , LightGBM*, kNN - 1.913 - 10.132
3) CatBoost* , L1*, LightGBM, RF, L2 - 1.911 - 10.118 


### С PCA
lm_pca_ind_pred, lm_pca_chain_pred, rf_pca_ind_pred, rf_pca_chain_pred, xgb_pca_ind_pred, xgb_pca_chain_pred, catb_pca_ind_pred, catb_pca_chain_pred, lgb_pca_ind_pred, lgb_pca_chain_pred, lgb_cv_pca_ind_pred, lgb_cv_pca_chain_pred

```{r}
# models_pred <- list(lm_pca_ind_pred, lm_pca_chain_pred, rf_pca_ind_pred, rf_pca_chain_pred, xgb_pca_ind_pred, xgb_pca_chain_pred, catb_pca_ind_pred, catb_pca_chain_pred, lgb_pca_ind_pred, lgb_pca_chain_pred, lgb_cv_pca_ind_pred, lgb_cv_pca_chain_pred)
models_pred1 <- list(lm_pca_ind_pred, lm_pca_chain_pred, rf_pca_ind_pred, rf_pca_chain_pred, xgb_pca_ind_pred, xgb_pca_chain_pred, catb_pca_ind_pred, catb_pca_chain_pred, lgb_pca_ind_pred, lgb_pca_chain_pred)
models_pred2 <- list(lm_pca_ind_pred, rf_pca_ind_pred, xgb_pca_ind_pred, catb_pca_ind_pred, lgb_pca_ind_pred)

# new
models_pred3 <- list(lm_pca_ind_pred, rf_pca_ind_pred, rf_pca_chain_pred, xgb_pca_ind_pred, catb_pca_ind_pred, lgb_pca_ind_pred, knn_pca_ind_pred, svr_pca_ind_pred, L1_pca_pred, L2_pca_pred)

w <- rep(1/length(models_pred1), length(models_pred1))
scalar_matrix_production(w_vec = w, models_pred1) %>% show_custom_metrics("All", Y_test_ = Y_test)

w <- rep(1/length(models_pred2), length(models_pred2))
scalar_matrix_production(w_vec = w, models_pred2) %>% show_custom_metrics("All", Y_test_ = Y_test)

w <- rep(1/length(models_pred3), length(models_pred3))
scalar_matrix_production(w_vec = w, models_pred3) %>% show_custom_metrics("All", Y_test_ = Y_test)
```


```{r}
selected_models_pred <- list(xgb_pca_ind_pred, lgb_pca_chain_pred, rf_pca_ind_pred, lm_pca_ind_pred, catb_pca_ind_pred, catb_pca_chain_pred)

w <- c(1, 5, 5, 1, 2, 6)
scalar_matrix_production(w_vec = w, selected_models_pred) %>% show_custom_metrics("Selected", Y_test_ = Y_test)
```

```{r}
selected2_models_pred <- list(lgb_pca_chain_pred, rf_pca_ind_pred, catb_pca_ind_pred, catb_pca_chain_pred)

w <- rep(1/length(selected2_models_pred), length(selected2_models_pred))
scalar_matrix_production(w_vec = w, selected2_models_pred) %>% show_custom_metrics("Selected 2", Y_test_ = Y_test)
```

models_pred3 <- list(lm_pca_ind_pred, rf_pca_ind_pred, rf_pca_chain_pred, xgb_pca_ind_pred, catb_pca_ind_pred, lgb_pca_ind_pred, knn_pca_ind_pred, svr_pca_ind_pred, L1_pca_pred, L2_pca_pred)
```{r}
selected3_models_pred <- list(rf_pca_ind_pred, lgb_pca_ind_pred, knn_pca_ind_pred)

w <- c(7, 3, 1)
# w <- rep(1/length(selected3_models_pred), length(selected3_models_pred))
scalar_matrix_production(w_vec = w, selected3_models_pred) %>% show_custom_metrics("Selected 3", Y_test_ = Y_test)
```

```{r}
selected4_models_pred <- list(catb_pca_ind_pred, L1_pca_pred, lgb_pca_ind_pred, rf_pca_ind_pred, L2_pca_pred)
w <- c(0.6, 0.25, 0.05, 0.05, 0.05)
scalar_matrix_production(w_vec = w, selected4_models_pred) %>% show_custom_metrics("Selected 4", Y_test_ = Y_test)
```



## 5.2 Стэкинг моделей отдельно на психологических тестах

Найти наилучшую модель для каждого из тестов. Мб миксовать с PCA.

```{r}
prepare_psytest_dataset_by_split <- function(input_dt, psytest_label = c("BF", "EY", "CT", "LN", "SC")) {
  psytest_label <- match.arg(psytest_label)
  
  input_dt <- input_dt %>% 
    copy() %>% 
    .[, .SD, .SDcols = names(input_dt) %like% paste0(psytest_label, "|HL")] %>% 
    drop_na()
  
  split_idx <- sample(c(TRUE, FALSE), nrow(input_dt), replace = TRUE, prob = c(0.8, 0.2))
  
  .[X_train_unscaled, X_test_unscaled] <- list(
    input_dt[split_idx, .SD, .SDcols = names(input_dt) %like% psytest_label], 
    input_dt[!split_idx, .SD, .SDcols = names(input_dt) %like% psytest_label])
  
  .[Y_train, Y_test] <- list(input_dt[split_idx, .SD, .SDcols = names(input_dt) %like% "HL_"], 
                             input_dt[!split_idx, .SD, .SDcols = names(input_dt) %like% "HL_"]) %>% 
    lapply(as.matrix)
  
  mean_train <- apply(X_train_unscaled, 2, mean, na.rm = TRUE)
  sd_train <- apply(X_train_unscaled, 2, sd, na.rm = TRUE)
  
  X_train <- scale(X_train_unscaled, center = mean_train, scale = sd_train)
  X_test  <- scale(X_test_unscaled, center = mean_train, scale = sd_train)
  
  if (nrow(X_train) == 0 | nrow(X_test) == 0) error("X_train or X_test are empty!")
  
  return(list(X_train = X_train, Y_train = Y_train, X_test = X_test, Y_test = Y_test))
}
```

Важно: обучаем на новых потестовых данных (wide_data2), оцениваем на wide_data (старые данные, которые есть по всем тестам, потому в конце можно оценивать ансамбль на них же).


### Catboost
Для примера берем Catboost и обучаем его на данных каждого и тестов по отдельности.

1. BF
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "BF")

.[BF_catb_ind, BF_catb_chain] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_Catboost_model, df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]], T, F
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

2. EY
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "EY")

.[EY_catb_ind, EY_catb_chain] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_Catboost_model, df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]], T, F
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```


3. CT
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "CT")

.[CT_catb_ind, CT_catb_chain] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_Catboost_model, df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]], T, F
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

4. LN
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "LN")

.[LN_catb_ind, LN_catb_chain] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_Catboost_model, df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]], T, F
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

5. SC
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "SC")

.[SC_catb_ind, SC_catb_chain] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_Catboost_model, df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]], T, F
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

Ансамбль
```{r}
models_pred <- list(BF_catb_chain, EY_catb_ind, CT_catb_ind, LN_catb_ind, SC_catb_ind)

w1 <- rep(1/length(models_pred), length(models_pred))
w2 <- c(1, 1, 10, 5, 2) # BF 2.203 (1), EY 2.203 (1), CT 2.089 (10), LN 2.149 (5), SC 2.192 (2)

scalar_matrix_production(w_vec = w1, models_pred) %>% 
  show_custom_metrics("All equal", Y_test_ = df[["Y_test"]])

scalar_matrix_production(w_vec = w2, models_pred) %>% 
  show_custom_metrics("All preferences", Y_test_ = df[["Y_test"]])
```
Catboost:
All equal. aRMSE: 2.117; aCosDist: 0.288
All preferences. aRMSE: 2.083; aCosDist: 0.284


### Random Forest

1. BF
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "BF")

.[BF_rf_ind, BF_rf_chain] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_RandomForest_model, df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]], T, F
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

2. EY
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "EY")

.[EY_rf_ind, EY_rf_chain] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_RandomForest_model, df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]], T, F
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```


3. CT
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "CT")

.[CT_rf_ind, CT_rf_chain] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_RandomForest_model, df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]], T, F
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

4. LN
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "LN")

.[LN_rf_ind, LN_rf_chain] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_RandomForest_model, df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]], T, F
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

5. SC
```{r}
df <- prepare_psytest_dataset(wide_data, wide_data2, "SC")

.[SC_rf_ind, SC_rf_chain] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_RandomForest_model, df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]], T, F
)

rep(colMeans(df[["Y_train"]]), each = nrow(df[["Y_test"]])) %>% 
  matrix(nrow = nrow(df[["Y_test"]])) %>% 
  show_custom_metrics("Константное", Y_test_ = df[["Y_test"]])
```

Ансамбль
```{r}
# list(BF_rf_chain 2.243, EY_rf_ind 2.241, CT_rf_ind 2.089, LN_rf_ind 2.193, SC_rf_ind 2.205)
models_pred <- list(BF_rf_chain, EY_rf_ind, CT_rf_ind, LN_rf_ind, SC_rf_ind)

w1 <- rep(1/length(models_pred), length(models_pred))
w2 <- c(1, 1, 6, 2, 2)
w3 <- c(1, 1, 10, 2, 2)

scalar_matrix_production(w_vec = w1, models_pred) %>% 
  show_custom_metrics("All equal", Y_test_ = df[["Y_test"]])

scalar_matrix_production(w_vec = w2, models_pred) %>% 
  show_custom_metrics("All preferences", Y_test_ = df[["Y_test"]])

scalar_matrix_production(w_vec = w3, models_pred) %>% 
  show_custom_metrics("All preferences 2", Y_test_ = df[["Y_test"]])
```

### Пример поиска лучшей модели для одного теста

Перечислим все модели:
```{r}
one_test_model_map <- function(model, df_ = df) {
  x <- map(
    list(perform_stack_MO_regression), exec, # perform_chain_MO_regression
    model, df_[["X_train"]], df_[["Y_train"]], df_[["X_test"]], df_[["Y_test"]], T, F
  )
}


show_constant_pred_metric <- function(df_) {
  res <- rep(colMeans(df_[["Y_train"]]), each = nrow(df_[["Y_test"]])) %>% 
    matrix(nrow = nrow(df_[["Y_test"]])) %>% 
    show_custom_metrics("Константное", Y_test_ = df_[["Y_test"]])
  return(res)
}


class_models <- data.table(
  models = list(my_lm_model, my_XGBoost_model, my_LightGBM_model, my_Catboost_model, my_RandomForest_model, my_knn_model, my_SVR_model),
  labels = list("lm", "XGBoost", "LightGBM", "CatBoost", "Random Forest", "kNN", "SVR")
)


# class_models2 <- data.table(
#   models = list(my_lm_model),
#   labels = list("lm")
# )
```

c("BF", "EY", "CT", "LN", "SC")
```{r}
sink("model_log_noPCA.txt", append = TRUE, split = TRUE)

models_by_tests <- NULL

for (test_lbl in c("BF", "EY", "CT", "LN", "SC")) {
  cat(test_lbl, "\n")
  # for (pca_b in c(TRUE, FALSE)) {
    # pca_lbl <- if (pca_b) "PCA" else "no PCA"
    
    # cat(pca_lbl, "\n")
    df <- prepare_psytest_dataset(wide_data, wide_data2, test_lbl, use_PCA_b = FALSE)
    # models_by_tests[[test_lbl]][[pca_lbl]] <- map(class_models$models, one_test_model_map, df_ = df)
    
    for (i in 1:nrow(class_models)) {
      models_by_tests[[test_lbl]][[class_models[i, labels][[1]]]] <- one_test_model_map(class_models[i, models][[1]], df)
    }
    
    # models_by_tests[[test_lbl]] <- map(class_models$models, one_test_model_map, df_ = df)
    models_by_tests[[test_lbl]][["Lasso"]] <- get_L1_L2_glmnet_preds(df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]], alpha = 1)
    models_by_tests[[test_lbl]][["Ridge"]] <- get_L1_L2_glmnet_preds(df[["X_train"]], df[["Y_train"]], df[["X_test"]], df[["Y_test"]], alpha = 0)
    show_constant_pred_metric(df) %>% cat()
    cat("\n")
  # }
  # cat("\n\n")
}

sink()
```

```{r}
for (test_lbl in c("BF", "EY", "CT", "LN", "SC")) {
  cat(test_lbl, "\n")
  df <- prepare_psytest_dataset(wide_data, wide_data2, test_lbl, use_PCA_b = FALSE)
  show_constant_pred_metric(df) %>% cat()
  cat("\n")
}
```

```{r}
for (cur_label in c("BF", "EY", "CT", "LN", "SC")) {
  df <- prepare_psytest_dataset(wide_data, wide_data2, cur_label, use_PCA_b = FALSE)
  mbt <- models_by_tests[[cur_label]]
  
  scalar_matrix_production(w_vec = rep(1/length(mbt), length(mbt)), lapply(names(mbt), \(ii) pluck(x, ii, 1))) %>% 
    show_custom_metrics("All equal", Y_test_ = df[["Y_test"]]) %>% 
    print()
}
```

```{r}
# models_pred <- list(x[[1]][[1]], x[[2]][[2]], x[[3]][[1]], x[[4]][[2]], x[[5]][[1]], x[[6]][[1]])
models_pred <- list(x[[1]][[1]], x[[2]][[2]], x[[3]][[1]], x[[4]][[1]], x[[5]][[1]], x[[6]][[1]])

w1 <- rep(1/length(models_pred), length(models_pred))
w2 <- c(1, 2, 2, 2, 9, 2)

scalar_matrix_production(w_vec = w1, models_pred) %>% 
  show_custom_metrics("All equal", Y_test_ = df[["Y_test"]])

scalar_matrix_production(w_vec = w2, models_pred) %>% 
  show_custom_metrics("All preferences", Y_test_ = df[["Y_test"]])
```

LN (no PCA):
```{r}
models_pred2 <- list(x[[1]][[1]], x[[2]][[1]], x[[3]][[2]], x[[4]][[1]], x[[5]][[1]], x[[6]][[1]])
w3 <- c(1, 3, 3, 3, 3, 13)

scalar_matrix_production(w_vec = w3, models_pred2) %>% 
  show_custom_metrics("All preferences", Y_test_ = df[["Y_test"]])
```


# 6. Multilabel classification

```{r}
bool_mask_row <- function(matrix_row) {
  ind <- matrix_row %>% order(decreasing = TRUE) %>% .[1:3]
  b_vec <- rep(FALSE, 6)
  b_vec[ind] <- TRUE
  return(b_vec)
}

targets_bool <- apply(targets, 1, bool_mask_row) %>% t()
.[Y_b_train, Y_b_test] <- list(targets_bool[split_idx, ], targets_bool[!split_idx, ])
```

randomForest
```{r}
ind_pred <- vector("list", 6)

for (col_i in 1:6) {
  rf <- randomForest(x = X_train, y = as.factor(Y_b_train[, col_i]), ntree = 1000, importance = TRUE) 
  ind_pred[[col_i]] <- predict(rf, X_test, type = "prob")[, "TRUE"]
}

## Для C-index
classif_ind_pred <- ind_pred %>% as.data.table() %>% as.matrix()
df_metric(classif_ind_pred, Y_test, func = calc_C_index)


## Для метрик классификации
Y_pred_rf <- ind_pred %>% as.data.table() %>% apply(., 1, bool_mask_row) %>% t()

# Случайные предсказания
Y_rand <- replicate(n = nrow(Y_b_test), sample(c(rep(TRUE, 3), rep(FALSE, 3)), size = 6)) %>% t()
```
C-index. Результат аналогичен RF regr (что логично и ожидаемо)


Catboost
```{r}
ind_pred <- vector("list", 6)

for (col_i in 1:6) {
  train_pool <- catboost.load_pool(data = X_train, label = Y_b_train[, col_i] %>% as.integer)
  test_pool  <- catboost.load_pool(data = X_test,  label = Y_b_test[, col_i] %>% as.integer)
  
  model <- catboost.train(
    learn_pool = train_pool,
    test_pool = test_pool,
    params = list(loss_function = 'CrossEntropy', logging_level = "Silent") # Logloss
  )
  
  # preds <- catboost.predict(model, test_pool, prediction_type = "Class")
  ind_pred[[col_i]] <- catboost.predict(model, test_pool, prediction_type = "Probability")
}

Y_pred_cb <- ind_pred %>% as.data.table() %>% apply(., 1, bool_mask_row) %>% t()

## Для C-index
classif_ind_pred <- ind_pred %>% as.data.table() %>% as.matrix()
df_metric(classif_ind_pred, Y_test, func = calc_C_index)
```


## Метрики

1. Hamming Loss - Доля неправильно предсказанных меток
```{r}
# доля неправильных ответов. От 0 до 1
hamming_loss <- function(y_true, y_pred) {
  mean(y_true != y_pred)
}

hamming_loss(Y_b_test, Y_pred_rf) # 0.377451
hamming_loss(Y_b_test, Y_pred_cb) # 0.377451
hamming_loss(Y_b_test, Y_rand) # 0.5392157
# hamming_loss(Y_b_test, Y_b_test)
# hamming_loss(Y_b_test, !Y_b_test)
```

2. Top-k Accuracy

RF
```{r}
# Расчет точности
mean(rowSums(Y_pred_rf * Y_b_test) >= 1)  # Хотя бы одна верная - 0.9852941
mean(rowSums(Y_pred_rf * Y_b_test) >= 2)  # Хотя бы 2 верные    - 0.7352941
mean(rowSums(Y_pred_rf * Y_b_test) >= 3)  # Все 3 верные        - 0.1470588

# последнее аналогично Subset Accuracy (Exact Match) - Доля объектов с полностью правильными метками
subset_accuracy <- function(y_true, y_pred) {
  mean(apply(y_true == y_pred, 1, all))
}

subset_accuracy(Y_b_test, Y_pred_rf) # 0.1470588
```

Catboost
```{r}
# Расчет точности
mean(rowSums(Y_pred_cb * Y_b_test) >= 1) # 0.9852941
mean(rowSums(Y_pred_cb * Y_b_test) >= 2) # 0.7205882
mean(rowSums(Y_pred_cb * Y_b_test) >= 3) # 0.1617647

subset_accuracy(Y_b_test, Y_pred_cb)
```

Rand
```{r}
# Расчет точности
mean(rowSums(Y_rand * Y_b_test) >= 1)  # Хотя бы одна верная - 0.8970588
mean(rowSums(Y_rand * Y_b_test) >= 2)  # Хотя бы 2 верные    - 0.4264706
mean(rowSums(Y_rand * Y_b_test) >= 3)  # Все 3 верные        - 0.05882353

subset_accuracy(Y_b_test, Y_rand)
```


3. Precision, Recall, F1-Score
```{r}
# library(Metrics)

calc_class_metrics <- function(y_true, y_pred) {
  answ <- data.table(y_true = y_true, y_pred = y_pred)
  TP <- answ[y_true == TRUE & y_pred == TRUE, .N]
  FP <- answ[y_true == FALSE & y_pred == TRUE, .N]
  FN <- answ[y_true == TRUE & y_pred == FALSE, .N]
  TN <- answ[y_true == FALSE & y_pred == FALSE, .N]
  
  return(list(
    recall = TP / (TP + FN),
    precision = TP / (TP + FP),
    acc = (TP + TN) / (TP + FP + FN + TN),
    F1_score = (2 * TP) / (2 * TP + FP + FN)
  ))
}

# calc_class_metrics(Y_b_test[, 1], Y_pred[, 1])ф
```

```{r}
get_confus_elems <- function(y_true, y_pred) {
  answ <- data.table(y_true = y_true, y_pred = y_pred)
  TP <- answ[y_true == TRUE & y_pred == TRUE, .N]
  FP <- answ[y_true == FALSE & y_pred == TRUE, .N]
  FN <- answ[y_true == TRUE & y_pred == FALSE, .N]
  TN <- answ[y_true == FALSE & y_pred == FALSE, .N]
  return(list(TP = TP, FP = FP, FN = FN, TN = TN))   
}

get_total_confus_dt <- function(Y_true, Y_pred) {
  conf_dt <- sapply(1:6, \(i) get_confus_elems(Y_true[, i], Y_pred[, i])) %>% 
    t() %>% 
    as.data.table() %>% 
    unnest(cols = c(TP, FP, FN, TN))
  return(conf_dt)
}

recall_    <- function(conf_row) conf_row[["TP"]] / (conf_row[["TP"]] + conf_row[["FN"]])
precision_ <- function(conf_row) conf_row[["TP"]] / (conf_row[["TP"]] + conf_row[["FP"]])
F1_score_  <- function(conf_row) {
  return((2 * conf_row[["TP"]]) / (2 * conf_row[["TP"]] + conf_row[["FP"]] + conf_row[["FN"]]))
}

micro_metric <- \(total_conf_dt, metric_f) colSums(total_conf_dt) %>% metric_f()
macro_metric <- \(total_conf_dt, metric_f) apply(total_conf_dt, 1, metric_f) %>% mean()

calc_multiclass_metrics <- function(Y_true, Y_pred) {
  total_confus_dt <- get_total_confus_dt(Y_true, Y_pred)
  return(list(
    micro_precision = micro_metric(total_confus_dt, precision_),
    micro_recall    = micro_metric(total_confus_dt, recall_),
    micro_F1_score  = micro_metric(total_confus_dt, F1_score_),
    
    macro_precision = macro_metric(total_confus_dt, precision_),
    macro_recall    = macro_metric(total_confus_dt, recall_),
    macro_F1_score  = macro_metric(total_confus_dt, F1_score_)
  ))
}
```

```{r}
bind_rows(
  RF = calc_multiclass_metrics(Y_b_test, Y_pred_rf),
  CB = calc_multiclass_metrics(Y_b_test, Y_pred_cb),
  rand = calc_multiclass_metrics(Y_b_test, Y_rand)
) %>% mutate(Y = c("RF", "CB", "rand"), .before = 1)
```

4. Jaccard Similarity Score
Сходство между метками:
```{r}
jaccard_score <- function(y_true, y_pred) {
  intersection <- rowSums(y_true * y_pred)
  union <- rowSums((y_true + y_pred) > 0)
  mean(intersection / union)
}

jaccard_score(Y_b_test, Y_pred_rf) # 0.4867647
jaccard_score(Y_b_test, Y_pred_cb) # 0.4941176
jaccard_score(Y_b_test, Y_rand) # 0.3632353
```


# 7. Python

```{r}
wide_data %>% 
  as.data.table() %>% 
  arrow::write_feather("../0. Data/wide_data.feather")

targets_bool %>% 
  as.data.table() %>% 
  rename_all(~colnames(targets)) %>% 
  arrow::write_feather("../0. Data/Y_bool.feather")
```


```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor, RegressorChain
# from sklearn.metrics import root_mean_squared_error

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor 
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
```

```{python}
def rmse(y_true, y_pred):
    mean_squared_error = np.mean((y_true - y_pred) ** 2)  # Среднее значение квадратов ошибок
    return np.sqrt(mean_squared_error)


def check_MO_regression(model_obj, model_name=""):
    MO_regr = {'stack': MultiOutputRegressor, 'Chain': RegressorChain}
    for name, model in MO_regr.items():
        y_pred = model(model_obj).fit(X_train, y_train).predict(X_test)
        print(f"{model_name} {name}. RMSE: {round(rmse(y_test, y_pred), 3)}")
    return(None)


def plot_nn_losses(hist):
    plt.plot(hist.history['loss'], label='Train Loss')
    plt.plot(hist.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
    return(None)
```

```{python}
data = pd.read_feather("0. Data/wide_data.feather")

X = data.loc[:, ~data.columns.isin(["HL_1", "HL_2", "HL_3", "HL_4", "HL_5", "HL_6", "id"])]
y = data.loc[:, "HL_1":"HL_6"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# y_bool = pd.read_feather("../0. Data/Y_bool.feather")

# y_b_train, y_b_test = train_test_split(y_bool, test_size=0.2, random_state=42)
```

```{python}
linear = LinearRegression().fit(X_train, y_train)
rf = RandomForestRegressor().fit(X_train, y_train)
etr = ExtraTreesRegressor().fit(X_train, y_train)
knn = KNeighborsRegressor().fit(X_train, y_train)

print(f"Linear. RMSE: {rmse(y_test, linear.predict(X_test))}")
print(f"ExtraTree. RMSE: {rmse(y_test, etr.predict(X_test))}")
print(f"Random Forest. RMSE: {rmse(y_test, rf.predict(X_test))}")
print(f"kNN. RMSE: {rmse(y_test, knn.predict(X_test))}")
```


# 8. Распределение и сбалансированность значений кодов Голланда

Распределение значений кодов HL_1:HL_6 предсказаний:
```{r}
df_long_preds <- as.data.table(rf_ind_pred) %>%  # lm_L1_pred
  setnames(paste0("HL_", 1:6)) %>% 
  .[, id := .I] %>% 
  pivot_longer(cols = HL_1:HL_6, names_to = "Group", values_to = "Value")


ggplot(df_long_preds, aes(x = Value, color = Group, fill = Group)) +
  geom_density(alpha = 0.2) +
  labs(title = "Накладывающиеся графики плотности предсказаний", x = "Значение", y = "Плотность") +
  theme_minimal()

ggplot(df_long_preds, aes(x = Value, color = Group, fill = Group)) +
  geom_density(alpha = 0.2) +
  labs(title = "Накладывающиеся графики плотности предсказаний", x = "Значение", y = "Плотность") +
  facet_wrap(~ Group) +
  theme_minimal()

ggplot(df_long_preds, aes(x = Value, y = Group, fill = stat(x))) +
  geom_density_ridges_gradient() +
  scale_fill_viridis_c(name = "Temp.", option = "C") +
  labs(title = "Накладывающиеся графики плотности предсказаний", x = "Значение", y = "Плотность") +
  theme_minimal()
```

Распределение значений топ1:топ6 предсказаний:
```{r}
value_rank_df <- df_long_preds %>% 
  as.data.table() %>% 
  .[order(id, -Value)] %>% 
  .[, rank := paste0("rank_", 1:.N), by = .(id)] %>% 
  .[, .(Value, rank)]

ggplot(value_rank_df, aes(x = Value, color = rank, fill = rank)) +
  geom_density(alpha = 0.2) +
  labs(title = "Накладывающиеся графики плотности", x = "Значение", y = "Плотность") +
  facet_wrap(~ rank) +
  theme_minimal()

ggplot(value_rank_df, aes(x = Value, y = rank, fill = stat(x))) +
  geom_density_ridges_gradient() +
  scale_fill_viridis_c(name = "Temp.", option = "C") +
  labs(title = "Частоты значений кодов предсказаний по убыванию") +
  theme_minimal()
```

# 9. Попытка решения задачи другими способами

## 9.1 Предсказание главного кода + все остальные
1. Предсказание главного кода i, затем с вероятностью p_ij предсказывается код j.
Как предсказывать третий код? Или второй максимум в p_ij, или максимум по комбинации двух кодов.

```{r}
untd_targets_bool <- untd_dt %>% 
  .[, .SD, .SDcols = patterns("HL_")] %>% 
  apply(1, bool_mask_row) 

cooccur_mat <- untd_targets_bool %*% t(untd_targets_bool)
diag(cooccur_mat) <- 0
cooccur_mat <- cooccur_mat / rowSums(cooccur_mat)
```

```{r}
predict_linkage_rnd_max <- function(cooccur_mat, first_ind = NULL) {
  if (is.null(first_ind)) first_ind <- sample(1:6, 1)
  second_ind <- cooccur_mat[first_ind, ] %>% which.max()
  
  third_probs <- cooccur_mat[c(first_ind, second_ind), ] %>% colSums()
  third_probs[c(first_ind, second_ind)] <- 0
  third_ind <- third_probs %>% which.max()
  
  return(c(first_ind, second_ind, third_ind))
}


predict_linkage_rnd_prob <- function(cooccur_mat, first_ind = NULL) {
  if (is.null(first_ind)) first_ind <- sample(1:6, 1) 
  second_ind <- sample(1:6, size = 1, prob = cooccur_mat[first_ind, ])
  
  third_probs <- cooccur_mat[c(first_ind, second_ind), ] %>% colSums()
  third_probs[c(first_ind, second_ind)] <- 0
  third_ind <- sample(1:6, size = 1, prob = third_probs)
  
  return(c(first_ind, second_ind, third_ind))
}


predict_by_linkage_metrics <- function(cooccur_mat, first_inds_vec = NULL, avg_prob_b = TRUE, n_repeat = 10^2) {
  rnd_max_score <- sapply(1:nrow(Y_test),
                          \(i) calc_C_index_threes(
                            predict_linkage_rnd_max(cooccur_mat, first_inds_vec[i]),
                            get_max_indx(Y_test[i, ])
                          )) %>% mean()
  
  # rnd_prob_score <- sapply(1:nrow(Y_test),
  #                          \(i) calc_C_index_threes(
  #                            predict_linkage_rnd_prob(cooccur_mat, first_inds_vec[i]),
  #                            get_max_indx(Y_test[i, ])
  #                          )) %>% mean()
  
  if (avg_prob_b == FALSE) n_repeat <- 1
  
  rnd_prob_score <- sapply(1:n_repeat, \(ii) sapply(
    1:nrow(Y_test),
    \(i) calc_C_index_threes(
      predict_linkage_rnd_prob(cooccur_mat, first_inds_vec[i]),
      get_max_indx(Y_test[i, ])
    )
  ) %>% mean()) %>%
    median()
  
  cat(rnd_max_score, rnd_prob_score)
  return(invisible(c(rnd_max_score, rnd_prob_score)))
}
```

Без указания первого главного кода (рандом):
```{r}
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = NULL)
```

### Регрессия
#### PCA
RF:
```{r}
best_k <- 32
pca_model <- prcomp(X_train, center = TRUE, scale. = TRUE)

X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

pred <- perform_stack_MO_regression(my_RandomForest_model, X_train_pca, Y_train, X_test_pca, Y_test, print_metric = TRUE)
first_inds_vec <- apply(pred, 1, \(x) which.max(x)) %>% unname()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```

Lasso:
```{r}
best_k <- 23
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

pred <- perform_stack_MO_regression(my_L1_L2_glmnet_model, X_train_pca, Y_train, X_test_pca, Y_test, print_metric = TRUE)
first_inds_vec <- apply(pred, 1, \(x) which.max(x)) %>% unname()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```

LightGBM
```{r}
best_k <- 31
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

pred <- perform_stack_MO_regression(my_LightGBM_model, X_train_pca, Y_train, X_test_pca, Y_test, print_metric = TRUE)
first_inds_vec <- apply(pred, 1, \(x) which.max(x)) %>% unname()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```


#### без PCA
```{r}
# my_RandomForest_model, my_LightGBM_model
pred <- perform_stack_MO_regression(my_LightGBM_model, X_train, Y_train, X_test, Y_test, print_metric = TRUE)
# pred <- get_L1_L2_glmnet_preds(X_train, Y_train, X_test, Y_test, print_metric = TRUE)
first_inds_vec <- apply(pred, 1, \(x) which.max(x)) %>% unname()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```
RandomForest: 9.761194 9.731343 (orig 9.627)
RandomForest PCA: 9.835821 9.5 (orig 9.657)
my_LightGBM_model: 9.522388 9.470149 (orig 9.687)
my_LightGBM_model: 9.701493 9.723881 (orig 10.045)
Lasso PCA: 9.776119 9.597015 (orig 9.746)
Lasso: 9.701493 9.61194 (orig 10.045)

Вывод: предсказание первой буквы и уже по ней по "статистике" второй и третьей (притом константно) работает лучше рандома, но хуже предсказания регрессией 2 и 3 факторов тройки


### Классификация первого кода
С классификацией первого фактора вместо регрессора с выбором первого:
```{r}
Y_train_classif <- apply(Y_train, 1, \(x) which.max(x)) %>% unname() %>% factor(levels = 1:6)
Y_test_classif <- apply(Y_test, 1, \(x) which.max(x)) %>% unname() %>% factor(levels = 1:6)

model_rf <- randomForest(
  x = X_train,
  y = Y_train_classif,
  ntree = 500,
  importance = TRUE
)

pred <- predict(model_rf, X_test)
# confusionMatrix(pred, Y_test_classif)
first_inds_vec <- pred %>% unname()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```

XGBoost
```{r}
dtrain <- xgb.DMatrix(X_train, label = as.integer(Y_train_classif)-1)
dtest <- xgb.DMatrix(X_test)

params <- list(
  objective = "multi:softprob",
  num_class = 6,
  eval_metric = "mlogloss"
)

model_xgb <- xgb.train(params, dtrain, nrounds = 100)
first_inds_vec <- predict(model_xgb, dtest, reshape = TRUE) %>% max.col()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```

LightGBM:
```{r}
dtrain <- lgb.Dataset(
  data = X_train %>% data.matrix(),
  label = as.integer(Y_train_classif)-1,
  params = list(feature_pre_filter = FALSE)
)

params <- list(
  objective = "multiclass",
  metric = "multi_logloss",
  num_class = 6,
  boosting_type = "gbdt"
)

model_lgb <- lgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  verbose = -1
)

first_inds_vec <- predict(model_lgb, X_test) %>% max.col()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```


PCA:
```{r}
best_k <- 32
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

model_rf <- randomForest(
  x = X_train_pca,
  y = Y_train_classif,
  ntree = 500,
  importance = TRUE
)

pred <- predict(model_rf, X_test_pca)
first_inds_vec <- pred %>% unname()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```


## 9.2. Ранжирование

```{r}
prepare_ranking_data <- function(data) {
  df <- data %>% 
    copy() %>% 
    melt(id.vars = c("id", colnames(features)), measure.vars = colnames(targets), variable.name = "Holland_code", value.name = "Score") %>% 
    .[order(id, Holland_code)] %>% 
    .[, score_rank := rank(Score, ties.method = "random"), by = id]
  return(df)
}


prepare_XGmodel_data <- function(df) {
  model_data <- xgb.DMatrix(data = df[, .SD, .SDcols = colnames(features)] %>% as.matrix(),
                            label = df[["score_rank"]],
                            group = df[, .(n = .N), by = id][, n])
  return(model_data)
}
```

Работает неверно:
```{r}
dtrain <- wide_data[split_idx] %>% prepare_ranking_data() %>% prepare_XGmodel_data()
test_df <- wide_data[!split_idx] %>% prepare_ranking_data()
dtest <- test_df %>% prepare_XGmodel_data()

params <- list(
  objective = "rank:pairwise",
  eval_metric = "ndcg@3",
  eta = 0.1,
  gamma = 1,
  max_depth = 6,
  subsample = 0.8
)

xgb_model <- xgb.train(
  params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain),
  print_every_n = 30
)

scores <- predict(xgb_model, dtest)

test_df %>% 
  copy() %>% 
  .[, scores := scores] %>% 
  .[]
```

```{r}


## 9.4 Nearest neigbour
Показывать результат ближайшего объекта, затем и кластера
```{r}
find_nearest_neighbour <- function(input_vec, X_train_, Y_train_, dist_func = cosine_dist) {
  sim_vec <- sapply(1:nrow(X_train_),
                    \(n_row) dist_func(input_vec, X_train_[n_row, ]) %>% abs())
  return(Y_train_[which.min(sim_vec), ])
}


oneNN <- function(X_test_, X_train_, Y_train_, dist_func = cosine_dist) {
  Y_pred <- apply(X_test_, MARGIN = 1, find_nearest_neighbour, 
                  X_train_ = X_train_, Y_train_ = Y_train_, dist_func = dist_func) %>% t()
  return(Y_pred)
}
```

```{r}
oneNN(X_test, X_train, Y_train, cosine_dist) %>% show_custom_metrics("OneNN. Cosine")
oneNN(X_test, X_train, Y_train, my_rmse) %>% show_custom_metrics("OneNN RMSE")
```
