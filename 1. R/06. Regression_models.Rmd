---
title: "Регрессионные модели"
author: "Egor Glushkov"
---

# 1. Библиотеки и функции
```{r include=FALSE}
# set.seed(142)
# source(paste0(here::here(), "/1. R/00. Source_all.R"))
```


# 2. Данные

Разделение данных на трейн и тест:
```{r}
# .[features, targets] <- separate_X_y(wide_data)
# .[X_train, X_test, Y_train, Y_test, split_idx] <- train_test_split(features, targets)
```


# 3. Модель

## 3.1 Выбор предикторов
Stepwise (back, forward), L2-regression

```{r}
.[lm_ind_pred, lm_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_stepwise_lm_model, X_train, Y_train, X_test, Y_test, T, F
)
```

Признаки после выполнения пошаговой регрессии
```{r}
features_stepw <- sapply(1:6, \(i) stepwise_results[[i]]$model %>% colnames() %>% .[. != "y"])
reduce(features_stepw, union)
```


## 3.2 Важность признаков

GLM
```{r}
glm_imp_1 <- glm_importance(X_train, Y_train, alpha = 1)
glm_imp_2 <- glm_importance(X_train, Y_train, alpha = 0)
```
        
```{r}
models <- list(my_stepwise_lm_model, my_XGBoost_model, my_LightGBM_model, 
               my_Catboost_model, my_RandomForest_model, my_ET_model)

fi_filename <- here("0. Data/1. Output/Feature_importance.xlsx")
translation_names_dict <- get_translation_names_dict(shrt_nms_df)

imps <- c(list(glm_imp_1, glm_imp_2), lapply(models, \(mdl) get_model_importance(mdl)))
model_names <- c("glm_L1", "glm_L2",  lapply(models, \(mdl) mdl$classname)) %>% 
  str_remove_all("my_|_model")

tmp <- map2(imps, model_names, 
  function(imp, model_name) {
    get_feature_importance(imp) %>% 
      merge(translation_names_dict, all.x = TRUE, by.x = "feature", by.y = "short_name") %>% 
      .[order(-avg_imp)] %>% 
      xlsx::write.xlsx(fi_filename, model_name, row.names = FALSE, append = TRUE)
  }
)
```

Часто самая важная фича -- CT_1 (открытость-замкнутость)
Порядок предсказания может иметь значение!


## 3.3 Проверка отдельных параметров моделей

### Константный предсказатель
```{r}
const_int_pred  <- matrix(rep(rep(7, times = 6), each = nrow(Y_test)), nrow = nrow(Y_test))
const_mean_pred <- matrix(rep(colMeans(Y_train), each = nrow(Y_test)), nrow = nrow(Y_test))

show_custom_metrics(const_int_pred,  "Константное")
show_custom_metrics(const_mean_pred, "Константное")
```

### LightGBM Cross Validation

```{r}
.[lgbm_cv_ind_pred, lgbm_cv_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_LightGBM_CV_model, X_train, Y_train, X_test, Y_test, T, F
)
```

### Линейная регрессия (lm)

```{r}
lm_ind_pred <- simple_lm(X_train, Y_train, X_test, Y_test)
show_custom_metrics(lm_ind_pred, "lm")
```

```{r}
.[lm_ind_pred, lm_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_lm_model, X_train, Y_train, X_test, Y_test, T, F
)

.[lm_stepw_ind_pred, lm_stepw_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_stepwise_lm_model, X_train, Y_train, X_test, Y_test, T, F
)
```

```{r}
lm_L2_pred <- regularized_lm(alpha = 0, print_metric = T) # Ridge
lm_L1_pred <- regularized_lm(alpha = 1, print_metric = T) # Lasso
```

При этом по отдельности результат хуже
```{r}
perform_stack_MO_regression(my_regularized_lm_model, X_train, Y_train, X_test, Y_test, T, alpha = 0)
perform_stack_MO_regression(my_regularized_lm_model, X_train, Y_train, X_test, Y_test, T, alpha = 1)
```


### kNN

```{r}
for (k in c(3, seq(5, ncol(X_train), 5))) {
  message(paste0("k = ", k))
  perform_stack_MO_regression(my_knn_model, X_train, Y_train, X_test, Y_test, T, F, k = k)
}
```
Лучше k = 5, 20 или 35


## 3.4 Stacking 

Настоящий стэкинг:
```{r}
# library(stacking)
# library(snow)

Method <- list(glmnet = data.frame(alpha = c(0.5, 0.8), lambda = c(0.1, 1)),
               pls = data.frame(ncomp = 5))

stacking_train_result <- NULL

for (i in 1:6) {
  stacking_train_result[[i]] <- stacking_train(
    X = X_train,
    Y = Y_train[, i],
    Method = Method,
    Metamodel = "glmnet",
    core = 2,
    cross_validation = FALSE,
    use_X = TRUE,
    TrainEachFold = TRUE,
    num_sample = 3,
    proportion = 0.8
  ) 
}

st_pred <- lapply(1:6, \(i) stacking_predict(newX = X_test, stacking_train_result[[i]]))
st_pred %>% 
  as.data.table() %>% 
  as.matrix() %>% 
  show_custom_metrics(case_name = "Stacking")
```


```{r}
library(MASS)

final_pred <- NULL

for (hol_i in 1:6) {
  y_train <- Y_train[, hol_i]
  y_test <- Y_test[, hol_i]
  folds <- createFolds(y_train, k = 5, list = TRUE)
  
  base_preds <- matrix(0, nrow = nrow(X_train), ncol = 5)
  colnames(base_preds) <- c("lasso", "ridge", "lightgbm", "catboost", "rf")
  
  # Обучение базовых моделей с out-of-fold предсказаниями
  for (i in 1:length(folds)) {
    fold_idx <- folds[[i]]
    
    # Разделение данных на тренировочную и валидационную части
    X_fold_train <- X_train[-fold_idx, ]
    y_fold_train <- y_train[-fold_idx]
    X_fold_val <- X_train[fold_idx, ]
    
    # L1-регрессия (Lasso)
    lasso <- cv.glmnet(X_fold_train, y_fold_train, alpha = 1)
    base_preds[fold_idx, "lasso"] <- predict(lasso, X_fold_val, s = "lambda.min")
    
    # L2-регрессия (Ridge)
    ridge <- cv.glmnet(X_fold_train, y_fold_train, alpha = 0)
    base_preds[fold_idx, "ridge"] <- predict(ridge, X_fold_val, s = "lambda.min")
    
    # LightGBM
    dtrain <- lgb.Dataset(data = X_fold_train, label = y_fold_train)
    params <- list(objective = "regression", metric = "rmse", num_iterations = 250)
    lgb_model <- lgb.train(params, dtrain, 100, verbose = -1)
    base_preds[fold_idx, "lightgbm"] <- predict(lgb_model, X_fold_val)
    
    # CatBoost
    pool_train <- catboost.load_pool(data = X_fold_train, label = y_fold_train)
    pool_val <- catboost.load_pool(data = X_fold_val)
    cat_model <- catboost.train(pool_train, NULL, params = list(loss_function = 'RMSE', logging_level = "Silent", allow_writing_files = FALSE))
    base_preds[fold_idx, "catboost"] <- catboost.predict(cat_model, pool_val)
    
    # Random Forest
    rf_model <- randomForest(X_fold_train, y_fold_train, ntree = 1000)
    base_preds[fold_idx, "rf"] <- predict(rf_model, X_fold_val)
  }
  
  # Обучение мета-модели на предсказаниях базовых моделей
  meta_train <- data.frame(base_preds, target = y_train)
  meta_model <- lm(target ~  ., data = meta_train)
  
  # Переобучение базовых моделей на полных данных
  final_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
  final_ridge <- cv.glmnet(X_train, y_train, alpha = 0)
  final_rf <- randomForest(X_train, y_train, ntree = 1000)
  
  # LightGBM final
  dtrain_full <- lgb.Dataset(data = X_train, label = y_train)
  lgb_final <- lgb.train(params, dtrain_full, 100, verbose = -1)
  
  # CatBoost final
  pool_full <- catboost.load_pool(X_train, label = y_train)
  cat_final <- catboost.train(pool_full, NULL, 
                              params = list(loss_function = 'RMSE', logging_level = "Silent",
                                            allow_writing_files = FALSE))
  
  # Предсказания на тестовых данных
  test_preds <- data.table(
    lasso = predict(final_lasso, X_test, s = "lambda.min"),
    ridge = predict(final_ridge, X_test, s = "lambda.min"),
    lightgbm = predict(lgb_final, X_test),
    catboost = catboost.predict(cat_final, catboost.load_pool(X_test)),
    rf = predict(final_rf, X_test)
  ) %>% 
    setnames(c("lasso", "ridge", "lightgbm", "catboost", "rf"))
  
  # Финальное предсказание мета-модели
  final_pred[[hol_i]] <- predict(meta_model, newdata = test_preds)
}
```

```{r}
final_pred %>% 
  as.data.table() %>% 
  as.matrix() %>% 
  show_custom_metrics(case_name = "Stacking", need_to_correct = TRUE)
```

Стэкинг из lm, rpart, randomForest: Stacking. aRMSE: 1.98; aCindex: 9.926


## 3.5 PCA

```{r}
pca_model <- prcomp(X_train, center = TRUE, scale. = TRUE)

# cumsum(pca_model$sdev^2 / sum(pca_model$sdev^2))
data.table(
  k = 1:length(pca_model$sdev),
  ssq = pca_model$sdev^2
) %>% 
  .[, cumsum := cumsum(ssq / sum(ssq))] %>% 
  .[]

plot(pca_model$sdev^2 / sum(pca_model$sdev^2), 
     type = "b", 
     xlab = "Главные компоненты", 
     ylab = "Доля объясненной дисперсии")
```

На примере lm

Зависимость k (~PCA) и результатов lm:
```{r}
res_k_lm1 <- get_k_PCA_model_table(my_lm_model, by = 1, metric_f = c_index_dist)
res_k_lm2 <- get_k_PCA_model_table(my_lm_model, by = 1, metric_f = my_rmse)
```

k = 29
```{r}
best_k <- res_k_lm1[avg_metric == min(avg_metric), k]
print(str_glue("Best k: {best_k}"))
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

.[lm_pca_ind_pred, lm_pca_chain_pred] <- map(
  list(perform_stack_MO_regression, perform_chain_MO_regression), exec,
  my_lm_model, X_train_pca, Y_train, X_test_pca, Y_test, T, F
)
```


# 4. Ансамбли

## 4.1 Взвешенное ансамблирование (блендинг) на данных без пропусков

Какие модели используем (функция/класс, имя, доп. параметры):
```{r}
regr_experiments <- tribble(
  ~model,                   ~name,                  ~MO_type,    ~params,
  my_const_model,           "const",                "none",      list(),
  my_regularized_lm_model,  "Lasso L1",             "none",      list(alpha = 1),
  my_regularized_lm_model,  "Ridge L2",             "none",      list(alpha = 0),
  my_lm_model,              "lm",                   "none",      list(),
  my_stepwise_lm_model,     "stepwise lm",          "stack",     list(),
  my_XGBoost_model,         "XGBoost",              "stack",     list(),
  my_LightGBM_model,        "LightGBM",             "stack",     list(),
  my_Catboost_model,        "CatBoost",             "stack",     list(),
  my_RandomForest_model,    "Random Forest (500)",  "stack",     list(ntree = 500),
  my_RandomForest_model,    "Random Forest (1000)", "stack",     list(ntree = 1000),
  my_knn_model,             "kNN (k = 5)",          "stack",     list(k = 5),
  my_knn_model,             "kNN (k = 25)",         "stack",     list(k = 25),
  my_knn_model,             "kNN (k = 50)",         "stack",     list(k = 50),
  my_SVR_model,             "SVR",                  "stack",     list(kernel = "sigmoid"),
  my_ET_model,              "ExtraTree (500)",      "stack",     list(ntree = 500),
  my_ET_model,              "ExtraTree (2000)",     "stack",     list(ntree = 2000)
) %>%
  as.data.table()

model_to_exclude <- c("Random Forest (500)", "ExtraTree (500)", "kNN (k = 5)", "const", "lm")
```

Краткие выводы по отдельным моделям:
 - Без PCA обе метрики лучше на всех алгоритмах
 - stack лучше по С-индексу, chain - по RMSE
 - лучшая модель - Lasso L1 без PCA; среди не lm, то Catboost


### Без уменьшения размерности (PCA)

Результаты по моделям:
```{r}
regr_stack_res <- calc_regression_models(regr_experiments, X_train, Y_train, X_test, Y_test)
regr_chain_res <- calc_regression_models(regr_experiments, X_train, Y_train, X_test, Y_test)

regr_res_file <- here("0. Data/1. Output/regression_results.xlsx")
xlsx::write.xlsx(regr_stack_res[order(-C_index), -"pred"],
                 regr_res_file, "stack", row.names = F, append = T)
xlsx::write.xlsx(regr_chain_res[order(-C_index), -"pred"], 
                 regr_res_file, "chain", row.names = F, append = T)
```

Нахождение весов по различным наборам моделей (независимым / по цепочке / только лучшие):
```{r}
stack_mtr <- regr_stack_res[!name %in% model_to_exclude] %>% extract_matrices()
chain_mtr <- regr_chain_res[!name %in% model_to_exclude] %>% extract_matrices()

stack_mtr_f <- cut_matr(stack_mtr, qnt_prob = 0.6, Y_true = Y_test)
chain_mtr_f <- cut_matr(chain_mtr, qnt_prob = 0.6, Y_true = Y_test)

regr_stack_w   <- get_regr_w(w_config, stack_mtr, Y_test)
regr_chain_w   <- get_regr_w(w_config, chain_mtr, Y_test)
regr_stack_w_f <- get_regr_w(w_config, stack_mtr_f, Y_test)
regr_chain_w_f <- get_regr_w(w_config, chain_mtr_f, Y_test)

# beepr::beep()
```
Код run_weights_search_experiments выполняется минуты 4 на полном наборе моделей и 2 на усеченном

```{r}
reg_ens_fname <- here("0. Data/1. Output/regression_ensemble_results.xlsx")

write_w_search_to_xlsx(regr_stack_w  , stack_mtr  , "stack", reg_ens_fname)
write_w_search_to_xlsx(regr_stack_w_f, stack_mtr_f, "filtered stack", reg_ens_fname)
write_w_search_to_xlsx(regr_chain_w  , stack_mtr  , "chain", reg_ens_fname)
write_w_search_to_xlsx(regr_chain_w_f, chain_mtr_f, "filtered chain", reg_ens_fname)
```
Один такой блок кода от и до выполняется 10 минут. 


### С использованием PCA

```{r}
.[X_train_pca, X_test_pca] <- pca_data_preparation(X_train, X_test, limit_ssq = 0.9)
```

Результаты по моделям:
```{r}
regr_stack_res_pca <- calc_regression_models(regr_experiments, X_train_pca, Y_train, X_test_pca, Y_test)
regr_chain_res_pca <- calc_regression_models(regr_experiments, X_train_pca, Y_train, X_test_pca, Y_test)

regr_stack_res_pca[order(-C_index), -"pred"] %>% 
  xlsx::write.xlsx(regr_res_file, "stack PCA", row.names = F, append = T)

regr_chain_res_pca[order(-C_index), -"pred"] %>% 
  xlsx::write.xlsx(regr_res_file, "chain PCA", row.names = F, append = T)
```

Нахождение весов по различным наборам моделей (независимым / по цепочке / только лучшие):
```{r}
stack_mtr_pca <- regr_stack_res_pca[!name %in% model_to_exclude] %>% extract_matrices()
chain_mtr_pca <- regr_chain_res_pca[!name %in% model_to_exclude] %>% extract_matrices()

stack_mtr_f_pca <- cut_matr(stack_mtr_pca, qnt_prob = 0.6, Y_true = Y_test)
chain_mtr_f_pca <- cut_matr(chain_mtr_pca, qnt_prob = 0.6, Y_true = Y_test)

regr_stack_w_pca   <- get_regr_w(w_config, stack_mtr_pca, Y_test)
regr_chain_w_pca   <- get_regr_w(w_config, chain_mtr_pca, Y_test)
regr_stack_w_f_pca <- get_regr_w(w_config, stack_mtr_f_pca, Y_test)
regr_chain_w_f_pca <- get_regr_w(w_config, chain_mtr_f_pca, Y_test)

# beepr::beep()
```

```{r}
write_w_search_to_xlsx(regr_stack_w_pca  , stack_mtr_pca  , "stack PCA", reg_ens_fname)
write_w_search_to_xlsx(regr_stack_w_f_pca, stack_mtr_f_pca, "filtered stack PCA", reg_ens_fname)
write_w_search_to_xlsx(regr_chain_w_pca  , stack_mtr_pca  , "chain PCA", reg_ens_fname)
write_w_search_to_xlsx(regr_chain_w_f_pca, chain_mtr_f_pca, "filtered chain PCA", reg_ens_fname)
```
 

## 4.2 Взвешенное ансамблирование (блендинг) отдельно на психологических тестах

Обучаем на wide_data, тестим на wide_data2:
```{r}
ftrs_na <- wide_data2[, .SD, .SDcols = !patterns("HL|HL2")]
Y_na <- wide_data2[, .SD, .SDcols =  patterns("HL|HL2")] %>% as.matrix()
X_na <- scale(ftrs_na, center = c(0, mean_train), scale = c(1, sd_train)) %>% as.data.table()

X_wd <- scale(features, center = mean_train, scale = sd_train)
Y_wd <- targets
```

Подходы:
  1. Множественная импутация (MICE) -- не применим для новых данных (надо каждый раз обучать)
  2. Матричные и тензорные методы
  3. Добавление масок
  4. Потестовое обучение + обединение с разными весами
  5. Модели, работающие с пропусками (RF, XGBoost) - оч плохо

Проверять на датасете, где есть все данные: искусственно сделать там NA. Обычно заполнено 3 или 4 теста:
```{r}
# na_data <- wide_data %>% copy()
# tests <- c("BF", "EY", "LN", "SC", "CT")
# 
# na_data <- na_data %>% 
#   .[, N_na := sample(c(1, 2), 1, prob = c(0.33, 1-0.33)), by = .I] %>% 
#   .[, tests_to_na := list(list(sample(tests, N_na))), by = .I] %>% 
#   melt(id.vars = c("id", "N_na", "tests_to_na"), variable.name = "factor_name") %>% 
#   .[, test := str_sub(factor_name, 1, 2)] %>% 
#   .[, is_a_in_b := mapply('%in%', test, tests_to_na)] %>% 
#   .[is_a_in_b == TRUE, value := NA] %>% 
#   .[, -c("tests_to_na", "test", "is_a_in_b", "N_na")] %>% 
#   dcast(id ~ factor_name, value.var = "value") %>% 
#   .[order(id)]
```


### 1. Multiple Imputation

Проба на данных с реальными пропусками:
```{r}
# для лучшего улавливания связей дадим и признаки из полных данных
# filled_X_na <- mice_imputation(rbind(X_wd, X_na[, -"id"])) %>% .[-c(1:nrow(X_wd))]
filled_X_na <- mice_imputation(X_na[, -"id"])

set.seed(SEED)
.[X_tr, X_ts, Y_tr, Y_ts] <- train_test_split(filled_X_na, Y_na)
regr_stack_res_na <- calc_regression_models(regr_experiments, X_tr, Y_tr, X_ts, Y_ts)
```
Лишь LightGBM (9.9) и SVR (9.7) лучше константы (9.69). Дальше CatBoost, stepwise, RF


### 2. Matrix/Tensor Completion

Как выбрать rank.max для train_matrix_completion:
```{r}
M_obs <- as.matrix(wide_data[, ..aux_feats])
sv <- svd(M_obs, nu = 0, nv = 0)$d
plot(sv, type="b", ylab="Singular values", xlab="Component")
```

```{r}
fit_obj <- train_matrix_completion(X_wd)
filled_X_na <- transform_matrix_completion(fit_obj, X_na[, -"id"])

regr_stack_res_na <- calc_regression_models(regr_experiments, X_wd, Y_wd, filled_X_na, Y_na)
```
 - Лучше всех: stepwise (9.929, но велик rmse), Lasso L1 (9.925)
 - Хороши: Ridge L2, kNN (k=50) (оба -- около 9.8)


### 3. Добавление масок

```{r}
regr_stack_res_na <- calc_regression_models(
  regr_experiments, 
  prepare_data(X_wd), Y_wd, prepare_data(X_na), Y_na
)
```
 - лучше всех: Lasso L1 (>10), stepwise (9.98)
 - почти топ: Ridge L2 (9.92), ET (9.9), lm (9.876)


### 4. Ансамбль на потестовых данных

Перебираем все регрессоры со всеми способами получить веса. Фактически каждую модель обучаем на данных различных тестов отдельно, затем для каждой из 31 комбинаций наличия тестов с помощью <метода получения весов> находим лучшие веса. Проверяем на данных с пропусками, т.е. wide_data2:
```{r}
for (row_exp in 1:nrow(regr_experiments)) {
  my_model     <- regr_experiments[row_exp, model][[1]]
  model_name   <- regr_experiments[row_exp, name][[1]]
  mo_type      <- regr_experiments[row_exp, MO_type][[1]]
  model_params <- regr_experiments[row_exp, params][[1]]
  
  method_f <- if (mo_type == "stack") stack_MO_regr else if (mo_type == "chain") chain_MO_regr else no_MO_regr
  cat(row_exp, ". ", model_name, "\n\n", sep = "")
  
  for (i in 1:nrow(w_bytest_comb_config)) {
    .[w_model, w_par, lbl] <- w_bytest_comb_config[i, .(method, params, label)] %>% sapply(\(obj) obj)
    
    tic()
    .[psytest_models, wght_combos] <- do.call(
      fit_by_tests_ensemble,
      args = c(
        list(
          model_class = my_model,
          X_train, Y_train, X_test, Y_test,
          psytests = c("BF", "CT", "EY", "LN", "SC"),
          MO_regr = method_f,
          weight_model = w_model,
          weight_model_params = w_par
        ),
        model_params
      )
    )
      
    pred_true_values <- predict_by_tests_ensemble(X_na, Y_na, psytest_models, wght_combos)
    toc()
    
    val <- pred_true_values$cindex %>% mean() %>% round(3)
    cat(lbl, ", cindex: ", val, "\n\n", sep = "")
  }
}
```

Лучше всего себя показали:
 - stepwise lm + GA (10.053, но долго - 124 сек)
 - Ridge L2 + PSO (10.038, но долго - 90 сек)
 - lm + (GS | simple | equal) (9.98, от 1 до 10 сек)
 - L1, L2, stepwise, ...
 
Вывод: 
 - метод: L2 (или L1 или lm)
 - Grid Search (для скорости), PSO (для точности, ибо GA долгий)
 

my_regularized_lm_model(alpha = 0) + no_MO_regr + particle_swarm_weights(swarm_size = NA)
```{r}
.[psytest_models, wght_combos] <- best_fit_bytests_ensemble(X_train, Y_train, X_test, Y_test)
pred_true_values <- predict_by_tests_ensemble(X_na, Y_na, psytest_models, wght_combos)

pred_true_values$cindex %>% mean() %>% round(3)   
pred_true_values$cindex %>% plot_cindex_hist()
```
Выполняется ~1 мин, результат ~9.929


### 5. Результаты моделей для одного теста

```{r}
res_by_psytests <- NULL

for (psytest_lbl in c("BF", "EY", "CT", "LN", "SC")) {
  cat(psytest_lbl, "\n")
  .[X_tr, X_ts, Y_tr, Y_ts] <- prepare_psytest_data(wide_data, wide_data2, psytest_lbl)
  
  res_tst <- calc_regression_models(regr_experiments, X_tr, Y_tr, X_ts, Y_ts) %>% 
    .[, psytest := psytest_lbl] %>% 
    .[, pred := NULL]
  res_by_psytests <- rbind(res_by_psytests, res_tst)
}

filename <- here("0. Data/1. Output/bytest_ensembles/bytest_model_results.xlsx")
res_by_psytests %>% dcast(name ~ psytest, value.var = "rmse") %>% .[order(BF)] %>% 
  xlsx::write.xlsx(., filename, "rmse", row.names = F, append = T)
res_by_psytests %>% dcast(name ~ psytest, value.var = "C_index") %>% .[order(-CT)] %>% 
  xlsx::write.xlsx(., filename, "C_index", row.names = F, append = T)
```


# 5. Распределение и сбалансированность предсказаний кодов Голланда

Распределение значений кодов HL_1:HL_6 предсказаний:
```{r}
df_long_preds <- as.data.table(rf_ind_pred) %>%  # lm_L1_pred
  setnames(paste0("HL_", 1:6)) %>% 
  .[, id := .I] %>% 
  pivot_longer(cols = HL_1:HL_6, names_to = "Group", values_to = "Value")


ggplot(df_long_preds, aes(x = Value, color = Group, fill = Group)) +
  geom_density(alpha = 0.2) +
  labs(title = "Накладывающиеся графики плотности предсказаний", x = "Значение", y = "Плотность") +
  theme_minimal()

ggplot(df_long_preds, aes(x = Value, color = Group, fill = Group)) +
  geom_density(alpha = 0.2) +
  labs(title = "Накладывающиеся графики плотности предсказаний", x = "Значение", y = "Плотность") +
  facet_wrap(~ Group) +
  theme_minimal()

ggplot(df_long_preds, aes(x = Value, y = Group, fill = stat(x))) +
  geom_density_ridges_gradient() +
  scale_fill_viridis_c(name = "Temp.", option = "C") +
  labs(title = "Накладывающиеся графики плотности предсказаний", x = "Значение", y = "Плотность") +
  theme_minimal()
```

Распределение значений топ1:топ6 предсказаний:
```{r}
value_rank_df <- df_long_preds %>% 
  as.data.table() %>% 
  .[order(id, -Value)] %>% 
  .[, rank := paste0("rank_", 1:.N), by = .(id)] %>% 
  .[, .(Value, rank)]

ggplot(value_rank_df, aes(x = Value, color = rank, fill = rank)) +
  geom_density(alpha = 0.2) +
  labs(title = "Накладывающиеся графики плотности", x = "Значение", y = "Плотность") +
  facet_wrap(~ rank) +
  theme_minimal()

ggplot(value_rank_df, aes(x = Value, y = rank, fill = stat(x))) +
  geom_density_ridges_gradient() +
  scale_fill_viridis_c(name = "Temp.", option = "C") +
  labs(title = "Частоты значений кодов предсказаний по убыванию") +
  theme_minimal()
```


# 6. Сохранение модели

```{r}
cv_glm <- cv.glmnet(X_train, Y_train, family = "mgaussian", alpha = 1)
# saveRDS(cv_glm, here("3. Shiny_app/model/MODEL_cv_glm.rds"))

glm_pred <- predict(cv_glm, newx = X_test[1, , drop = F], s = "lambda.min")[,,1] %>% 
  smart_integer_round()
```
