---
title: "Модели, отличные от регрессии, ранжирования, классификации"
author: "Egor Glushkov"
---

# 1. Библиотеки и функции

Библиотеки:
```{r include=FALSE}
source(paste0(here::here(), "/1. R/00. Source_all.R"))
```

```{r}
plot_rmse_vs_cind <- function(data, x_name = "k_dim", additional_title = "") {
  plot <- plot_ly(data) %>%
    add_trace(x = ~get(x_name),  y = ~rmse, name = "RMSE", type = "scatter",
              mode = "lines+markers", line = list(color = "blue"), 
              marker = list(color = "blue")
    ) %>%
    add_trace(x = ~get(x_name), y = ~cind, name = "CInd", type = "scatter", 
              mode = "lines+markers", line = list(color = "red"), 
              marker = list(color = "red"), yaxis = "y2"  
    ) %>%
    layout(
      title = paste0("График метрик RMSE и C-index. ", additional_title),
      xaxis = list(title = x_name),
      yaxis = list(
        title = "RMSE",
        titlefont = list(color = "blue"),
        tickfont = list(color = "blue")
      ),
      margin = list(r = 50),
      yaxis2 = list(
        title = "CInd",
        titlefont = list(color = "red"),
        tickfont = list(color = "red"),
        overlaying = "y",
        side = "right",
        automargin = TRUE,
        title_standoff = 15
      )
    ) %>% 
    hide_legend()
  return(plot)
}
```


```{r}
find_K_nearest_neighbour <- function(input_vec, X_train_, Y_train_, k_nn = 43,
                                     dist_func = cosine_dist, avg_func = median) {
  sim_vec <- sapply(1:nrow(X_train_),
                    \(n_row) dist_func(input_vec, X_train_[n_row, ]) %>% abs())
  
  ind_nn <- data.table(sim_vec) %>% 
    .[, ind := .I] %>% 
    .[order(sim_vec)] %>% 
    .[1:k_nn, ind]
  
  return(apply(Y_train_[ind_nn, , drop = F], 2, avg_func))
}


my_kNN <- function(X_train_, X_test_, Y_train_, k_nn = 43,
                   # аргументы далее лучше не трогать (выбраны лучшие согласно тестам)
                   dist_func = my_rmse, avg_func = median, 
                   need_dim_red = TRUE, k_dim = NULL, limit_ssq = 0.9) {
  if (need_dim_red == TRUE) {
    .[X_train_, X_test_] <- pca_data_preparation(X_train_, X_test_, k_dim, limit_ssq)
  }
    
  Y_pred <- apply(X_test_, MARGIN = 1, find_K_nearest_neighbour, X_train_ = X_train_,
                  Y_train_ = Y_train_, k_nn = k_nn, dist_func = dist_func,
                  avg_func = avg_func) %>% t()
  return(Y_pred)
}
```



# 2. Данные

```{r}
# .[features, targets] <- separate_X_y(wide_data)
# .[X_train, X_test, Y_train, Y_test, split_idx] <- train_test_split(features, targets)
```


# 3. Предсказание главного кода + все остальные
## 3.1 Случайное предсказание главного кода

Предсказание главного кода i, затем с вероятностью p_ij предсказывается код j.
Как предсказывать третий код? Или второй максимум в p_ij, или максимум по комбинации двух кодов.

```{r}
untd_targets_bool <- untd_dt %>% 
  .[, .SD, .SDcols = patterns("HL_")] %>% 
  apply(1, bool_mask_row) 

cooccur_mat <- untd_targets_bool %*% t(untd_targets_bool)
diag(cooccur_mat) <- 0
cooccur_mat <- cooccur_mat / rowSums(cooccur_mat)
```

```{r}
# Второй и третий индексы берутся как наиболее вероятные для первого (главного) кода, постоянны
predict_linkage_rnd_max <- function(cooccur_mat, first_ind = NULL) {
  if (is.null(first_ind)) first_ind <- sample(1:6, 1)
  second_ind <- cooccur_mat[first_ind, ] %>% which.max()
  
  third_probs <- cooccur_mat[c(first_ind, second_ind), ] %>% colSums()
  third_probs[c(first_ind, second_ind)] <- 0
  third_ind <- third_probs %>% which.max()
  
  return(c(first_ind, second_ind, third_ind))
}


# Второй и третий индексы берутся по вероятности на основе исторических данных
predict_linkage_rnd_prob <- function(cooccur_mat, first_ind = NULL) {
  if (is.null(first_ind)) first_ind <- sample(1:6, 1) 
  second_ind <- sample(1:6, size = 1, prob = cooccur_mat[first_ind, ])
  
  third_probs <- cooccur_mat[c(first_ind, second_ind), ] %>% colSums()
  third_probs[c(first_ind, second_ind)] <- 0
  third_ind <- sample(1:6, size = 1, prob = third_probs)
  
  return(c(first_ind, second_ind, third_ind))
}


predict_by_linkage_metrics <- function(cooccur_mat, first_inds_vec = NULL, avg_prob_b = TRUE, n_repeat = 10^2) {
  rnd_max_score <- sapply(1:nrow(Y_test),
                          \(i) calc_C_index_threes(
                            predict_linkage_rnd_max(cooccur_mat, first_inds_vec[i]),
                            get_max_indx(Y_test[i, ])
                          )) %>% mean()
  
  # rnd_prob_score <- sapply(1:nrow(Y_test),
  #                          \(i) calc_C_index_threes(
  #                            predict_linkage_rnd_prob(cooccur_mat, first_inds_vec[i]),
  #                            get_max_indx(Y_test[i, ])
  #                          )) %>% mean()
  
  if (avg_prob_b == FALSE) n_repeat <- 1
  
  rnd_prob_score <- sapply(1:n_repeat, \(ii) sapply(
    1:nrow(Y_test),
    \(i) calc_C_index_threes(
      predict_linkage_rnd_prob(cooccur_mat, first_inds_vec[i]),
      get_max_indx(Y_test[i, ])
    )
  ) %>% mean()) %>%
    median()
  
  cat(rnd_max_score, rnd_prob_score)
  return(invisible(c(rnd_max_score, rnd_prob_score)))
}
```

Без указания первого главного кода (рандом):
```{r}
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = NULL)
```
Взятие наиболее вероятного кода (как второго и третьего) работает лучше, чем взятие случайного (по весу)


## 3.2 Регрессия

Предсказывание первого (главного) кода с помощью регрессии, остальные два -- по связям с главным.

### PCA
Random Forest:
```{r}
best_k <- 32
pca_model <- prcomp(X_train, center = TRUE, scale. = TRUE)

X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

pred <- perform_stack_MO_regression(my_RandomForest_model, X_train_pca, Y_train, X_test_pca, Y_test, print_metric = TRUE)
first_inds_vec <- apply(pred, 1, \(x) which.max(x)) %>% unname()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```

Lasso:
```{r}
best_k <- 23
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

pred <- perform_stack_MO_regression(my_L1_L2_glmnet_model, X_train_pca, Y_train, X_test_pca, Y_test, print_metric = TRUE)
first_inds_vec <- apply(pred, 1, \(x) which.max(x)) %>% unname()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```

LightGBM:
```{r}
best_k <- 31
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

pred <- perform_stack_MO_regression(my_LightGBM_model, X_train_pca, Y_train, X_test_pca, Y_test, print_metric = TRUE)
first_inds_vec <- apply(pred, 1, \(x) which.max(x)) %>% unname()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```


### без PCA
```{r}
# my_RandomForest_model, my_LightGBM_model
pred <- perform_stack_MO_regression(my_LightGBM_model, X_train, Y_train, X_test, Y_test, print_metric = TRUE)
# pred <- get_L1_L2_glmnet_preds(X_train, Y_train, X_test, Y_test, print_metric = TRUE)
first_inds_vec <- apply(pred, 1, \(x) which.max(x)) %>% unname()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```
RandomForest: 9.761194 9.731343 (orig 9.627)
RandomForest PCA: 9.835821 9.5 (orig 9.657)
my_LightGBM_model: 9.522388 9.470149 (orig 9.687)
my_LightGBM_model: 9.701493 9.723881 (orig 10.045)
Lasso PCA: 9.776119 9.597015 (orig 9.746)
Lasso: 9.701493 9.61194 (orig 10.045)

Вывод: предсказание первой буквы и уже по ней по "статистике" второй и третьей (притом константно) работает лучше рандома, но хуже предсказания регрессией 2 и 3 факторов тройки


## 3.3 Классификация первого кода
С классификацией первого фактора вместо регрессора с выбором первого:
```{r}
Y_train_classif <- apply(Y_train, 1, \(x) which.max(x)) %>% unname() %>% factor(levels = 1:6)
Y_test_classif <- apply(Y_test, 1, \(x) which.max(x)) %>% unname() %>% factor(levels = 1:6)

model_rf <- randomForest(
  x = X_train,
  y = Y_train_classif,
  ntree = 500,
  importance = TRUE
)

pred <- predict(model_rf, X_test)
# confusionMatrix(pred, Y_test_classif)
first_inds_vec <- pred %>% unname()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```

XGBoost
```{r}
dtrain <- xgb.DMatrix(X_train, label = as.integer(Y_train_classif)-1)
dtest <- xgb.DMatrix(X_test)

params <- list(
  objective = "multi:softprob",
  num_class = 6,
  eval_metric = "mlogloss"
)

model_xgb <- xgb.train(params, dtrain, nrounds = 100)
first_inds_vec <- predict(model_xgb, dtest, reshape = TRUE) %>% max.col()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```

LightGBM:
```{r}
dtrain <- lgb.Dataset(
  data = X_train %>% data.matrix(),
  label = as.integer(Y_train_classif)-1,
  params = list(feature_pre_filter = FALSE)
)

params <- list(
  objective = "multiclass",
  metric = "multi_logloss",
  num_class = 6,
  boosting_type = "gbdt"
)

model_lgb <- lgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  verbose = -1
)

first_inds_vec <- predict(model_lgb, X_test) %>% max.col()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```


PCA:
```{r}
best_k <- 32
X_train_pca <- pca_scaler(X_train, pca_model, k = best_k)
X_test_pca  <- pca_scaler(X_test,  pca_model, k = best_k)

model_rf <- randomForest(
  x = X_train_pca,
  y = Y_train_classif,
  ntree = 500,
  importance = TRUE
)

pred <- predict(model_rf, X_test_pca)
first_inds_vec <- pred %>% unname()
predict_by_linkage_metrics(cooccur_mat, first_inds_vec = first_inds_vec)
```


# 4. Nearest neigbours

## 4.1 1-NN
k = 1 (без PCA)
```{r}
my_kNN(X_train, X_test, Y_train, k_nn = 1, dist_func = cosine_dist, need_dim_red = FALSE) %>% 
  show_custom_metrics("OneNN. Cosine")

my_kNN(X_train, X_test, Y_train, k_nn = 1, dist_func = my_rmse, need_dim_red = FALSE) %>% 
  show_custom_metrics("OneNN. RMSE")
```

k = 1 (+ PCA)
```{r}
my_kNN(X_train, X_test, Y_train, k_nn = 1, dist_func = cosine_dist) %>% 
  show_custom_metrics("OneNN. Cosine. PCA")

my_kNN(X_train, X_test, Y_train, k_nn = 1, dist_func = my_rmse) %>% 
  show_custom_metrics("OneNN. RMSE. PCA")
```


Найдем наилучшиее значение k:
```{r}
k_to_explore <- 2:ncol(X_train)

res_1NN <- sapply(k_to_explore, function(k_dim) {
  pred.1NN <- my_kNN(X_train, X_test, Y_train, k_nn = 1, k_dim = k_dim) 
  list(
    rmse = df_metric(pred.1NN, Y_test, my_rmse),
    cind = df_metric(pred.1NN, Y_test, calc_C_index)
  )
}) %>% 
  t() %>% as.data.table() %>% 
  unnest(cols = c(rmse, cind)) %>% 
  mutate(k_dim = k_to_explore, .before = 1)

plot_rmse_vs_cind(res_1NN, x_name = "k_dim")
```
   k_dim     rmse     cind
1:    21 3.011074 9.907692
2:    35 3.174961 9.876923
3:    49 2.959134 9.861538
4:    50 2.969922 9.861538
5:    55 2.914749 9.861538


## 4.2 k-NN

Лучше использовать median (вместо mean), потому что:
```{r}
knn_avg_func_test <- function(avg_func, 
                              X_train_ = X_train, X_test_ = X_test, 
                              Y_train_ = Y_train, Y_test_ = Y_test, 
                              k_to_explore = 2:100) {
  res_kNN <- sapply(k_to_explore, function(k_nn) {
    pred.kNN <- my_kNN(X_train_, X_test_, Y_train_, k_nn = k_nn, avg_func = avg_func) 
    list(
      rmse = df_metric(pred.kNN, Y_test_, my_rmse),
      cind = df_metric(pred.kNN, Y_test_, calc_C_index)
    )
  }) %>% 
    t() %>% as.data.table() %>% 
    unnest(cols = c(rmse, cind)) %>% 
    mutate(k_nn = k_to_explore, .before = 1)
  
  plot <- plot_rmse_vs_cind(res_kNN, x_name = "k_nn", 
                            additional_title = substitute(avg_func) %>% as.character())
  return(plot)
}

knn_avg_func_test(avg_func = mean)
knn_avg_func_test(avg_func = median)
```
Median лучше mean (как avg_func)

     k_nn     rmse     cind
 1:    42 2.302586 11.00000
 2:    44 2.298526 10.76923
 3:    41 2.306252 10.75385
 4:   100 2.271181 10.73846
 5:    52 2.278021 10.69231
 6:    98 2.276894 10.69231
 7:    40 2.310040 10.67692
 8:    47 2.296732 10.63077
 9:    23 2.313786 10.55385
10:    60 2.295795 10.55385


## 4.3 Кластеры

```{r}
cluster_prediction <- function(X_train_, X_test_, Y_train_, k_clusters = 50,
                               # аргументы далее лучше не трогать (выбраны лучшие согласно тестам)
                               avg_func = median, need_dim_red = TRUE, k_dim = NULL, limit_ssq = 0.9) {
  predict_kmeans <- function(new_data, kmeans_model) {
    distances <- sapply(1:nrow(kmeans_model$centers), function(i) {
      rowSums((new_data - kmeans_model$centers[i, ])^2)
    })
    return(apply(distances, 1, which.min))
  }
  
  if (need_dim_red == TRUE) {
    .[X_train_, X_test_] <- pca_data_preparation(X_train_, X_test_, k_dim, limit_ssq)
  }
  
  km_pca <- kmeans(X_train_, centers = k_clusters, nstart = 50)
  pred_cluster_nums <- predict_kmeans(X_test_, km_pca)
  
  uniq_cl_nums <- unique(pred_cluster_nums)
  cluster_HL_df <- data.table(
    cl_num = uniq_cl_nums, 
    sapply(uniq_cl_nums, 
           \(cl_num) apply(Y_train_[km_pca$cluster == cl_num, , drop = F], 2, avg_func)) %>%
      t()) 
  
  Y_pred <- data.table(cl_num = pred_cluster_nums) %>% 
    .[, id := .I] %>%
    merge(cluster_HL_df, all.x = TRUE, by = "cl_num") %>% 
    .[order(id)] %>% 
    .[, -c("id", "cl_num")] %>% 
    as.matrix()
  
  return(Y_pred)
}
```

Среднее или медиана как агрегирующая функция:
```{r}
check_avg_f <- lapply(
  1:100, \(i)
  c(
    cluster_prediction(X_train, X_test, Y_train, k_clusters = 50, 
                       need_dim_red = T, avg_func = mean) %>% df_metric(Y_test, calc_C_index),
    cluster_prediction(X_train, X_test, Y_train, k_clusters = 50, 
                       need_dim_red = T, avg_func = median) %>% df_metric(Y_test, calc_C_index)
  )  
)

check_avg_f <- check_avg_f %>% 
  as.data.table() %>% t() %>% as.data.table() %>% 
  setnames(new = c("mean_col", "median_col"))

check_avg_f %>% colMeans()
check_avg_f[, .N, by = mean_col >= median_col]
```
На 1к случаев: 
  mean_col  median_col 
  8.916169   9.126446 

Mean >= Median раз: 373 vs 627 
Медиана и среднее (как avg_func) практически идентичны, но с PCA лучше median.

```{r}
k_clusters_exploration <- function(k_clusters_to_explore, X_train_, X_test_, Y_train_) {
  res_clust <- sapply(k_clusters_to_explore, function(k_clusters) {
    pred_cl <- cluster_prediction(X_train_, X_test_, Y_train_, k_clusters)
    list(
      rmse = df_metric(pred_cl, Y_test, my_rmse),
      cind = df_metric(pred_cl, Y_test, calc_C_index)
    )
  }) %>% 
    t() %>% as.data.table() %>% 
    unnest(cols = c(rmse, cind)) %>% 
    mutate(k_cl = k_clusters_to_explore, .before = 1)
  return(res_clust) 
}
```

```{r}
res_cl <- k_clusters_exploration(2:100, X_train, X_test, Y_train)
# res_cl %>% as.data.table() %>% .[order(-cind)] %>% .[1:5]

plot_rmse_vs_cind(res_cl, x_name = "k_cl")
```
    k_cl     rmse      cind
1:    12 2.390497 10.246154
2:    38 2.527507 10.246154
3:    43 2.579419 10.015385
4:    15 2.447570  9.769231
5:    18 2.396921  9.769231
