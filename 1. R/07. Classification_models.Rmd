---
title: "Модели классификации"
author: "Egor Glushkov"
---

# 1. Библиотеки и функции

Библиотеки:
```{r include=FALSE}
# source(paste0(here::here(), "/1. R/00. Source_all.R"))
# source(paste0(here::here(), "/1. R/10. Classification_functions.R"))
```


# 2. Данные

```{r}
# .[features, targets] <- separate_X_y(wide_data)
# .[X_train, X_test, Y_train, Y_test, split_idx] <- train_test_split(features, targets)

targets_bool <- apply(targets, 1, bool_mask_row) %>% t()
.[Y_b_train, Y_b_test] <- list(targets_bool[split_idx, ], targets_bool[!split_idx, ])

.[X_train_pca, X_test_pca] <- pca_data_preparation(X_train, X_test, limit_ssq = 0.8)
```


# 3. Решение

1. Линейные модели: V Логистическая регрессия, V MLP 
2. Деревья: V RF, V ExtraTree, Boosting (V XGBoost, V LightGBM, V Catboost)
3. V Naive Bayes
4. V kNN
5. V SVM

+ PCA!!! 

Разница есть:
 - 1 классификатор для multiclass, где берем 3 максимальные вероятности
 - 6 классификаторов для предсказания каждой метки (1 метка - 1 классификатор "да" или "нет")
 - предсказание одного из 20 классов в label powerset


Label Powerset
    Рассматриваем каждую из возможных комбинаций ровно трёх факторов (комбинаций C^3_6=20) как отдельный класс.
    Затем решаем задачу «многоклассовой» классификации на 20 классах.
    Подходит, если вам важен учёт корреляций «именно трёх» сразу, но количество классов быстро растёт при увеличении исходного числа меток.
    Реализовано в библиотеке scikit-multilearn через LabelPowerset


## 3.1 Пример решения одной моделью и как multiclass (с выбором 3 или 1 кодов), как multilabel

Multiclass на примере SVM
На примере SVM (пакет e1071)

Multiclass:
1) или берем топ1
2) или для топ3 размножаем датасет на 3

Берем размноженный на 3 датасет:
X[i, ] --> top1_lbl,
X[i, ] --> top2_lbl,
X[i, ] --> top3_lbl, ... (это k = 3)

k = 1 значит, что для каждой строки берем лишь один наиболее значимый лейбл, а не 3

```{r}
classification_test_framework(Y_test, Y_b_test, clsf_func = multiclass_pred_by_SVM, n_retry = 1,
                              label = "", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
                              kernel = "sigmoid", k = 3)
```


```{r}
print("k = 3")
lapply(c("linear", "polynomial", "radial", "sigmoid"),
       \(krnl) classification_test_framework(
         Y_test, Y_b_test, clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
         label = krnl, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
         kernel = krnl, k = 3
        )
) %>% bind_rows()

print("k = 1")
lapply(c("linear", "polynomial", "radial", "sigmoid"),
       \(krnl) classification_test_framework(
         Y_test, Y_b_test, clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
         label = krnl, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
         kernel = krnl, k = 1
        )
) %>% bind_rows()
```
k = 3 стабильно лучше, притом наилучшее ядро sigmoid.

Уменьшение размерности.
Насколько уменьшаем размерность:
```{r}
lapply(seq(0.3, 1, 0.1), \(s) {
  cat("ssq: ", s, "\t")
  .[X_train_pca, X_test_pca] <- pca_data_preparation(X_train, X_test, limit_ssq = s)
  
  classification_test_framework(
    Y_test, Y_b_test, clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
    label = s, X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
  )
}) %>% bind_rows()
```
Лучшее при ssq = 0.8 (k = 19)
Возможно 0.9

## 3.2 Модели

### 1 SVM
### 2 Random Forest
### 3 Catboost
### 4 Логистическая регрессия
### 5 Naive Bayes
### 6 kNN

Multilabel
```{r}
lapply(seq(5, 55, by = 5), \(k_neighb) {
  classification_test_framework(
    Y_test, Y_b_test, clsf_func = multilabel_knn_clsf, n_retry = 10, 
    label = str_glue("kNN ({k_neighb}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test, k_neighb = k_neighb
  )
}) %>% bind_rows()
```
Оптимально: k_neighb = 30 и 50

### 7 ExtraTree

Multilabel
```{r}
lapply(c(100, 1000, 2000, 3500), \(nt) {
  classification_test_framework(
    Y_test, Y_b_test, clsf_func = multilabel_extratree_clsf, n_retry = 5, ntree = nt,
    label = str_glue("ExtraTree ({nt}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test
  )
}) %>% bind_rows()
```
ntree = 2000

### 8 XGBoost

Multilabel
```{r}
lapply(c(100, 200, 500), \(nr) {
  classification_test_framework(
    Y_test, Y_b_test, clsf_func = multilabel_xgb_clsf, n_retry = 3, nrounds = nr,
    label = str_glue("XGBoost ({nr}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test
  )
}) %>% bind_rows()
```
Хватит и nrounds = 500

### 9 LightGBM

Multilabel
```{r}
lapply(c(100, 250, 500), \(nr) {
  classification_test_framework(
    Y_test, Y_b_test, clsf_func = multilabel_lightgbm_clsf, n_retry = 3, nrounds = nr,
    label = str_glue("LightGBM ({nr}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test
  )
}) %>% bind_rows()
```

### 10 MLP

```{R}
# pred <- multiclass_pred_by_MLP(X_train, Y_train, X_test)

classification_test_framework(
  Y_test, Y_b_test, clsf_func = multiclass_pred_by_MLP, n_retry = 10,
  label = "MLP multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)
```


## 3.3 Обобщение результатов

### 3.3.1 Multiclass

```{r}
multiclass_experiments <- tribble(
  ~multcls_clsf_func,              ~label,                       ~params,                  ~n_retry,
  multiclass_pred_by_SVM,          "SVM multiclass",             list(kernel = "sigmoid"), 3,
  multiclass_pred_by_RF,           "Random Forest multiclass",   list(ntree = 1000),       3,
  multiclass_pred_by_ExtraTree,    "ExtraTree multiclass",       list(ntree = 2000),       3,
  multiclass_pred_by_Logregr,      "Logit (ridge) multiclass",   list(alpha = 0),          3,
  multiclass_pred_by_Logregr,      "Logit (lasso) multiclass",   list(alpha = 1),          3,
  multiclass_pred_by_NB,           "Naive Bayes multiclass",     list(),                   3,
  multiclass_pred_by_kNN,          "kNN (3) multiclass",         list(k_neighb = 3),       3,
  multiclass_pred_by_kNN,          "kNN (30) multiclass",        list(k_neighb = 30),      3,
  multiclass_pred_by_kNN,          "kNN (50) multiclass",        list(k_neighb = 50),      3,
  multiclass_pred_by_XGBoost,      "XGBoost multiclass",         list(),                   3,
  multiclass_pred_by_LightGBM,     "LightGBM (100) multiclass",  list(nrounds = 100),      3,
  multiclass_pred_by_LightGBM,     "LightGBM (500) multiclass",  list(nrounds = 500),      3,
  multiclass_pred_by_LightGBM,     "LightGBM (1000) multiclass", list(nrounds = 1000),     3,
  multiclass_pred_by_Catboost,     "Catboost multiclass",        list(),                   3
)

mc_res     <- run_multiclass_experiments(multiclass_experiments, X_train, Y_train, X_test, Y_test, Y_b_test)
mc_res_pca <- run_multiclass_experiments(multiclass_experiments, X_train_pca, Y_train, X_test_pca, Y_test, Y_b_test)

print(mc_res)
print(mc_res_pca)
```

```{r}
bind_rows(
  compare_clsf_results(list(mc_res, mc_res_pca), mean, "mean"),
  compare_clsf_results(list(mc_res, mc_res_pca), median, "median")
)
```


### 3.3.2 Multilabel

```{r}
multilabel_experiments <- tribble(
  ~multlbl_clsf_func,             ~label,                        ~params,              ~n_retry,
  multilabel_svm_clsf,            "SVM multilabel",              list(),               3,
  multilabel_rf_clsf,             "Random Forest multilabel",    list(),               3,
  multilabel_extratree_clsf,      "ExtraTree (1000) multilabel", list(ntree = 1000),   3,
  multilabel_extratree_clsf,      "ExtraTree (2000) multilabel", list(ntree = 2000),   3,
  multilabel_logit_clsf,          "Logit (ridge) multilabel",    list(alpha = 0),      3,
  multilabel_logit_clsf,          "Logit (lasso) multilabel",    list(alpha = 1),      3,
  multilabel_nb_clsf,             "Naive Bayes multilabel",      list(),               3,
  multilabel_knn_clsf,            "kNN (k = 5) multilabel",      list(k_neighb = 5),   3,
  multilabel_knn_clsf,            "kNN (k = 30) multilabel",     list(k_neighb = 30),  3,
  multilabel_knn_clsf,            "kNN (k = 50) multilabel",     list(k_neighb = 50),  3,
  multilabel_xgb_clsf,            "XGBoost multilabel",          list(nrounds = 500),  3,
  multilabel_lightgbm_clsf,       "LightGBM multilabel",         list(nrounds = 500),  3,
  multilabel_catboost_clsf,       "CatBoost multilabel",         list(nrounds = 500),  3,
)

mltlbl_res     <- run_multilabel_experiments(multilabel_experiments, X_train, Y_b_train, X_test, Y_b_test)
mltlbl_res_pca <- run_multilabel_experiments(multilabel_experiments, X_train_pca, Y_b_train, X_test_pca, Y_b_test)
print(mltlbl_res)
print(mltlbl_res_pca)
```
Топ (PCA): 
 - kNN (k = 25) multilabel
 - ExtraTree (2000) multilabel
 - XGBoost multilabel
 
Топ (без PCA):
 - kNN (k = 25) multilabel (хотя и другие kNN норм)
 - другие примерно равны (в чем-то лучше, в чем-то хуже)
 
Upd. Забыл про alpha для logit:
```{r}
ml_logit_experminets <- tribble(
  ~multlbl_clsf_func,             ~label,                        ~params,              ~n_retry,
  multilabel_logit_clsf,          "Logit (ridge) multilabel",    list(alpha = 0),      5,
  multilabel_logit_clsf,          "Logit (lasso) multilabel",    list(alpha = 1),      5
)

ml_logit_res     <- run_multilabel_experiments(ml_logit_experminets, X_train, Y_b_train, X_test, Y_b_test)
ml_logit_res_pca <- run_multilabel_experiments(ml_logit_experminets, X_train_pca, Y_b_train, X_test_pca, Y_b_test)
# print(ml_logit_res)
# print(ml_logit_res_pca)

bind_rows(ml_logit_res[, is_pca := FALSE], ml_logit_res_pca[, is_pca := TRUE]) %>% 
  xlsx::write.xlsx(here("4. Output/09. Classif_results.xlsx"), "ml logit", row.names = F, append = T)
```


С PCA или без результаты почти идентичны, хотя с PCA чуть лучше. 
```{r}
bind_rows(
  compare_clsf_results(list(mltlbl_res, mltlbl_res_pca), mean, "mean"),
  compare_clsf_results(list(mltlbl_res, mltlbl_res_pca), median, "median")
)
```


### 3.3.3 Label Powerset

```{r}
lp_experiments <- tribble(
  ~model_fn,              ~label,                          ~params,
  lp_svm_letters,         "SVM",                           list(),
  lp_rf_letters,          "Random Forest",                 list(),
  lp_nb_letters,          "Naive Bayes",                   list(),
  lp_logit_letters,       "Logistic regr Ridge",           list(alpha = 0),
  lp_logit_letters,       "Logistic regr Lasso",           list(alpha = 1),
  lp_knn_letters,         "kNN, k = 30",                   list(k_neighb = 30),
  lp_knn_letters,         "kNN, k = 100",                  list(k_neighb = 100),
  lp_et_letters,          "ExtraTrees (ntree = 1000)",     list(ntree = 1000),
  lp_et_letters,          "ExtraTrees (ntree = 2000)",     list(ntree = 2000),
  lp_catboost_letters,    "CatBoost (nrounds = 200)",      list(nrounds = 200),
  lp_catboost_letters,    "CatBoost (nrounds = 500)",      list(nrounds = 500),
  lp_xgboost_letters,     "XGBoost (nrounds = 200)",       list(nrounds = 200),
  lp_lightgbm_letters,    "LightGBM (nrounds = 100)",      list(nrounds = 100),
  lp_lightgbm_letters,    "LightGBM (nrounds = 500)",      list(nrounds = 500)
)

lp_res     <- run_label_powerset_experiments(lp_experiments, X_train, Y_b_train, X_test, Y_b_test)
lp_res_pca <- run_label_powerset_experiments(lp_experiments, X_train_pca, Y_b_train, X_test_pca, Y_b_test)
print(lp_res)
print(lp_res_pca)
```
Без PCA выделяются Naive Bayes, Random Forest, ExtraTrees (ntree = 2000)
С PCA Catboost лучший по всему. Хороши Logistic regr Ridge, ExtraTrees (ntree = 1000)

С PCA или без результаты почти идентичны
```{r}
bind_rows(
  compare_clsf_results(list(lp_res, lp_res_pca), mean, "mean"),
  compare_clsf_results(list(lp_res, lp_res_pca), median, "median")
)
```

### 3.3.4 Сравнение результатов

```{r}
map2(
  list(mc_res, mc_res_pca, mltlbl_res, mltlbl_res_pca, lp_res, lp_res_pca),
  list("Multiclass", "Multiclass with PCA", "Multilabel", "Multilabel with PCA",
       "Label Powerset", "Label Powerset with PCA"),
  \(df, sheetname) xlsx::write.xlsx(df, here("4. Output/09. Classif_results.xlsx"), sheetname, 
                                    row.names = FALSE, append = TRUE)
)
```

Случайный классификатор:
```{r}
res <- NULL

for (i in 1:100) {
  random_sample <- sapply(1:nrow(Y_test), \(x) runif(6, 0, 14)) %>% t() 
  cindex <- df_metric(random_sample, Y_test, func = calc_C_index)
  classif_metrics <- random_sample %>% 
    apply(., 1, bool_mask_row) %>% t() %>% 
    calc_classification_metrics(., Y_b_test, "random classif") %>% 
    mutate(cindex = cindex, .after = 1)
  res <- res %>% bind_rows(classif_metrics)
}

bind_rows(
  res %>% copy() %>% .[, lapply(.SD, \(col) mean(col,   na.rm = TRUE)), .SDcols = is.numeric],
  res %>% copy() %>% .[, lapply(.SD, \(col) median(col, na.rm = TRUE)), .SDcols = is.numeric] 
)
```
Ожидаемо результаты близки к теоретическим: С-индекс около 9.0, вероятность угадать хотя бы 1 код из триады = 0.95, все три -- 0.05. 
Top2 -- это угадать ровно 2 кода (0.45) или ровно 3 кода (0.05) = 0.5
Это -- классическая задача из теорвера про шары двух цветов в корзине и взятие без повторений.


## 3.4 Ансамблирование

```{r}
multiclass_probs <- map2(
  multiclass_experiments$multcls_clsf_func,
  multiclass_experiments$params,
  \(fn, args) exec(fn, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test, !!!args) %>% as.matrix()
)

names(multiclass_probs) <- multiclass_experiments[["label"]] %>% str_remove(" multi(class|label)")

multilabel_probs <- map2(
  multilabel_experiments$multlbl_clsf_func,
  multilabel_experiments$params,
  \(fn, args) exec(fn, X_train = X_train, Y_b_train = Y_b_train, X_test = X_test, !!!args) %>% as.matrix()
)

names(multilabel_probs) <- multilabel_experiments[["label"]] %>% str_remove(" multi(class|label)")


# Для возможности вычисления C-индекса преобразуем "XYZ" в [T, F, T, ...] -> [1, 0, 1, ...] и далее работаем как обычно
lp_probs <- map2(
  lp_experiments$model_fn,
  lp_experiments$params,
  \(fn, args) exec(fn, X_train = X_train, Y_b_train = Y_b_train, X_test = X_test, !!!args) %>% as.matrix()
) %>% 
  lapply(\(mat) mat %>% three_letters_to_bool_matrix() %>% apply(2, as.integer))

names(lp_probs) <- lp_experiments[["label"]]
```


### Простые манипуляции с весами 

```{r}
mat <- multiclass_probs
# mat <- multilabel_probs
# mat <- lp_probs

## 1. Равные веса
w_eq <- rep(1/length(mat), length(mat))
weighted_cindex(w_eq, mat, label = "equal w")

## 2. Веса на основе метрик каждого
w_diff <- sapply(mat, \(probs) df_metric(probs, Y_test, func = calc_C_index))
cat("Metric proportion:\n")
weighted_cindex(w_diff, mat, label = "w")

w_cut <- cut_w(w_diff, qnt_prob = 0.67)
w_b <- w_diff - min(w_diff)
w_b_cut <- cut_w(w_b, qnt_prob = 0.67)

weighted_cindex(w_cut,   mat, label = "w_cut")
weighted_cindex(w_b,     mat, label = "w - min")
weighted_cindex(w_b_cut, mat, label = "cut [w - min]")

## 4. Softmax
w_sftm <- mclust::softmax(w_diff)
# softmax бессмысленно занулять
weighted_cindex(w_sftm, mat, label = "softmax")

## 4. Уберем худшие
lapply(seq(0.1, 1, 0.05),
       \(q) list(qnt_prob = q, cind = weighted_cindex(cut_w(w_diff, qnt_prob = q), mat))) %>%
  bind_rows()
```

1. Просто без всего - 0.46 sec
2. Шэпли - 81.54 sec
3. grid search - 53.66 sec
4. stacking_qp_weights - 0.08 sec
5. GA - 18.83 sec
6. pso_optimize_weights - 26.97 sec
7. bayes_optimize_weights - 3272.82 sec
8. coordinate_optimize_weights - 26.8 sec


### Единый фреймворк для проверки

Вывод: исключение "слабых" алгоритмов, лишь затем их комбо. Это улучшает результаты + серьезно сокращает вычисления

```{r}
# tmp_cfg <- config[!method %in% c(approx_shapley, bayes_optimize_weights)]
filterd_mc_probs <- cut_matr(multiclass_probs, Y_true = Y_test)
filterd_ml_probs <- cut_matr(multilabel_probs, Y_true = Y_test)
filterd_lp_probs <- cut_matr(lp_probs, Y_true = Y_test)

out_mc   <- run_weights_search_experiments(w_config, multiclass_probs, Y_test)
out_mc_f <- run_weights_search_experiments(w_config, filterd_mc_probs, Y_test)

out_ml   <- run_weights_search_experiments(w_config, multilabel_probs, Y_test)
out_ml_f <- run_weights_search_experiments(w_config, filterd_ml_probs, Y_test)

out_lp   <- run_weights_search_experiments(w_config, lp_probs, Y_test)
out_lp_f <- run_weights_search_experiments(w_config, filterd_lp_probs, Y_test)
beepr::beep()
```

Выполнение по времени:
  3675.12 sec (mc), где всё, кроме bayes, выполняется за 3 мин в сумме
  334.1 sec   (mc_f)
  2359.67 sec (ml)
  245.25 sec  (ml_f)

Сохранение результатов (как R-объекты и как развернутые таблицы):
```{r}
saveRDS(out_mc,   here("0. Data/1. Output/clsf_ensembles/clsf_ensemble_multiclass.rds"))
saveRDS(out_mc_f, here("0. Data/1. Output/clsf_ensembles/clsf_ensemble_multiclass_filtered.rds"))

saveRDS(out_ml,   here("0. Data/1. Output/clsf_ensembles/clsf_ensemble_multilabel.rds"))
saveRDS(out_ml_f, here("0. Data/1. Output/clsf_ensembles/clsf_ensemble_multilabel_filtered.rds"))

saveRDS(out_lp,   here("0. Data/1. Output/clsf_ensembles/clsf_ensemble_labelpowerset.rds"))
saveRDS(out_lp_f, here("0. Data/1. Output/clsf_ensembles/clsf_ensemble_labelpowerset_filtered.rds"))

write_w_search_to_xlsx(out_mc,   multiclass_probs, "Multiclass")
write_w_search_to_xlsx(out_mc_f, filterd_mc_probs, "Multiclass filtered")

write_w_search_to_xlsx(out_ml,   multilabel_probs, "Multilabel")
write_w_search_to_xlsx(out_ml_f, filterd_ml_probs, "Multilabel filtered")

write_w_search_to_xlsx(out_lp,   lp_probs, "Label Powerset")
write_w_search_to_xlsx(out_lp_f, filterd_lp_probs, "Label Powerset filtered")
```


### Проверка на валидационной выборке

На примере multiclass filtered. Предсказываем весь тест. Затем половину теста используем как валидационную, на ней настраиваем веса (гиперпараметры). На оставшейся половине теста (истинный тест) применяем веса и вычисляем метрику.
```{r}
mc_predicted_probs <- map2(
  multiclass_experiments$multcls_clsf_func,
  multiclass_experiments$params,
  \(fn, args) exec(fn, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test, !!!args) %>% as.matrix()
)

names(mc_predicted_probs) <- multiclass_experiments[["label"]] %>% str_remove(" multi(class|label)")

nn <- round(nrow(X_test)/2)
valid_probs <- lapply(mc_predicted_probs, \(mat) mat[1:nn, ])
test_probs  <- lapply(mc_predicted_probs, \(mat) mat[(nn+1):nrow(mat), ])

.[mc_Y_valid, mc_Y_test] <- list(Y_test[1:nn, ], Y_test[(nn+1):nrow(Y_test), ])
```

```{r}
flt_valid_probs <- cut_matr(valid_probs, Y_true = mc_Y_valid)
# w <- particle_swarm_weights(flt_valid_probs, mc_Y_valid)

res <- run_weights_search_experiments(w_config, flt_valid_probs, mc_Y_valid)

cind_valid <- sapply(res$w, \(w_) weighted_cindex_value(w_, flt_valid_probs, Y_true = mc_Y_valid))
cind_test <- sapply(res$w, \(w_) weighted_cindex_value(w_, test_probs[names(flt_valid_probs)], Y_true = mc_Y_test))

res[, ':='(cind_valid = cind_valid, cind_test = cind_test)]
res
```


# 4. Метрики классификации

```{r}
# Случайные предсказания
set.seed(SEED)
Y_rand <- replicate(n = nrow(Y_b_test), sample(c(rep(TRUE, 3), rep(FALSE, 3)), size = 6)) %>% t()

bind_rows(
  calc_classification_metrics(Y_pred_cb, Y_b_test, "Catboost"),
  calc_classification_metrics(Y_pred_rf, Y_b_test, "Random Forest"),
  calc_classification_metrics(Y_rand,    Y_b_test, "Random (baseline)")
)
```

Распределение C-индекса
```{r}
# pl <- multiclass_probs %>% 
#   scalar_matrix_production(particle_swarm_weights(., Y_test), .) %>% 
#   distr_metric(., Y_test, func = calc_C_index)

pso_w <- particle_swarm_weights(multiclass_probs, Y_test)
prob <- scalar_matrix_production(pso_w, multiclass_probs)
distr_metric(prob, Y_test, func = calc_C_index)
```
