---
title: "Модели классификации"
author: "Egor Glushkov"
---

# 1. Библиотеки и функции

Библиотеки:
```{r include=FALSE}
# source(paste0(here::here(), "/1. R/00. Source_all.R"))
# source(paste0(here::here(), "/1. R/10. Classification_functions.R"))
```


# 2. Данные

```{r}
# .[features, targets] <- separate_X_y(wide_data)
# .[X_train, X_test, Y_train, Y_test, split_idx] <- train_test_split(features, targets)

targets_bool <- apply(targets, 1, bool_mask_row) %>% t()
.[Y_b_train, Y_b_test] <- list(targets_bool[split_idx, ], targets_bool[!split_idx, ])
```


# 3. Решение

1. Линейные модели: V Логистическая регрессия, MLP 
2. Деревья: V RF, V ExtraTree, Boosting (V XGBoost, V LightGBM, V Catboost)
3. V Naive Bayes
4. V kNN
5. V SVM
6. Нейронки

+ PCA!!! 

Разница есть:
 - 1 классификатор для multiclass, где берем 3 максимальные вероятности
 - 6 классификаторов для предсказания каждой метки (1 метка - 1 классификатор "да" или "нет")
 - предсказание одного из 20 классов в label powerset


Label Powerset
    Рассматриваем каждую из возможных комбинаций ровно трёх факторов (комбинаций C^3_6=20) как отдельный класс.
    Затем решаем задачу «многоклассовой» классификации на 20 классах.
    Подходит, если вам важен учёт корреляций «именно трёх» сразу, но количество классов быстро растёт при увеличении исходного числа меток.
    Реализовано в библиотеке scikit-multilearn через LabelPowerset


## 3.1 Пример решения одной моделью и как multiclass (с выбором 3), как multilabel

На примере SVM (пакет e1071)

### 3.1.1 Multiclass
Multiclass:
1) или берем топ1
2) или для топ3 размножаем датасет на 3

Берем размноженный на 3 датасет:
X[i, ] --> top1_lbl,
X[i, ] --> top2_lbl,
X[i, ] --> top3_lbl, ... (это k = 3)

k = 1 значит, что для каждой строки берем лишь один наиболее значимый лейбл, а не 3

```{r}
classification_test_framework(Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 1,
                              label = "", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
                              kernel = "sigmoid", k = 3)
```


```{r}
print("k = 3")
lapply(c("linear", "polynomial", "radial", "sigmoid"),
       \(krnl) classification_test_framework(
         Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
         label = krnl, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
         kernel = krnl, k = 3
        )
) %>% bind_rows()

print("k = 1")
lapply(c("linear", "polynomial", "radial", "sigmoid"),
       \(krnl) classification_test_framework(
         Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
         label = krnl, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
         kernel = krnl, k = 1
        )
) %>% bind_rows()
```
k = 3 стабильно лучше, притом наилучшее ядро sigmoid.

Уменьшение размерности.
Насколько уменьшаем размерность:
```{r}
lapply(seq(0.3, 1, 0.1), \(s) {
  cat("ssq: ", s, "\t")
  .[X_train_pca, X_test_pca] <- pca_data_preparation(X_train, X_test, limit_ssq = s)
  
  classification_test_framework(
    Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
    label = s, X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
  )
}) %>% bind_rows()
```
Лучшее при ssq = 0.8 (k = 19)
Возможно 0.8

Итого лучшее:
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
  label = "simple SVM", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
.[X_train_pca, X_test_pca] <- pca_data_preparation(X_train, X_test, limit_ssq = 0.8)

r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
  label = "SVM with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```


### 3.1.2 Multilabel

multilabel_svm_clsf -- multilabel (Multilabel)

Модифицировать под разные модели (R6class)
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_svm_clsf, n_retry = 5, 
  label = "SVM multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```


### 3.1.3 Label Powerset

```{r}
svm_model <- svm(
  x = X_train,
  y = get_three_letter_code(Y_b_train),
  kernel = "sigmoid",
  probability = TRUE
)

Y_svm_pred <- predict(svm_model, X_test, probability = TRUE) %>% three_letters_to_bool_matrix()
calc_classification_metrics(Y_svm_pred, Y_b_test, "Label Powerset")
```


## 3.2 Модели

### 1 SVM

См выше


### 2 Random Forest

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_RF, n_retry = 5, 
  label = "RF multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_RF, n_retry = 5, 
  label = "RF multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```

Multilabel
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_rf_clsf, n_retry = 5, 
  label = "RF multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```

Label powerset
```{r}
model <- randomForest(x = X_train, y = get_three_letter_code(Y_b_train), ntree = 1000)

Y_model_pred <- predict(model, X_test) %>% three_letters_to_bool_matrix()
calc_classification_metrics(Y_model_pred, Y_b_test, "RF Label Powerset")
```


#### Старый код
```{r}
ind_pred <- vector("list", 6)

for (col_i in 1:6) {
  rf <- randomForest(x = X_train, y = as.factor(Y_b_train[, col_i]), ntree = 1000, importance = TRUE) 
  ind_pred[[col_i]] <- predict(rf, X_test, type = "prob")[, "TRUE"]
}

## Для C-index
classif_ind_pred <- ind_pred %>% as.data.table() %>% as.matrix()
df_metric(classif_ind_pred, Y_test, func = calc_C_index)

## Для метрик классификации
Y_pred_rf <- ind_pred %>% as.data.table() %>% apply(., 1, bool_mask_row) %>% t()
```
C-index. Результат аналогичен RF regr (что логично и ожидаемо)


### 3 Catboost

Multiclass
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_Catboost, n_retry = 3, k = 1,
  label = "Catboost multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_Catboost, n_retry = 3, k = 3,
  label = "Catboost multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)
```

Multilabel
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_catboost_clsf, n_retry = 3, 
  label = "Catboost multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```

Label powerset
```{r}
train_pool <- catboost.load_pool(data = X_train, 
                                 label = (get_three_letter_code(Y_b_train) %>% as.integer()) - 1)
test_pool  <- catboost.load_pool(data = X_test %>% as.data.frame())

model <- catboost.train(
  learn_pool = train_pool,
  params = list(loss_function = 'MultiClass', iterations = 500, logging_level = "Silent") 
)

Y_model_pred <- (catboost.predict(model, test_pool, prediction_type = "Class") + 1) %>% 
  all_combos[.] %>% 
  three_letters_to_bool_matrix()

calc_classification_metrics(Y_model_pred, Y_b_test, "Catboost Label Powerset")
```


#### Старый код
```{r}
ind_pred <- vector("list", 6)

for (col_i in 1:6) {
  train_pool <- catboost.load_pool(data = X_train, label = Y_b_train[, col_i] %>% as.integer)
  test_pool  <- catboost.load_pool(data = X_test,  label = Y_b_test[, col_i] %>% as.integer)
  
  model <- catboost.train(
    learn_pool = train_pool,
    test_pool = test_pool,
    params = list(loss_function = 'CrossEntropy', logging_level = "Silent") # Logloss для бинарной, CrossEntropy для мультикласса
  )
  
  # preds <- catboost.predict(model, test_pool, prediction_type = "Class")
  ind_pred[[col_i]] <- catboost.predict(model, test_pool, prediction_type = "Probability")
}

Y_pred_cb <- ind_pred %>% as.data.table() %>% apply(., 1, bool_mask_row) %>% t()

## Для C-index
classif_ind_pred <- ind_pred %>% as.data.table() %>% as.matrix()
df_metric(classif_ind_pred, Y_test, func = calc_C_index)
```


### 4 Логистическая регрессия

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_Logregr, n_retry = 5, 
  label = "logit multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_Logregr, n_retry = 5, 
  label = "logit multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```


Multilabel
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_logit_clsf, n_retry = 10, 
  label = "Logistic regr multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```

Label powerset
```{r}
model <- suppressWarnings({
  glmnet(
    x = X_train,
    y = get_three_letter_code(Y_b_train),
    family = "multinomial",
    alpha = 0  # ridge; можно alpha = 1 для lasso
  )
})

Y_model_pred <- predict(model, newx = X_test, s = last(model$lambda), type = "class") %>% 
  three_letters_to_bool_matrix()

calc_classification_metrics(Y_model_pred, Y_b_test, "Logreg Label Powerset")
```


### 5 Naive Bayes

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_NB, n_retry = 5, 
  label = "Naive Bayes multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_NB, n_retry = 5, 
  label = "Naive Bayes multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```


Multilabel
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_nb_clsf, n_retry = 10, 
  label = "Naive Bayes multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```

Label powerset
```{r}
model <- naiveBayes(x = X_train, y = get_three_letter_code(Y_b_train))

Y_model_pred <- predict(model, X_test) %>% three_letters_to_bool_matrix()
calc_classification_metrics(Y_model_pred, Y_b_test, "Naive Bayes Label Powerset")
```

### 6 kNN

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_kNN, n_retry = 5, k_neighb = 3,
  label = "kNN multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_kNN, n_retry = 5, k_neighb = 3,
  label = "kNN multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```


Multilabel
```{r}
lapply(seq(5, 55, by = 5), \(k_neighb) {
  classification_test_framework(
    Y_test, Y_b_test, ind_clsf_func = multilabel_knn_clsf, n_retry = 10, 
    label = str_glue("kNN ({k_neighb}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test, k_neighb = k_neighb
  )
}) %>% bind_rows()
```
Оптимально: k_neighb = 25

Label powerset
```{r}
model <- knn3(x = X_train, y = get_three_letter_code(Y_b_train), k = 25)

Y_model_pred <- predict(model, X_test, type = "class") %>% three_letters_to_bool_matrix()
calc_classification_metrics(Y_model_pred, Y_b_test, "kNN Label Powerset")
```

### 7 ExtraTree

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_ExtraTree, n_retry = 5,
  label = "ExtraTree multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_ExtraTree, n_retry = 5, 
  label = "ExtraTree multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```

Multilabel
```{r}
lapply(c(100, 500, 1000, 2000), \(nt) {
  classification_test_framework(
    Y_test, Y_b_test, ind_clsf_func = multilabel_extratree_clsf, n_retry = 5, ntree = nt,
    label = str_glue("ExtraTree ({nt}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test
  )
}) %>% bind_rows()
```
ntree = 2000

Label powerset
```{r}
model <- extraTrees(x = X_train, y = get_three_letter_code(Y_b_train), ntree = 1000)

Y_model_pred <- predict(model, X_test) %>% three_letters_to_bool_matrix()
calc_classification_metrics(Y_model_pred, Y_b_test, "ExtraTree Label Powerset")
```


### 8 XGBoost

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_XGBoost, n_retry = 3,
  label = "XGBoost multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_XGBoost, n_retry = 3, 
  label = "XGBoost multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```

Multilabel
```{r}
lapply(c(100, 200, 500), \(nr) {
  classification_test_framework(
    Y_test, Y_b_test, ind_clsf_func = multilabel_xgb_clsf, n_retry = 3, nrounds = nr,
    label = str_glue("XGBoost ({nr}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test
  )
}) %>% bind_rows()
```
Хватит и nrounds = 500

Label powerset
```{r}
labels <- get_three_letter_code(Y_b_train) %>% as.integer() - 1

dtrain <- xgboost::xgb.DMatrix(data = X_train, label = labels)
dtest  <- xgboost::xgb.DMatrix(data = X_test)

model <- xgboost::xgb.train(
  params = list(objective = "multi:softprob", num_class = length(all_combos), eval_metric = "mlogloss"),
  data = dtrain,
  nrounds = 500,
  verbose = 0
)

Y_model_pred <- predict(model, dtest) %>%
  matrix(ncol = length(all_combos), byrow = TRUE) %>% 
  max.col() %>% 
  all_combos[.] %>%
  three_letters_to_bool_matrix()

calc_classification_metrics(Y_model_pred, Y_b_test, "XGBoost Label Powerset")
```

### 9 LightGBM

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_LightGBM, n_retry = 3,
  label = "LightGBM multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_LightGBM, n_retry = 3, 
  label = "LightGBM multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```

Multilabel
```{r}
lapply(c(100, 250, 500), \(nr) {
  classification_test_framework(
    Y_test, Y_b_test, ind_clsf_func = multilabel_lightgbm_clsf, n_retry = 3, nrounds = nr,
    label = str_glue("LightGBM ({nr}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test
  )
}) %>% bind_rows()
```

Label Powerset
```{r}
labels <- get_three_letter_code(Y_b_train) %>% as.integer() - 1

dtrain <- lightgbm::lgb.Dataset(data = X_train, label = labels)
model <- lightgbm::lgb.train(
  params = list(objective = "multiclass", num_class = length(all_combos),
                metric = "multi_logloss", verbose = -1),
  data = dtrain,
  nrounds = 100
)

Y_model_pred <- predict(model, X_test) %>% 
  matrix(ncol = length(all_combos), byrow = TRUE) %>% 
  max.col() %>% 
  all_combos[.] %>% 
  three_letters_to_bool_matrix()

calc_classification_metrics(Y_model_pred, Y_b_test, "LightGBM Label Powerset")
```


# 4. Метрики классификации

```{r}
# Случайные предсказания
Y_rand <- replicate(n = nrow(Y_b_test), sample(c(rep(TRUE, 3), rep(FALSE, 3)), size = 6)) %>% t()

bind_rows(
  calc_classification_metrics(Y_pred_cb, Y_b_test, "Catboost"),
  calc_classification_metrics(Y_pred_rf, Y_b_test, "Random Forest"),
  calc_classification_metrics(Y_rand,    Y_b_test, "Random (baseline)")
)
```
