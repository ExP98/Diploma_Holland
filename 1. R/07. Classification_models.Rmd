---
title: "Модели классификации"
author: "Egor Glushkov"
---

# 1. Библиотеки и функции

Библиотеки:
```{r include=FALSE}
# source(paste0(here::here(), "/1. R/00. Source_all.R"))
# source(paste0(here::here(), "/1. R/10. Classification_functions.R"))
```


# 2. Данные

```{r}
# .[features, targets] <- separate_X_y(wide_data)
# .[X_train, X_test, Y_train, Y_test, split_idx] <- train_test_split(features, targets)

targets_bool <- apply(targets, 1, bool_mask_row) %>% t()
.[Y_b_train, Y_b_test] <- list(targets_bool[split_idx, ], targets_bool[!split_idx, ])

.[X_train_pca, X_test_pca] <- pca_data_preparation(X_train, X_test, limit_ssq = 0.8)
```


# 3. Решение

1. Линейные модели: V Логистическая регрессия, V MLP 
2. Деревья: V RF, V ExtraTree, Boosting (V XGBoost, V LightGBM, V Catboost)
3. V Naive Bayes
4. V kNN
5. V SVM

+ PCA!!! 

Разница есть:
 - 1 классификатор для multiclass, где берем 3 максимальные вероятности
 - 6 классификаторов для предсказания каждой метки (1 метка - 1 классификатор "да" или "нет")
 - предсказание одного из 20 классов в label powerset


Label Powerset
    Рассматриваем каждую из возможных комбинаций ровно трёх факторов (комбинаций C^3_6=20) как отдельный класс.
    Затем решаем задачу «многоклассовой» классификации на 20 классах.
    Подходит, если вам важен учёт корреляций «именно трёх» сразу, но количество классов быстро растёт при увеличении исходного числа меток.
    Реализовано в библиотеке scikit-multilearn через LabelPowerset


## 3.1 Пример решения одной моделью и как multiclass (с выбором 3), как multilabel

На примере SVM (пакет e1071)

### 3.1.1 Multiclass для SVM
Multiclass:
1) или берем топ1
2) или для топ3 размножаем датасет на 3

Берем размноженный на 3 датасет:
X[i, ] --> top1_lbl,
X[i, ] --> top2_lbl,
X[i, ] --> top3_lbl, ... (это k = 3)

k = 1 значит, что для каждой строки берем лишь один наиболее значимый лейбл, а не 3

```{r}
classification_test_framework(Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 1,
                              label = "", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
                              kernel = "sigmoid", k = 3)
```


```{r}
print("k = 3")
lapply(c("linear", "polynomial", "radial", "sigmoid"),
       \(krnl) classification_test_framework(
         Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
         label = krnl, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
         kernel = krnl, k = 3
        )
) %>% bind_rows()

print("k = 1")
lapply(c("linear", "polynomial", "radial", "sigmoid"),
       \(krnl) classification_test_framework(
         Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
         label = krnl, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
         kernel = krnl, k = 1
        )
) %>% bind_rows()
```
k = 3 стабильно лучше, притом наилучшее ядро sigmoid.

Уменьшение размерности.
Насколько уменьшаем размерность:
```{r}
lapply(seq(0.3, 1, 0.1), \(s) {
  cat("ssq: ", s, "\t")
  .[X_train_pca, X_test_pca] <- pca_data_preparation(X_train, X_test, limit_ssq = s)
  
  classification_test_framework(
    Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
    label = s, X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
  )
}) %>% bind_rows()
```
Лучшее при ssq = 0.8 (k = 19)
Возможно 0.8


### 3.1.2 Label Powerset
```{r}
experiments <- tribble(
  ~model_fn,              ~label,                          ~params,
  lp_svm_letters,         "SVM",                           list(),
  lp_rf_letters,          "Random Forest",                 list(),
  lp_nb_letters,          "Naive Bayes",                   list(),
  lp_logit_letters,       "Logistic regr Ridge",           list(alpha = 0),
  lp_logit_letters,       "Logistic regr Lasso",           list(alpha = 1),
  lp_knn_letters,         "kNN, k = 25",                   list(k_neighb = 25),
  lp_knn_letters,         "kNN, k = 100",                  list(k_neighb = 100),
  lp_et_letters,          "ExtraTrees (ntree = 1000)",     list(ntree = 1000),
  lp_et_letters,          "ExtraTrees (ntree = 2000)",     list(ntree = 2000),
  lp_catboost_letters,    "CatBoost (nrounds = 200)",      list(nrounds = 200),
  lp_catboost_letters,    "CatBoost (nrounds = 500)",      list(nrounds = 500),
  lp_xgboost_letters,     "XGBoost (nrounds = 200)",       list(nrounds = 200),
  lp_lightgbm_letters,    "LightGBM (nrounds = 100)",      list(nrounds = 100),
  lp_lightgbm_letters,    "LightGBM (nrounds = 500)",      list(nrounds = 500)
)

all_results <- run_label_powerset_experiments(experiments, X_train, Y_b_train, X_test, Y_b_test)
print(all_results)
```


## 3.2 Модели

### 1 SVM

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
  label = "simple SVM", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
  label = "SVM with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```

Multilabel
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_svm_clsf, n_retry = 5, 
  label = "SVM multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```

Label powerset
```{r}
# lp_svm_letters
```


### 2 Random Forest

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_RF, n_retry = 5, 
  label = "RF multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_RF, n_retry = 5, 
  label = "RF multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```

Multilabel
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_rf_clsf, n_retry = 5, 
  label = "RF multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```

Label powerset
```{r}
# lp_rf_letters
```


### 3 Catboost

Multiclass
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_Catboost, n_retry = 3, k = 1,
  label = "Catboost multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_Catboost, n_retry = 3, k = 3,
  label = "Catboost multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)
```

Multilabel
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_catboost_clsf, n_retry = 3, 
  label = "Catboost multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```

Label powerset
```{r}
# lp_catboost_letters
```


### 4 Логистическая регрессия

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_Logregr, n_retry = 5, 
  label = "logit multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_Logregr, n_retry = 5, 
  label = "logit multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```


Multilabel
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_logit_clsf, n_retry = 10, 
  label = "Logit multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```

Label powerset
```{r}
# lp_logit_letters
```


### 5 Naive Bayes

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_NB, n_retry = 5, 
  label = "Naive Bayes multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_NB, n_retry = 5, 
  label = "Naive Bayes multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```


Multilabel
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_nb_clsf, n_retry = 10, 
  label = "Naive Bayes multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```

Label powerset
```{r}
# lp_nb_letters
```

### 6 kNN

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_kNN, n_retry = 5, k_neighb = 3,
  label = "kNN multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_kNN, n_retry = 5, k_neighb = 3,
  label = "kNN multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```


Multilabel
```{r}
lapply(seq(5, 55, by = 5), \(k_neighb) {
  classification_test_framework(
    Y_test, Y_b_test, ind_clsf_func = multilabel_knn_clsf, n_retry = 10, 
    label = str_glue("kNN ({k_neighb}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test, k_neighb = k_neighb
  )
}) %>% bind_rows()
```
Оптимально: k_neighb = 25

Label powerset
```{r}
# lp_knn_letters
```


### 7 ExtraTree

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_ExtraTree, n_retry = 5,
  label = "ExtraTree multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_ExtraTree, n_retry = 5, 
  label = "ExtraTree multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```

Multilabel
```{r}
lapply(c(100, 1000, 2000, 3500), \(nt) {
  classification_test_framework(
    Y_test, Y_b_test, ind_clsf_func = multilabel_extratree_clsf, n_retry = 5, ntree = nt,
    label = str_glue("ExtraTree ({nt}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test
  )
}) %>% bind_rows()
```
ntree = 1000

Label powerset
```{r}
# lp_et_letters
```


### 8 XGBoost

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_XGBoost, n_retry = 3,
  label = "XGBoost multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_XGBoost, n_retry = 3, 
  label = "XGBoost multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```

Multilabel
```{r}
lapply(c(100, 200, 500), \(nr) {
  classification_test_framework(
    Y_test, Y_b_test, ind_clsf_func = multilabel_xgb_clsf, n_retry = 3, nrounds = nr,
    label = str_glue("XGBoost ({nr}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test
  )
}) %>% bind_rows()
```
Хватит и nrounds = 500

Label powerset
```{r}
# lp_xgboost_letters
```

### 9 LightGBM

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_LightGBM, n_retry = 3,
  label = "LightGBM multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_LightGBM, n_retry = 3, 
  label = "LightGBM multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```

Multilabel
```{r}
lapply(c(100, 250, 500), \(nr) {
  classification_test_framework(
    Y_test, Y_b_test, ind_clsf_func = multilabel_lightgbm_clsf, n_retry = 3, nrounds = nr,
    label = str_glue("LightGBM ({nr}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test
  )
}) %>% bind_rows()
```

Label Powerset
```{r}
# lp_lightgbm_letters
```

### 10 MLP

```{R}
# pred <- multiclass_pred_by_MLP(X_train, Y_train, X_test)

classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_MLP, n_retry = 10,
  label = "MLP multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)
```


# 4. Метрики классификации

```{r}
# Случайные предсказания
Y_rand <- replicate(n = nrow(Y_b_test), sample(c(rep(TRUE, 3), rep(FALSE, 3)), size = 6)) %>% t()

bind_rows(
  calc_classification_metrics(Y_pred_cb, Y_b_test, "Catboost"),
  calc_classification_metrics(Y_pred_rf, Y_b_test, "Random Forest"),
  calc_classification_metrics(Y_rand,    Y_b_test, "Random (baseline)")
)
```


#### Старый код

catboost
```{r}
ind_pred <- vector("list", 6)

for (col_i in 1:6) {
  train_pool <- catboost.load_pool(data = X_train, label = Y_b_train[, col_i] %>% as.integer)
  test_pool  <- catboost.load_pool(data = X_test,  label = Y_b_test[, col_i] %>% as.integer)
  
  model <- catboost.train(
    learn_pool = train_pool,
    test_pool = test_pool,
    params = list(loss_function = 'CrossEntropy', logging_level = "Silent", allow_writing_files = FALSE) # Logloss для бинарной, CrossEntropy для мультикласса
  )
  
  # preds <- catboost.predict(model, test_pool, prediction_type = "Class")
  ind_pred[[col_i]] <- catboost.predict(model, test_pool, prediction_type = "Probability")
}

Y_pred_cb <- ind_pred %>% as.data.table() %>% apply(., 1, bool_mask_row) %>% t()

## Для C-index
classif_ind_pred <- ind_pred %>% as.data.table() %>% as.matrix()
df_metric(classif_ind_pred, Y_test, func = calc_C_index)
```

randomForest
```{r}
ind_pred <- vector("list", 6)

for (col_i in 1:6) {
  rf <- randomForest(x = X_train, y = as.factor(Y_b_train[, col_i]), ntree = 1000, importance = TRUE) 
  ind_pred[[col_i]] <- predict(rf, X_test, type = "prob")[, "TRUE"]
}

## Для C-index
classif_ind_pred <- ind_pred %>% as.data.table() %>% as.matrix()
df_metric(classif_ind_pred, Y_test, func = calc_C_index)

## Для метрик классификации
Y_pred_rf <- ind_pred %>% as.data.table() %>% apply(., 1, bool_mask_row) %>% t()
```
C-index. Результат аналогичен RF regr (что логично и ожидаемо)