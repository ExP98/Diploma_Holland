---
title: "Модели классификации"
author: "Egor Glushkov"
---

# 1. Библиотеки и функции

Библиотеки:
```{r include=FALSE}
# source(paste0(here::here(), "/1. R/00. Source_all.R"))
# source(paste0(here::here(), "/1. R/10. Classification_functions.R"))
```


# 2. Данные

```{r}
# .[features, targets] <- separate_X_y(wide_data)
# .[X_train, X_test, Y_train, Y_test, split_idx] <- train_test_split(features, targets)

targets_bool <- apply(targets, 1, bool_mask_row) %>% t()
.[Y_b_train, Y_b_test] <- list(targets_bool[split_idx, ], targets_bool[!split_idx, ])

.[X_train_pca, X_test_pca] <- pca_data_preparation(X_train, X_test, limit_ssq = 0.8)
```


# 3. Решение

1. Линейные модели: V Логистическая регрессия, V MLP 
2. Деревья: V RF, V ExtraTree, Boosting (V XGBoost, V LightGBM, V Catboost)
3. V Naive Bayes
4. V kNN
5. V SVM

+ PCA!!! 

Разница есть:
 - 1 классификатор для multiclass, где берем 3 максимальные вероятности
 - 6 классификаторов для предсказания каждой метки (1 метка - 1 классификатор "да" или "нет")
 - предсказание одного из 20 классов в label powerset


Label Powerset
    Рассматриваем каждую из возможных комбинаций ровно трёх факторов (комбинаций C^3_6=20) как отдельный класс.
    Затем решаем задачу «многоклассовой» классификации на 20 классах.
    Подходит, если вам важен учёт корреляций «именно трёх» сразу, но количество классов быстро растёт при увеличении исходного числа меток.
    Реализовано в библиотеке scikit-multilearn через LabelPowerset


## 3.1 Пример решения одной моделью и как multiclass (с выбором 3 или 1 кодов), как multilabel

Multiclass на примере SVM
На примере SVM (пакет e1071)

Multiclass:
1) или берем топ1
2) или для топ3 размножаем датасет на 3

Берем размноженный на 3 датасет:
X[i, ] --> top1_lbl,
X[i, ] --> top2_lbl,
X[i, ] --> top3_lbl, ... (это k = 3)

k = 1 значит, что для каждой строки берем лишь один наиболее значимый лейбл, а не 3

```{r}
classification_test_framework(Y_test, Y_b_test, multlbl_clsf_func = multiclass_pred_by_SVM, n_retry = 1,
                              label = "", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
                              kernel = "sigmoid", k = 3)
```


```{r}
print("k = 3")
lapply(c("linear", "polynomial", "radial", "sigmoid"),
       \(krnl) classification_test_framework(
         Y_test, Y_b_test, multlbl_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
         label = krnl, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
         kernel = krnl, k = 3
        )
) %>% bind_rows()

print("k = 1")
lapply(c("linear", "polynomial", "radial", "sigmoid"),
       \(krnl) classification_test_framework(
         Y_test, Y_b_test, multlbl_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
         label = krnl, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
         kernel = krnl, k = 1
        )
) %>% bind_rows()
```
k = 3 стабильно лучше, притом наилучшее ядро sigmoid.

Уменьшение размерности.
Насколько уменьшаем размерность:
```{r}
lapply(seq(0.3, 1, 0.1), \(s) {
  cat("ssq: ", s, "\t")
  .[X_train_pca, X_test_pca] <- pca_data_preparation(X_train, X_test, limit_ssq = s)
  
  classification_test_framework(
    Y_test, Y_b_test, multlbl_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
    label = s, X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
  )
}) %>% bind_rows()
```
Лучшее при ssq = 0.8 (k = 19)
Возможно 0.8

## 3.2 Модели

### 1 SVM
### 2 Random Forest
### 3 Catboost
### 4 Логистическая регрессия
### 5 Naive Bayes
### 6 kNN

Multilabel
```{r}
lapply(seq(5, 55, by = 5), \(k_neighb) {
  classification_test_framework(
    Y_test, Y_b_test, multlbl_clsf_func = multilabel_knn_clsf, n_retry = 10, 
    label = str_glue("kNN ({k_neighb}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test, k_neighb = k_neighb
  )
}) %>% bind_rows()
```
Оптимально: k_neighb = 25

### 7 ExtraTree

Multilabel
```{r}
lapply(c(100, 1000, 2000, 3500), \(nt) {
  classification_test_framework(
    Y_test, Y_b_test, multlbl_clsf_func = multilabel_extratree_clsf, n_retry = 5, ntree = nt,
    label = str_glue("ExtraTree ({nt}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test
  )
}) %>% bind_rows()
```
ntree = 1000

### 8 XGBoost

Multilabel
```{r}
lapply(c(100, 200, 500), \(nr) {
  classification_test_framework(
    Y_test, Y_b_test, multlbl_clsf_func = multilabel_xgb_clsf, n_retry = 3, nrounds = nr,
    label = str_glue("XGBoost ({nr}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test
  )
}) %>% bind_rows()
```
Хватит и nrounds = 500

### 9 LightGBM

Multilabel
```{r}
lapply(c(100, 250, 500), \(nr) {
  classification_test_framework(
    Y_test, Y_b_test, multlbl_clsf_func = multilabel_lightgbm_clsf, n_retry = 3, nrounds = nr,
    label = str_glue("LightGBM ({nr}) multilabel"), X_train = X_train, Y_b_train = Y_b_train, 
    X_test = X_test
  )
}) %>% bind_rows()
```

### 10 MLP

```{R}
# pred <- multiclass_pred_by_MLP(X_train, Y_train, X_test)

classification_test_framework(
  Y_test, Y_b_test, clsf_func = multiclass_pred_by_MLP, n_retry = 10,
  label = "MLP multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)
```


## 3.3 Обобщение результатов

### 3.3.1 Multiclass

```{r}
multiclass_experiments <- tribble(
  ~multcls_clsf_func,              ~label,                       ~params,                  ~n_retry,
  multiclass_pred_by_SVM,          "SVM multiclass",             list(kernel = "sigmoid"), 5,
  multiclass_pred_by_RF,           "Random Forest multiclass",   list(ntree = 1000),       5,
  multiclass_pred_by_ExtraTree,    "ExtraTree multiclass",       list(ntree = 1000),       5,
  multiclass_pred_by_Logregr,      "Logit (ridge) multiclass",   list(alpha = 0),          5,
  multiclass_pred_by_Logregr,      "Logit (lasso) multiclass",   list(alpha = 1),          5,
  multiclass_pred_by_NB,           "Naive Bayes multiclass",     list(),                   5,
  multiclass_pred_by_kNN,          "kNN (3) multiclass",         list(k_neighb = 3),       5,
  multiclass_pred_by_kNN,          "kNN (25) multiclass",        list(k_neighb = 25),      5,
  multiclass_pred_by_kNN,          "kNN (50) multiclass",        list(k_neighb = 50),      5,
  multiclass_pred_by_XGBoost,      "XGBoost multiclass",         list(),                   3,
  multiclass_pred_by_LightGBM,     "LightGBM (100) multiclass",  list(nrounds = 100),      3,
  multiclass_pred_by_LightGBM,     "LightGBM (500) multiclass",  list(nrounds = 500),      3,
  multiclass_pred_by_LightGBM,     "LightGBM (1000) multiclass", list(nrounds = 1000),     3,
  multiclass_pred_by_Catboost,     "Catboost multiclass",        list(),                   3
)

mc_res     <- run_multiclass_experiments(multiclass_experiments, X_train, Y_train, X_test, Y_test, Y_b_test)
mc_res_pca <- run_multiclass_experiments(multiclass_experiments, X_train_pca, Y_train, X_test_pca, Y_test, Y_b_test)

print(mc_res)
print(mc_res_pca)
```

```{r}
bind_rows(
  compare_clsf_results(list(mc_res, mc_res_pca), mean, "mean"),
  compare_clsf_results(list(mc_res, mc_res_pca), median, "median")
)
```


### 3.3.2 Multilabel

```{r}
multilabel_experiments <- tribble(
  ~multlbl_clsf_func,             ~label,                        ~params,              ~n_retry,
  multilabel_svm_clsf,            "SVM multilabel",              list(),               5,
  multilabel_rf_clsf,             "Random Forest multilabel",    list(),               5,
  multilabel_extratree_clsf,      "ExtraTree (1000) multilabel", list(ntree = 1000),   5,
  multilabel_extratree_clsf,      "ExtraTree (2000) multilabel", list(ntree = 2000),   5,
  multilabel_logit_clsf,          "Logit multilabel",            list(),               5,
  multilabel_nb_clsf,             "Naive Bayes multilabel",      list(),               5,
  multilabel_knn_clsf,            "kNN (k = 5) multilabel",      list(k_neighb = 5),   5,
  multilabel_knn_clsf,            "kNN (k = 25) multilabel",     list(k_neighb = 25),  5,
  multilabel_knn_clsf,            "kNN (k = 50) multilabel",     list(k_neighb = 50),  5,
  multilabel_xgb_clsf,            "XGBoost multilabel",          list(nrounds = 500),  3,
  multilabel_lightgbm_clsf,       "LightGBM multilabel",         list(nrounds = 500),  3,
  multilabel_catboost_clsf,       "CatBoost multilabel",         list(nrounds = 500),  3,
)

mltlbl_res     <- run_multilabel_experiments(multilabel_experiments, X_train, Y_b_train, X_test, Y_b_test)
mltlbl_res_pca <- run_multilabel_experiments(multilabel_experiments, X_train_pca, Y_b_train, X_test_pca, Y_b_test)
print(mltlbl_res)
print(mltlbl_res_pca)
```
Топ (PCA): 
 - kNN (k = 25) multilabel
 - ExtraTree (2000) multilabel
 - XGBoost multilabel
 
Топ (без PCA):
 - kNN (k = 25) multilabel (хотя и другие kNN норм)
 - другие примерно равны (в чем-то лучше, в чем-то хуже)
 

С PCA или без результаты почти идентичны, хотя с PCA чуть лучше. 
```{r}
bind_rows(
  compare_clsf_results(list(mltlbl_res, mltlbl_res_pca), mean, "mean"),
  compare_clsf_results(list(mltlbl_res, mltlbl_res_pca), median, "median")
)
```


### 3.3.3 Label Powerset

```{r}
lp_experiments <- tribble(
  ~model_fn,              ~label,                          ~params,
  lp_svm_letters,         "SVM",                           list(),
  lp_rf_letters,          "Random Forest",                 list(),
  lp_nb_letters,          "Naive Bayes",                   list(),
  lp_logit_letters,       "Logistic regr Ridge",           list(alpha = 0),
  lp_logit_letters,       "Logistic regr Lasso",           list(alpha = 1),
  lp_knn_letters,         "kNN, k = 25",                   list(k_neighb = 25),
  lp_knn_letters,         "kNN, k = 100",                  list(k_neighb = 100),
  lp_et_letters,          "ExtraTrees (ntree = 1000)",     list(ntree = 1000),
  lp_et_letters,          "ExtraTrees (ntree = 2000)",     list(ntree = 2000),
  lp_catboost_letters,    "CatBoost (nrounds = 200)",      list(nrounds = 200),
  lp_catboost_letters,    "CatBoost (nrounds = 500)",      list(nrounds = 500),
  lp_xgboost_letters,     "XGBoost (nrounds = 200)",       list(nrounds = 200),
  lp_lightgbm_letters,    "LightGBM (nrounds = 100)",      list(nrounds = 100),
  lp_lightgbm_letters,    "LightGBM (nrounds = 500)",      list(nrounds = 500)
)

lp_res     <- run_label_powerset_experiments(lp_experiments, X_train, Y_b_train, X_test, Y_b_test)
lp_res_pca <- run_label_powerset_experiments(lp_experiments, X_train_pca, Y_b_train, X_test_pca, Y_b_test)
print(lp_res)
print(lp_res_pca)
```
Без PCA выделяются Naive Bayes, Random Forest, ExtraTrees (ntree = 2000)
С PCA Catboost лучший по всему. Хороши Logistic regr Ridge, ExtraTrees (ntree = 1000)

С PCA или без результаты почти идентичны
```{r}
bind_rows(
  compare_clsf_results(list(lp_res, lp_res_pca), mean, "mean"),
  compare_clsf_results(list(lp_res, lp_res_pca), median, "median")
)
```

### 3.3.4 Сравнение результатов

```{r}
map2(
  list(mc_res, mc_res_pca, mltlbl_res, mltlbl_res_pca, lp_res, lp_res_pca),
  list("Multiclass", "Multiclass with PCA", "Multilabel", "Multilabel with PCA",
       "Label Powerset", "Label Powerset with PCA"),
  \(df, sheetname) xlsx::write.xlsx(df, here("0. Data/1. Output/Classif_results.xlsx"), sheetname, 
                                    row.names = FALSE, append = TRUE)
)
```
Текстовое сравнение смотреть в файле Classif_results_txt.txt


## 3.4 Ансамблирование

```{r}
multiclass_probs <- map2(
  multiclass_experiments$multcls_clsf_func,
  multiclass_experiments$params,
  \(fn, args) exec(fn, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test, !!!args) %>% as.matrix()
)

multilabel_probs <- map2(
  multilabel_experiments$multlbl_clsf_func,
  multilabel_experiments$params,
  \(fn, args) exec(fn, X_train = X_train, Y_b_train = Y_b_train, X_test = X_test, !!!args) %>% as.matrix()
)
```

```{r}
mat <- multiclass_probs
# mat <- multilabel_probs

## 1. Равные веса
w <- rep(1/length(mat), length(mat))
weighted_cindex(w, mat)

## 2. Веса на основе вектора Шэпли
# appr_w1 <- approx_shapley_cpp(mat, Y_test, matr_cind, R = 1000)
# appr_w2 <- approx_shapley(mat, Y_test, matr_cind)
# weighted_cindex(appr_w1, mat)
# weighted_cindex(appr_w2, mat)


## 3. Веса на основе метрик каждого
w <- sapply(mat, \(probs) df_metric(probs, Y_test, func = calc_C_index))
weighted_cindex(w, mat)


## 3.2 Уберем худшие
w <- sapply(mat, \(probs) df_metric(probs, Y_test, func = calc_C_index))
# w <- cut_w(w, 9.5)

lapply(seq(9.0, 10.0, 0.1),
       \(th) list(th = th, cind = weighted_cindex(cut_w(w, th), mat))) %>%
  bind_rows()


## 3.3 Softmax
w <- sapply(mat, \(probs) df_metric(probs, Y_test, func = calc_C_index))
w <- mclust::softmax(w)
# softmax бессмысленно занулять

weighted_cindex(w, mat)
```

```{r}
w <- sapply(mat, \(probs) df_metric(probs, Y_test, func = calc_C_index))
w_norm <- cut_w(w, qnt_prob = 0.67)
sngnf_ind <- which(w_norm > 1e-3)

filtered_w <- w[sngnf_ind]
filtered_matr <- mat[sngnf_ind]
```


Шэпли
```{r}
library(tictoc)
# mat <- mat
mat <- filtered_matr

tic()
appr_w1 <- approx_shapley_cpp(mat, Y_test, matr_cind, R = 1000)
toc()

tic()
appr_w2 <- approx_shapley(mat, Y_test, matr_cind)
toc()

weighted_cindex(appr_w1, mat)
weighted_cindex(appr_w2, mat)
beepr::beep()
```

Обрезать малые веса:
```{r}
w <- sapply(mat, \(probs) df_metric(probs, Y_test, func = calc_C_index))
w_cut <- cut_w(w, qnt_prob = 0.67)

w_b <- w - min(w)
w_b_cut <- cut_w(w_b, qnt_prob = 0.67)

sapply(list(w, w_cut, w_b, w_b_cut), \(w_) weighted_cindex(w_, mat))
```

Сразу уберем малые веса и применим остальные методы:
```{r}
# 1. Просто без всего
weighted_cindex(filtered_w, filtered_matr)


# 2. Шэпли
# shap_fltr_w <- approx_shapley_cpp(filtered_matr, Y_test, matr_cind, R = 1000)
shap_fltr_w <- approx_shapley(filtered_matr, Y_test)
weighted_cindex(shap_fltr_w, filtered_matr)


# 3. grid search
gs_fltr_res <- grid_search_weights(filtered_matr, Y_test)
weighted_cindex(gs_fltr_res$best_weights, filtered_matr)


# 4. stacking_qp_weights 
# очень быстро
Y_one_hot <- apply(Y_b_test, 2, as.integer)
w_qp1 <- stacking_qp_weights(filtered_matr, Y_one_hot) %>% round(4)
w_qp2 <- stacking_qp_weights(mat, Y_one_hot) %>% round(4)

weighted_cindex(w_qp1, filtered_matr)
weighted_cindex(w_qp2, mat)


# 5. GA
# быстро
ga_res1 <- ga_optimize_weights(filtered_matr, Y_test)
ga_res2 <- ga_optimize_weights(mat, Y_test)

weighted_cindex(ga_res1$weights, filtered_matr)
weighted_cindex(ga_res2$weights, mat)


# 6. pso_optimize_weights
# быстро
pso_res1 <- pso_optimize_weights(filtered_matr, Y_test)
pso_res2 <- pso_optimize_weights(mat, Y_test)

weighted_cindex(pso_res1$weights, filtered_matr)
weighted_cindex(pso_res2$weights, mat)


# 7. bayes_optimize_weights
# долго...
bo_res <- bayes_optimize_weights(filtered_matr, Y_test)
weighted_cindex(bo_res$weights, filtered_matr)


# 8. coordinate_optimize_weights
сo_res <- coordinate_optimize_weights(filtered_matr, Y_test)
weighted_cindex(сo_res$weights, filtered_matr)
```
Вывод: исключение "слабых" алгоритмов, лишь затем их комбо. Это улучшает результаты + серьезно сокращает вычисления


# 4. Метрики классификации

```{r}
# Случайные предсказания
Y_rand <- replicate(n = nrow(Y_b_test), sample(c(rep(TRUE, 3), rep(FALSE, 3)), size = 6)) %>% t()

bind_rows(
  calc_classification_metrics(Y_pred_cb, Y_b_test, "Catboost"),
  calc_classification_metrics(Y_pred_rf, Y_b_test, "Random Forest"),
  calc_classification_metrics(Y_rand,    Y_b_test, "Random (baseline)")
)
```
