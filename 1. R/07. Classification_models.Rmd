---
title: "Модели классификации"
author: "Egor Glushkov"
---

# 1. Библиотеки и функции

Библиотеки:
```{r include=FALSE}
# source(paste0(here::here(), "/1. R/00. Source_all.R"))
# source(paste0(here::here(), "/1. R/10. Classification_functions.R"))
```


# 2. Данные

```{r}
# .[features, targets] <- separate_X_y(wide_data)
# .[X_train, X_test, Y_train, Y_test, split_idx] <- train_test_split(features, targets)

targets_bool <- apply(targets, 1, bool_mask_row) %>% t()
.[Y_b_train, Y_b_test] <- list(targets_bool[split_idx, ], targets_bool[!split_idx, ])
```


# 3. Решение

1. Линейные модели: Логистическая регрессия, glm, MLP 
2. Деревья: V RF, ExtraTree, Boosting (XGBoost, LightGBM, Catboost)
3. Naive Bayes
4. kNN
5. SVM
6. Нейронки

+ PCA!!! 

Разница есть:
 - 1 классификатор для multiclass, где берем 3 максимальные вероятности
 - 6 классификаторов для предсказания каждой метки (1 метка - 1 классификатор "да" или "нет")
 - предсказание одного из 20 классов в label powerset


Label Powerset
    Рассматриваем каждую из возможных комбинаций ровно трёх факторов (комбинаций C^3_6=20) как отдельный класс.
    Затем решаем задачу «многоклассовой» классификации на 20 классах.
    Подходит, если вам важен учёт корреляций «именно трёх» сразу, но количество классов быстро растёт при увеличении исходного числа меток.
    Реализовано в библиотеке scikit-multilearn через LabelPowerset


## 3.1 Пример решения одной моделью и как multiclass (с выбором 3), как multilabel

На примере SVM (пакет e1071)

### 3.1.1 Multiclass
Multiclass:
1) или берем топ1
2) или для топ3 размножаем датасет на 3

Берем размноженный на 3 датасет:
X[i, ] --> top1_lbl,
X[i, ] --> top2_lbl,
X[i, ] --> top3_lbl, ... (это k = 3)

k = 1 значит, что для каждой строки берем лишь один наиболее значимый лейбл, а не 3

```{r}
classification_test_framework(Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 1,
                              label = "", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
                              kernel = "sigmoid", k = 3)
```


```{r}
print("k = 3")
lapply(c("linear", "polynomial", "radial", "sigmoid"),
       \(krnl) classification_test_framework(
         Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
         label = krnl, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
         kernel = krnl, k = 3
        )
) %>% bind_rows()

print("k = 1")
lapply(c("linear", "polynomial", "radial", "sigmoid"),
       \(krnl) classification_test_framework(
         Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
         label = krnl, X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test,
         kernel = krnl, k = 1
        )
) %>% bind_rows()
```
k = 3 стабильно лучше, притом наилучшее ядро sigmoid.

Уменьшение размерности.
Насколько уменьшаем размерность:
```{r}
lapply(seq(0.3, 1, 0.1), \(s) {
  cat("ssq: ", s, "\t")
  .[X_train_pca, X_test_pca] <- pca_data_preparation(X_train, X_test, limit_ssq = s)
  
  classification_test_framework(
    Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
    label = s, X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
  )
}) %>% bind_rows()
```
Лучшее при ssq = 0.8 (k = 19)
Возможно 0.8

Итого лучшее:
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
  label = "simple SVM", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
.[X_train_pca, X_test_pca] <- pca_data_preparation(X_train, X_test, limit_ssq = 0.8)

r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_SVM, n_retry = 5, 
  label = "SVM with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```


### 3.1.2 Multilabel

multilabel_svm_clsf -- multilabel (Multilabel)

Модифицировать под разные модели (R6class)
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_svm_clsf, n_retry = 5, 
  label = "SVM multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```


### 3.1.3 Label Powerset

```{r}
svm_model <- svm(
  x = X_train,
  y = get_three_letter_code(Y_b_train),
  kernel = "sigmoid",
  probability = TRUE
)

Y_svm_pred <- predict(svm_model, X_test, probability = TRUE) %>% three_letters_to_bool_matrix()
calc_classification_metrics(Y_svm_pred, Y_b_test, "Label Powerset")
```


## 3.2 Модели

### 3.2.1 RandomForest

Multiclass
```{r}
# a)
r1 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_RF, n_retry = 5, 
  label = "RF multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

# b)
r2 <- classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_RF, n_retry = 5, 
  label = "RF multiclass with PCA", X_train_ = X_train_pca, Y_train_ = Y_train, X_test_ = X_test_pca
)

bind_rows(r1, r2)
```

Multilabel
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_rf_clsf, n_retry = 5, 
  label = "RF multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```

Label superset
```{r}
model <- randomForest(x = X_train, y = get_three_letter_code(Y_b_train), ntree = 1000)

Y_model_pred <- predict(model, X_test) %>% three_letters_to_bool_matrix()
calc_classification_metrics(Y_model_pred, Y_b_test, "RF Label Powerset")
```


#### Старый код
```{r}
ind_pred <- vector("list", 6)

for (col_i in 1:6) {
  rf <- randomForest(x = X_train, y = as.factor(Y_b_train[, col_i]), ntree = 1000, importance = TRUE) 
  ind_pred[[col_i]] <- predict(rf, X_test, type = "prob")[, "TRUE"]
}

## Для C-index
classif_ind_pred <- ind_pred %>% as.data.table() %>% as.matrix()
df_metric(classif_ind_pred, Y_test, func = calc_C_index)

## Для метрик классификации
Y_pred_rf <- ind_pred %>% as.data.table() %>% apply(., 1, bool_mask_row) %>% t()
```
C-index. Результат аналогичен RF regr (что логично и ожидаемо)


### 3.2.2 Catboost

```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_Catboost, n_retry = 3, k = 1,
  label = "Catboost multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)

classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multiclass_pred_by_Catboost, n_retry = 3, k = 3,
  label = "Catboost multiclass", X_train_ = X_train, Y_train_ = Y_train, X_test_ = X_test
)
```

Multilabel
```{r}
classification_test_framework(
  Y_test, Y_b_test, ind_clsf_func = multilabel_catboost_clsf, n_retry = 1, 
  label = "Catboost multilabel", X_train = X_train, Y_b_train = Y_b_train, X_test = X_test
)
```

Label superset
```{r}
train_pool <- catboost.load_pool(data = X_train, 
                                 label = (get_three_letter_code(Y_b_train) %>% as.integer()) - 1)
test_pool  <- catboost.load_pool(data = X_test %>% as.data.frame())

model <- catboost.train(
  learn_pool = train_pool,
  params = list(loss_function = 'MultiClass', iterations = 500, logging_level = "Silent") 
)

Y_model_pred <- (catboost.predict(model, test_pool, prediction_type = "Class") + 1) %>% 
  all_combos[.] %>% 
  three_letters_to_bool_matrix()

calc_classification_metrics(Y_model_pred, Y_b_test, "Catboost Label Powerset")
```


#### Старый код
```{r}
ind_pred <- vector("list", 6)

for (col_i in 1:6) {
  train_pool <- catboost.load_pool(data = X_train, label = Y_b_train[, col_i] %>% as.integer)
  test_pool  <- catboost.load_pool(data = X_test,  label = Y_b_test[, col_i] %>% as.integer)
  
  model <- catboost.train(
    learn_pool = train_pool,
    test_pool = test_pool,
    params = list(loss_function = 'CrossEntropy', logging_level = "Silent") # Logloss для бинарной, CrossEntropy для мультикласса
  )
  
  # preds <- catboost.predict(model, test_pool, prediction_type = "Class")
  ind_pred[[col_i]] <- catboost.predict(model, test_pool, prediction_type = "Probability")
}

Y_pred_cb <- ind_pred %>% as.data.table() %>% apply(., 1, bool_mask_row) %>% t()

## Для C-index
classif_ind_pred <- ind_pred %>% as.data.table() %>% as.matrix()
df_metric(classif_ind_pred, Y_test, func = calc_C_index)
```


# 4. Метрики классификации

```{r}
# Случайные предсказания
Y_rand <- replicate(n = nrow(Y_b_test), sample(c(rep(TRUE, 3), rep(FALSE, 3)), size = 6)) %>% t()

bind_rows(
  calc_classification_metrics(Y_pred_cb, Y_b_test, "Catboost"),
  calc_classification_metrics(Y_pred_rf, Y_b_test, "Random Forest"),
  calc_classification_metrics(Y_rand,    Y_b_test, "Random (baseline)")
)
```
